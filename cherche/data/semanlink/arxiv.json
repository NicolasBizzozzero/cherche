[
    {
        "document": "http://www.semanlink.net/doc/2020/02/joint_embedding_of_words_and_la",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning_attention",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_text_classification",
            "http://www.semanlink.net/tag/label_embedding"
        ],
        "comment": "> text classification as\r\na label-word joint embedding problem:\r\n**each label is embedded in the same space\r\nwith the word vectors**. We introduce\r\nan attention framework that measures the\r\ncompatibility of embeddings between text\r\nsequences and labels. The attention is\r\nlearned on a training set of labeled samples\r\nto ensure that, given a text sequence, the\r\nrelevant words are weighted higher than\r\nthe irrelevant ones.\r\n\r\n(from introduction:)\r\n\r\n> For the task of text classification,\r\nlabels play a central role of the final performance.\r\nA natural question to ask is how we can\r\ndirectly use label information in constructing the\r\ntext-sequence representations\r\n\r\n> The proposed LEAM (Label-\r\nEmbedding Attentive Mode) is implemented by jointly\r\nembedding the word and label in the same latent\r\nspace, and **the text representations are constructed\r\ndirectly using the text-label compatibility**.",
        "title": "[1805.04174] Joint Embedding of Words and Labels for Text Classification (ACL Anthology 2018)",
        "relatedDoc": [],
        "creationTime": "2020-02-18T15:01:31Z",
        "creationDate": "2020-02-18",
        "bookmarkOf": [
            "https://arxiv.org/abs/1805.04174"
        ],
        "arxiv_author": [
            "Wenlin Wang",
            "Chunyuan Li",
            "Xinyuan Zhang",
            "Lawrence Carin",
            "Ricardo Henao",
            "Dinghan Shen",
            "Guoyin Wang",
            "Yizhe Zhang"
        ],
        "arxiv_summary": "Word embeddings are effective intermediate representations for capturing\nsemantic regularities between words, when learning the representations of text\nsequences. We propose to view text classification as a label-word joint\nembedding problem: each label is embedded in the same space with the word\nvectors. We introduce an attention framework that measures the compatibility of\nembeddings between text sequences and labels. The attention is learned on a\ntraining set of labeled samples to ensure that, given a text sequence, the\nrelevant words are weighted higher than the irrelevant ones. Our method\nmaintains the interpretability of word embeddings, and enjoys a built-in\nability to leverage alternative sources of information, in addition to input\ntext sequences. Extensive results on the several large text datasets show that\nthe proposed framework outperforms the state-of-the-art methods by a large\nmargin, in terms of both accuracy and speed.",
        "arxiv_firstAuthor": "Guoyin Wang",
        "arxiv_title": "Joint Embedding of Words and Labels for Text Classification",
        "arxiv_num": "1805.04174",
        "arxiv_published": "2018-05-10T20:42:52Z",
        "arxiv_updated": "2018-05-10T20:42:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2010_12309_a_survey_on_recent",
        "tag": [
            "http://www.semanlink.net/tag/bosch",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_low_resource_scenarios",
            "http://www.semanlink.net/tag/low_resource_languages"
        ],
        "comment": "Low-resource scenarios: low-resource languages, but also non standard domain and tasks.\r\n\r\none key goal of this survey is to highlight the underlying assumptions\r\n",
        "title": "[2010.12309] A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
        "relatedDoc": [],
        "creationTime": "2021-07-06T13:08:01Z",
        "creationDate": "2021-07-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.12309"
        ],
        "arxiv_author": [
            "Jannik Str\u00f6tgen",
            "Lukas Lange",
            "Dietrich Klakow",
            "Heike Adel",
            "Michael A. Hedderich"
        ],
        "arxiv_summary": "Deep neural networks and huge language models are becoming omnipresent in\nnatural language applications. As they are known for requiring large amounts of\ntraining data, there is a growing body of work to improve the performance in\nlow-resource settings. Motivated by the recent fundamental changes towards\nneural models and the popular pre-train and fine-tune paradigm, we survey\npromising approaches for low-resource natural language processing. After a\ndiscussion about the different dimensions of data availability, we give a\nstructured overview of methods that enable learning when training data is\nsparse. This includes mechanisms to create additional labeled data like data\naugmentation and distant supervision as well as transfer learning settings that\nreduce the need for target supervision. A goal of our survey is to explain how\nthese methods differ in their requirements as understanding them is essential\nfor choosing a technique suited for a specific low-resource setting. Further\nkey aspects of this work are to highlight open issues and to outline promising\ndirections for future research.",
        "arxiv_firstAuthor": "Michael A. Hedderich",
        "arxiv_title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
        "arxiv_num": "2010.12309",
        "arxiv_published": "2020-10-23T11:22:01Z",
        "arxiv_updated": "2021-04-09T13:48:02Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1508.01991",
        "tag": [
            "http://www.semanlink.net/tag/conditional_random_field",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/named_entity_recognition",
            "http://www.semanlink.net/tag/bi_lstm",
            "http://www.semanlink.net/tag/sequence_labeling"
        ],
        "comment": "",
        "title": "[1508.01991] Bidirectional LSTM-CRF Models for Sequence Tagging",
        "relatedDoc": [],
        "creationTime": "2018-03-05T19:03:20Z",
        "creationDate": "2018-03-05",
        "bookmarkOf": [],
        "arxiv_author": [
            "Zhiheng Huang",
            "Wei Xu",
            "Kai Yu"
        ],
        "arxiv_summary": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based\nmodels for sequence tagging. These models include LSTM networks, bidirectional\nLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer\n(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is\nthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to\nNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model\ncan efficiently use both past and future input features thanks to a\nbidirectional LSTM component. It can also use sentence level tag information\nthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or\nclose to) accuracy on POS, chunking and NER data sets. In addition, it is\nrobust and has less dependence on word embedding as compared to previous\nobservations.",
        "arxiv_firstAuthor": "Zhiheng Huang",
        "arxiv_title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
        "arxiv_num": "1508.01991",
        "arxiv_published": "2015-08-09T06:32:47Z",
        "arxiv_updated": "2015-08-09T06:32:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/08/2010_02353_participatory_rese",
        "tag": [
            "http://www.semanlink.net/tag/nlp_4_africa",
            "http://www.semanlink.net/tag/machine_translation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/masakhane",
            "http://www.semanlink.net/tag/low_resource_languages",
            "http://www.semanlink.net/tag/emnlp_2020",
            "http://www.semanlink.net/tag/african_languages"
        ],
        "comment": "about machine translation using parallel corpora only",
        "title": "[2010.02353] Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages",
        "relatedDoc": [],
        "creationTime": "2021-08-25T17:01:12Z",
        "creationDate": "2021-08-25",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.02353"
        ],
        "arxiv_author": [
            "Tajudeen Kolawole",
            "Bonaventure Dossou",
            "Christopher Onyefuluchi",
            "Kevin Degila",
            "Sackey Freshia",
            "Wilhelmina Nekoto",
            "Elan van Biljon",
            "Kelechi Ogueji",
            "Shamsuddeen Hassan Muhammad",
            "Ayodele Olabiyi",
            "Julia Kreutzer",
            "Taiwo Fagbohungbe",
            "Hady Elsahar",
            "Laura Jane Martinus",
            "Herman Kamper",
            "Solomon Oluwole Akinola",
            "Daniel Whitenack",
            "Ghollah Kioko",
            "Espoir Murhabazi",
            "Masabata Mokgesi-Selinga",
            "Salomey Osei",
            "Perez Ogayo",
            "Lawrence Okegbemi",
            "Kolawole Tajudeen",
            "Blessing Sibanda",
            "Tshinondiwa Matsila",
            "Chris Emezue",
            "Ignatius Ezeani",
            "Abdallah Bashir",
            "Idris Abdulkabir Dangana",
            "Arshath Ramkilowan",
            "Jamiil Toure Ali",
            "Salomon Kabongo",
            "Mofe Adeyemi",
            "Alp \u00d6ktem",
            "Timi Fasubaa",
            "Goodness Duru",
            "Vukosi Marivate",
            "Blessing Itoro Bassey",
            "Jason Webster",
            "Adewale Akinfaderin",
            "Iroro Orife",
            "Orevaoghene Ahia",
            "Kathleen Siminyu",
            "Ricky Macharm",
            "Musie Meressa",
            "Rubungo Andre Niyongabo",
            "Jade Abbott"
        ],
        "arxiv_summary": "Research in NLP lacks geographic diversity, and the question of how NLP can\nbe scaled to low-resourced languages has not yet been adequately solved.\n\"Low-resourced\"-ness is a complex problem going beyond data availability and\nreflects systemic problems in society. In this paper, we focus on the task of\nMachine Translation (MT), that plays a crucial role for information\naccessibility and communication worldwide. Despite immense improvements in MT\nover the past decade, MT is centered around a few high-resourced languages. As\nMT researchers cannot solve the problem of low-resourcedness alone, we propose\nparticipatory research as a means to involve all necessary agents required in\nthe MT development process. We demonstrate the feasibility and scalability of\nparticipatory research with a case study on MT for African languages. Its\nimplementation leads to a collection of novel translation datasets, MT\nbenchmarks for over 30 languages, with human evaluations for a third of them,\nand enables participants without formal training to make a unique scientific\ncontribution. Benchmarks, models, data, code, and evaluation results are\nreleased under https://github.com/masakhane-io/masakhane-mt.",
        "arxiv_firstAuthor": "Wilhelmina Nekoto",
        "arxiv_title": "Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages",
        "arxiv_num": "2010.02353",
        "arxiv_published": "2020-10-05T21:50:38Z",
        "arxiv_updated": "2020-11-06T23:30:45Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1801.04016",
        "tag": [
            "http://www.semanlink.net/tag/judea_pearl",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/machine_learning",
            "http://www.semanlink.net/tag/artificial_general_intelligence",
            "http://www.semanlink.net/tag/human_level_ai"
        ],
        "comment": "To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks",
        "title": "[1801.04016] Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution",
        "relatedDoc": [],
        "creationTime": "2018-02-21T23:48:03Z",
        "creationDate": "2018-02-21",
        "bookmarkOf": [],
        "arxiv_author": [
            "Judea Pearl"
        ],
        "arxiv_summary": "Current machine learning systems operate, almost exclusively, in a\nstatistical, or model-free mode, which entails severe theoretical limits on\ntheir power and performance. Such systems cannot reason about interventions and\nretrospection and, therefore, cannot serve as the basis for strong AI. To\nachieve human level intelligence, learning machines need the guidance of a\nmodel of reality, similar to the ones used in causal inference tasks. To\ndemonstrate the essential role of such models, I will present a summary of\nseven tasks which are beyond reach of current machine learning systems and\nwhich have been accomplished using the tools of causal modeling.",
        "arxiv_firstAuthor": "Judea Pearl",
        "arxiv_title": "Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution",
        "arxiv_num": "1801.04016",
        "arxiv_published": "2018-01-11T23:37:48Z",
        "arxiv_updated": "2018-01-11T23:37:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2010_05234_a_practical_guide_",
        "tag": [
            "http://www.semanlink.net/tag/sample_code",
            "http://www.semanlink.net/tag/tutorial",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/graph_neural_networks"
        ],
        "comment": "",
        "title": "[2010.05234] A Practical Guide to Graph Neural Networks",
        "relatedDoc": [],
        "creationTime": "2020-10-15T00:07:48Z",
        "creationDate": "2020-10-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.05234"
        ],
        "arxiv_author": [
            "Casey Lickfold",
            "Yulan Guo",
            "Jack Joyner",
            "Isaac Ronald Ward",
            "Stash Rowe",
            "Mohammed Bennamoun"
        ],
        "arxiv_summary": "Graph neural networks (GNNs) have recently grown in popularity in the field\nof artificial intelligence due to their unique ability to ingest relatively\nunstructured data types as input data. Although some elements of the GNN\narchitecture are conceptually similar in operation to traditional neural\nnetworks (and neural network variants), other elements represent a departure\nfrom traditional deep learning techniques. This tutorial exposes the power and\nnovelty of GNNs to the average deep learning enthusiast by collating and\npresenting details on the motivations, concepts, mathematics, and applications\nof the most common types of GNNs. Importantly, we present this tutorial\nconcisely, alongside worked code examples, and at an introductory pace, thus\nproviding a practical and accessible guide to understanding and using GNNs.",
        "arxiv_firstAuthor": "Isaac Ronald Ward",
        "arxiv_title": "A Practical Guide to Graph Neural Networks",
        "arxiv_num": "2010.05234",
        "arxiv_published": "2020-10-11T12:36:17Z",
        "arxiv_updated": "2020-10-11T12:36:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2104_08663_beir_a_heterogeno",
        "tag": [
            "http://www.semanlink.net/tag/neural_models_for_information_retrieval",
            "http://www.semanlink.net/tag/information_retrieval",
            "http://www.semanlink.net/tag/benchmark",
            "http://www.semanlink.net/tag/zero_shot",
            "http://www.semanlink.net/tag/nlp_datasets",
            "http://www.semanlink.net/tag/nils_reimers",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "[GitHub](doc:2021/07/ukplab_beir_a_heterogeneous_be)\r\n\r\n> Our results show **BM25 is a robust baseline**\r\nand Reranking-based models overall achieve\r\nthe best zero-shot performances, however, at\r\nhigh computational costs. In contrast, **Denseretrieval\r\nmodels are computationally more efficient\r\nbut often underperform other approaches**\r\n\r\n17 English evaluation datasets, 9 heterogeneous tasks (Non-English left for future work)",
        "title": "[2104.08663] BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/07/ukplab_beir_a_heterogeneous_be"
        ],
        "creationTime": "2021-07-09T12:36:38Z",
        "creationDate": "2021-07-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/2104.08663"
        ],
        "arxiv_author": [
            "Nils Reimers",
            "Nandan Thakur",
            "Andreas R\u00fcckl\u00e9",
            "Abhishek Srivastava",
            "Iryna Gurevych"
        ],
        "arxiv_summary": "Neural IR models have often been studied in homogeneous and narrow settings,\nwhich has considerably limited insights into their generalization capabilities.\nTo address this, and to allow researchers to more broadly establish the\neffectiveness of their models, we introduce BEIR (Benchmarking IR), a\nheterogeneous benchmark for information retrieval. We leverage a careful\nselection of 17 datasets for evaluation spanning diverse retrieval tasks\nincluding open-domain datasets as well as narrow expert domains. We study the\neffectiveness of nine state-of-the-art retrieval models in a zero-shot\nevaluation setup on BEIR, finding that performing well consistently across all\ndatasets is challenging. Our results show BM25 is a robust baseline and\nReranking-based models overall achieve the best zero-shot performances,\nhowever, at high computational costs. In contrast, Dense-retrieval models are\ncomputationally more efficient but often underperform other approaches,\nhighlighting the considerable room for improvement in their generalization\ncapabilities. In this work, we extensively analyze different retrieval models\nand provide several suggestions that we believe may be useful for future work.\nBEIR datasets and code are available at https://github.com/UKPLab/beir.",
        "arxiv_firstAuthor": "Nandan Thakur",
        "arxiv_title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
        "arxiv_num": "2104.08663",
        "arxiv_published": "2021-04-17T23:29:55Z",
        "arxiv_updated": "2021-04-28T13:59:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1812.04616",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_machine_translation",
            "http://www.semanlink.net/tag/sequence_to_sequence_learning",
            "http://www.semanlink.net/tag/language_model"
        ],
        "comment": "predicting embeddings instead of word IDs (avoids a discrete softmax, using a new loss)\r\n\r\n[@honnibal](https://twitter.com/honnibal/status/1073513114468081664)\r\n\r\n",
        "title": "[1812.04616] Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
        "relatedDoc": [],
        "creationTime": "2018-12-14T14:50:03Z",
        "creationDate": "2018-12-14",
        "bookmarkOf": [],
        "arxiv_author": [
            "Yulia Tsvetkov",
            "Sachin Kumar"
        ],
        "arxiv_summary": "The Softmax function is used in the final layer of nearly all existing\nsequence-to-sequence models for language generation. However, it is usually the\nslowest layer to compute which limits the vocabulary size to a subset of most\nfrequent types; and it has a large memory footprint. We propose a general\ntechnique for replacing the softmax layer with a continuous embedding layer.\nOur primary innovations are a novel probabilistic loss, and a training and\ninference procedure in which we generate a probability distribution over\npre-trained word embeddings, instead of a multinomial distribution over the\nvocabulary obtained via softmax. We evaluate this new class of\nsequence-to-sequence models with continuous outputs on the task of neural\nmachine translation. We show that our models obtain upto 2.5x speed-up in\ntraining time while performing on par with the state-of-the-art models in terms\nof translation quality. These models are capable of handling very large\nvocabularies without compromising on translation quality. They also produce\nmore meaningful errors than in the softmax-based models, as these errors\ntypically lie in a subspace of the vector space of the reference translations.",
        "arxiv_firstAuthor": "Sachin Kumar",
        "arxiv_title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
        "arxiv_num": "1812.04616",
        "arxiv_published": "2018-12-10T20:00:36Z",
        "arxiv_updated": "2019-03-22T03:08:01Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2010_11967_language_models_ar",
        "tag": [
            "http://www.semanlink.net/tag/language_models_as_knowledge_bases",
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2010.11967] Language Models are Open Knowledge Graphs",
        "relatedDoc": [],
        "creationTime": "2020-10-26T17:10:56Z",
        "creationDate": "2020-10-26",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.11967"
        ],
        "arxiv_author": [
            "Chenguang Wang",
            "Xiao Liu",
            "Dawn Song"
        ],
        "arxiv_summary": "This paper shows how to construct knowledge graphs (KGs) from pre-trained\nlanguage models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs\n(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised\nmanner, requiring humans to create knowledge. Recent deep language models\nautomatically acquire knowledge from large-scale corpora via pre-training. The\nstored knowledge has enabled the language models to improve downstream NLP\ntasks, e.g., answering questions, and writing code and articles. In this paper,\nwe propose an unsupervised method to cast the knowledge contained within\nlanguage models into KGs. We show that KGs are constructed with a single\nforward pass of the pre-trained language models (without fine-tuning) over the\ncorpora. We demonstrate the quality of the constructed KGs by comparing to two\nKGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual\nknowledge that is new in the existing KGs. Our code and KGs will be made\npublicly available.",
        "arxiv_firstAuthor": "Chenguang Wang",
        "arxiv_title": "Language Models are Open Knowledge Graphs",
        "arxiv_num": "2010.11967",
        "arxiv_published": "2020-10-22T18:01:56Z",
        "arxiv_updated": "2020-10-22T18:01:56Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1906_02715_visualizing_and_me",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bertology",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/these_irit_renault_biblio",
            "http://www.semanlink.net/tag/geometry_of_language_embeddings",
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/tree_embeddings"
        ],
        "comment": "> At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations",
        "title": "[1906.02715] Visualizing and Measuring the Geometry of BERT",
        "relatedDoc": [],
        "creationTime": "2019-06-07T23:33:36Z",
        "creationDate": "2019-06-07",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.02715"
        ],
        "arxiv_author": [
            "Adam Pearce",
            "Martin Wattenberg",
            "Andy Coenen",
            "Been Kim",
            "Emily Reif",
            "Fernanda Vi\u00e9gas",
            "Ann Yuan"
        ],
        "arxiv_summary": "Transformer architectures show significant promise for natural language\nprocessing. Given that a single pretrained model can be fine-tuned to perform\nwell on many different tasks, these networks appear to extract generally useful\nlinguistic features. A natural question is how such networks represent this\ninformation internally. This paper describes qualitative and quantitative\ninvestigations of one particularly effective model, BERT. At a high level,\nlinguistic features seem to be represented in separate semantic and syntactic\nsubspaces. We find evidence of a fine-grained geometric representation of word\nsenses. We also present empirical descriptions of syntactic representations in\nboth attention matrices and individual word embeddings, as well as a\nmathematical argument to explain the geometry of these representations.",
        "arxiv_firstAuthor": "Andy Coenen",
        "arxiv_title": "Visualizing and Measuring the Geometry of BERT",
        "arxiv_num": "1906.02715",
        "arxiv_published": "2019-06-06T17:33:22Z",
        "arxiv_updated": "2019-10-28T17:53:14Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1812_00417_snorkel_drybell_a",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_resources",
            "http://www.semanlink.net/tag/weak_supervision",
            "http://www.semanlink.net/tag/snorkel",
            "http://www.semanlink.net/tag/nlp_using_knowledge",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> study showing how existing knowledge resources from across an organization can be used as weak supervision in order to bring development time and cost down by an order of magnitude.\r\n> Snorkel DryBell, a new weak supervision management system for this setting.\r\n\r\n[Blog post](/doc/2019/06/google_ai_blog_harnessing_orga)",
        "title": "[1812.00417] Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/06/google_ai_blog_harnessing_orga"
        ],
        "creationTime": "2019-06-28T00:31:17Z",
        "creationDate": "2019-06-28",
        "bookmarkOf": [
            "https://arxiv.org/abs/1812.00417"
        ],
        "arxiv_author": [
            "Cassandra Xia",
            "Chong Luo",
            "Daniel Rodriguez",
            "Christopher R\u00e9",
            "Yintao Liu",
            "Stephen H. Bach",
            "Houman Alborzi",
            "Braden Hancock",
            "Haidong Shao",
            "Souvik Sen",
            "Rob Malkin",
            "Rahul Kuchhal",
            "Alexander Ratner"
        ],
        "arxiv_summary": "Labeling training data is one of the most costly bottlenecks in developing\nmachine learning-based applications. We present a first-of-its-kind study\nshowing how existing knowledge resources from across an organization can be\nused as weak supervision in order to bring development time and cost down by an\norder of magnitude, and introduce Snorkel DryBell, a new weak supervision\nmanagement system for this setting. Snorkel DryBell builds on the Snorkel\nframework, extending it in three critical aspects: flexible, template-based\ningestion of diverse organizational knowledge, cross-feature production\nserving, and scalable, sampling-free execution. On three classification tasks\nat Google, we find that Snorkel DryBell creates classifiers of comparable\nquality to ones trained with tens of thousands of hand-labeled examples,\nconverts non-servable organizational resources to servable models for an\naverage 52% performance improvement, and executes over millions of data points\nin tens of minutes.",
        "arxiv_firstAuthor": "Stephen H. Bach",
        "arxiv_title": "Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale",
        "arxiv_num": "1812.00417",
        "arxiv_published": "2018-12-02T16:23:36Z",
        "arxiv_updated": "2019-06-03T22:52:25Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/04/1902_00751_parameter_efficien",
        "tag": [
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/transfer_learning_in_nlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/continual_learning"
        ],
        "comment": "**Adapter tuning for NLP**.\r\n\r\n\r\nA strategy for tuning a large text model on several\r\ndownstream tasks, that permits training on\r\ntasks sequentially, and that adds only a small number\r\nof additional parameters per task.\r\n\r\nNew modules added between layers of a\r\npre-trained network. Parameters of the original network are frozen\r\nand therefore may be shared by many tasks.\r\n\r\n\r\n[GitHub](https://github.com/google-research/adapter-bert)",
        "title": "[1902.00751] Parameter-Efficient Transfer Learning for NLP",
        "relatedDoc": [],
        "creationTime": "2021-04-11T13:13:13Z",
        "creationDate": "2021-04-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1902.00751"
        ],
        "arxiv_author": [
            "Andrei Giurgiu",
            "Andrea Gesmundo",
            "Bruna Morrone",
            "Mona Attariyan",
            "Sylvain Gelly",
            "Quentin de Laroussilhe",
            "Neil Houlsby",
            "Stanislaw Jastrzebski"
        ],
        "arxiv_summary": "Fine-tuning large pre-trained models is an effective transfer mechanism in\nNLP. However, in the presence of many downstream tasks, fine-tuning is\nparameter inefficient: an entire new model is required for every task. As an\nalternative, we propose transfer with adapter modules. Adapter modules yield a\ncompact and extensible model; they add only a few trainable parameters per\ntask, and new tasks can be added without revisiting previous ones. The\nparameters of the original network remain fixed, yielding a high degree of\nparameter sharing. To demonstrate adapter's effectiveness, we transfer the\nrecently proposed BERT Transformer model to 26 diverse text classification\ntasks, including the GLUE benchmark. Adapters attain near state-of-the-art\nperformance, whilst adding only a few parameters per task. On GLUE, we attain\nwithin 0.4% of the performance of full fine-tuning, adding only 3.6% parameters\nper task. By contrast, fine-tuning trains 100% of the parameters per task.",
        "arxiv_firstAuthor": "Neil Houlsby",
        "arxiv_title": "Parameter-Efficient Transfer Learning for NLP",
        "arxiv_num": "1902.00751",
        "arxiv_published": "2019-02-02T16:29:47Z",
        "arxiv_updated": "2019-06-13T17:48:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1806.04470",
        "tag": [
            "http://www.semanlink.net/tag/sequence_labeling",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "design challenges of constructing effective and efficient neural sequence labeling systems",
        "title": "[1806.04470] Design Challenges and Misconceptions in Neural Sequence Labeling",
        "relatedDoc": [],
        "creationTime": "2018-06-28T01:21:31Z",
        "creationDate": "2018-06-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Shuailong Liang",
            "Yue Zhang",
            "Jie Yang"
        ],
        "arxiv_summary": "We investigate the design challenges of constructing effective and efficient\nneural sequence labeling systems, by reproducing twelve neural sequence\nlabeling models, which include most of the state-of-the-art structures, and\nconduct a systematic model comparison on three benchmarks (i.e. NER, Chunking,\nand POS tagging). Misconceptions and inconsistent conclusions in existing\nliterature are examined and clarified under statistical experiments. In the\ncomparison and analysis process, we reach several practical conclusions which\ncan be useful to practitioners.",
        "arxiv_firstAuthor": "Jie Yang",
        "arxiv_title": "Design Challenges and Misconceptions in Neural Sequence Labeling",
        "arxiv_num": "1806.04470",
        "arxiv_published": "2018-06-12T12:43:42Z",
        "arxiv_updated": "2018-07-12T09:31:10Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/11/_1807_00082_amanuensis_the_pr",
        "tag": [
            "http://www.semanlink.net/tag/computational_neuroscience",
            "http://www.semanlink.net/tag/personal_assistant",
            "http://www.semanlink.net/tag/ml_google",
            "http://www.semanlink.net/tag/ai_stanford",
            "http://www.semanlink.net/tag/artificial_human_intelligence",
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/human_ai_collaboration",
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation",
            "http://www.semanlink.net/tag/global_workspace_theory",
            "http://www.semanlink.net/tag/connectionist_vs_symbolic_debate",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neuroscience_and_ai",
            "http://www.semanlink.net/tag/consciousness_prior"
        ],
        "comment": "**The use of natural language to facilitate communication\r\nbetween the expert programmer and apprentice AI system.**\r\n\r\n> an overview of the material covered in a course taught at Stanford in the spring quarter of 2018. The course draws upon **insight from cognitive and systems neuroscience to implement hybrid connectionist and symbolic reasoning systems** that leverage and extend the state of the art in machine learning **by integrating human and machine intelligence**. As a concrete example we focus on digital assistants that learn from continuous dialog with an expert software engineer while providing initial value as powerful analytical, computational and mathematical savants.\r\n\r\n> [#Dehaene](/tag/stanislas_dehaene)'s work extends the [#Global Workspace Theory](/tag/global_workspace_theory) of Bernard Baars. Dehaene\u2019s version of the theory combined with Yoshua Bengio\u2019s concept of a [#consciousness prior](/tag/consciousness_prior.html) and deep reinforcement learning suggest a model for constructing and maintaining the cognitive states that arise and persist during complex problem solving.",
        "title": "[1807.00082] Amanuensis: The Programmer's Apprentice",
        "relatedDoc": [],
        "creationTime": "2019-11-12T16:25:10Z",
        "creationDate": "2019-11-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/1807.00082"
        ],
        "arxiv_author": [
            "Sophia Sanchez",
            "Nate Gruver",
            "Thomas Dean",
            "Marcus Gomez",
            "Michael Smith",
            "Catherine Wong",
            "Michelle Lam",
            "Peter Lu",
            "Yousef Hindy",
            "Rohun Saxena",
            "Lucy Wang",
            "Maurice Chiang"
        ],
        "arxiv_summary": "This document provides an overview of the material covered in a course taught\nat Stanford in the spring quarter of 2018. The course draws upon insight from\ncognitive and systems neuroscience to implement hybrid connectionist and\nsymbolic reasoning systems that leverage and extend the state of the art in\nmachine learning by integrating human and machine intelligence. As a concrete\nexample we focus on digital assistants that learn from continuous dialog with\nan expert software engineer while providing initial value as powerful\nanalytical, computational and mathematical savants. Over time these savants\nlearn cognitive strategies (domain-relevant problem solving skills) and develop\nintuitions (heuristics and the experience necessary for applying them) by\nlearning from their expert associates. By doing so these savants elevate their\ninnate analytical skills allowing them to partner on an equal footing as\nversatile collaborators - effectively serving as cognitive extensions and\ndigital prostheses, thereby amplifying and emulating their human partner's\nconceptually-flexible thinking patterns and enabling improved access to and\ncontrol over powerful computing resources.",
        "arxiv_firstAuthor": "Thomas Dean",
        "arxiv_title": "Amanuensis: The Programmer's Apprentice",
        "arxiv_num": "1807.00082",
        "arxiv_published": "2018-06-29T22:59:08Z",
        "arxiv_updated": "2018-11-08T13:33:18Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/02/1911_03876_dynamic_neuro_symb",
        "tag": [
            "http://www.semanlink.net/tag/allennlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/commonsense_question_answering",
            "http://www.semanlink.net/tag/zero_shot"
        ],
        "comment": "",
        "title": "[1911.03876] Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
        "relatedDoc": [],
        "creationTime": "2021-02-08T13:48:51Z",
        "creationDate": "2021-02-08",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.03876"
        ],
        "arxiv_author": [
            "Antoine Bosselut",
            "Yejin Choi",
            "Ronan Le Bras"
        ],
        "arxiv_summary": "Understanding narratives requires reasoning about implicit world knowledge\nrelated to the causes, effects, and states of situations described in text. At\nthe core of this challenge is how to access contextually relevant knowledge on\ndemand and reason over it.\nIn this paper, we present initial studies toward zero-shot commonsense\nquestion answering by formulating the task as inference over dynamically\ngenerated commonsense knowledge graphs. In contrast to previous studies for\nknowledge integration that rely on retrieval of existing knowledge from static\nknowledge graphs, our study requires commonsense knowledge integration where\ncontextually relevant knowledge is often not present in existing knowledge\nbases. Therefore, we present a novel approach that generates\ncontextually-relevant symbolic knowledge structures on demand using generative\nneural commonsense knowledge models.\nEmpirical results on two datasets demonstrate the efficacy of our\nneuro-symbolic approach for dynamically constructing knowledge graphs for\nreasoning. Our approach achieves significant performance boosts over pretrained\nlanguage models and vanilla knowledge models, all while providing interpretable\nreasoning paths for its predictions.",
        "arxiv_firstAuthor": "Antoine Bosselut",
        "arxiv_title": "Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
        "arxiv_num": "1911.03876",
        "arxiv_published": "2019-11-10T08:20:20Z",
        "arxiv_updated": "2020-10-30T07:30:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/04/2012_02558_pre_trained_langua",
        "tag": [
            "http://www.semanlink.net/tag/language_models_as_knowledge_bases",
            "http://www.semanlink.net/tag/porsche",
            "http://www.semanlink.net/tag/automobile",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2012.02558] Pre-trained language models as knowledge bases for Automotive Complaint Analysis",
        "relatedDoc": [],
        "creationTime": "2021-04-11T09:30:04Z",
        "creationDate": "2021-04-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2012.02558"
        ],
        "arxiv_author": [
            "M. A\u00dfenmacher",
            "V. D. Viellieber"
        ],
        "arxiv_summary": "Recently it has been shown that large pre-trained language models like BERT\n(Devlin et al., 2018) are able to store commonsense factual knowledge captured\nin its pre-training corpus (Petroni et al., 2019). In our work we further\nevaluate this ability with respect to an application from industry creating a\nset of probes specifically designed to reveal technical quality issues captured\nas described incidents out of unstructured customer feedback in the automotive\nindustry. After probing the out-of-the-box versions of the pre-trained models\nwith fill-in-the-mask tasks we dynamically provide it with more knowledge via\ncontinual pre-training on the Office of Defects Investigation (ODI) Complaints\ndata set. In our experiments the models exhibit performance regarding queries\non domain-specific topics compared to when queried on factual knowledge itself,\nas Petroni et al. (2019) have done. For most of the evaluated architectures the\ncorrect token is predicted with a $Precision@1$ ($P@1$) of above 60\\%, while\nfor $P@5$ and $P@10$ even values of well above 80\\% and up to 90\\% respectively\nare reached. These results show the potential of using language models as a\nknowledge base for structured analysis of customer feedback.",
        "arxiv_firstAuthor": "V. D. Viellieber",
        "arxiv_title": "Pre-trained language models as knowledge bases for Automotive Complaint Analysis",
        "arxiv_num": "2012.02558",
        "arxiv_published": "2020-12-04T12:49:47Z",
        "arxiv_updated": "2020-12-04T12:49:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/2006_13365_bringing_light_int",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/survey"
        ],
        "comment": "",
        "title": "[2006.13365] Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
        "relatedDoc": [],
        "creationTime": "2020-06-26T16:33:57Z",
        "creationDate": "2020-06-26",
        "bookmarkOf": [
            "https://arxiv.org/abs/2006.13365"
        ],
        "arxiv_author": [
            "Max Berrendorf",
            "Asja Fischer",
            "Mehdi Ali",
            "Mikhail Galkin",
            "Laurent Vermue",
            "Charles Tapley Hoyt",
            "Jens Lehmann",
            "Volker Tresp",
            "Sahand Sharifzadeh"
        ],
        "arxiv_summary": "The heterogeneity in recently published knowledge graph embedding models'\nimplementations, training, and evaluation has made fair and thorough\ncomparisons difficult. In order to assess the reproducibility of previously\npublished results, we re-implemented and evaluated 19 interaction models in the\nPyKEEN software package. Here, we outline which results could be reproduced\nwith their reported hyper-parameters, which could only be reproduced with\nalternate hyper-parameters, and which could not be reproduced at all as well as\nprovide insight as to why this might be the case.\nWe then performed a large-scale benchmarking on four datasets with several\nthousands of experiments and 21,246 GPU hours of computation time. We present\ninsights gained as to best practices, best configurations for each model, and\nwhere improvements could be made over previously published best configurations.\nOur results highlight that the combination of model architecture, training\napproach, loss function, and the explicit modeling of inverse relations is\ncrucial for a model's performances, and not only determined by the model\narchitecture. We provide evidence that several architectures can obtain results\ncompetitive to the state-of-the-art when configured carefully. We have made all\ncode, experimental configurations, results, and analyses that lead to our\ninterpretations available at https://github.com/pykeen/pykeen and\nhttps://github.com/pykeen/benchmarking",
        "arxiv_firstAuthor": "Mehdi Ali",
        "arxiv_title": "Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
        "arxiv_num": "2006.13365",
        "arxiv_published": "2020-06-23T22:30:52Z",
        "arxiv_updated": "2020-06-23T22:30:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/1908_11860_adapt_or_get_left_",
        "tag": [
            "http://www.semanlink.net/tag/domain_adaptation",
            "http://www.semanlink.net/tag/aspect_target_sentiment_classification",
            "http://www.semanlink.net/tag/language_model_fine_tuning",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1908.11860] Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
        "relatedDoc": [],
        "creationTime": "2021-10-21T12:56:49Z",
        "creationDate": "2021-10-21",
        "bookmarkOf": [
            "https://arxiv.org/abs/1908.11860"
        ],
        "arxiv_author": [
            "Alexander Rietzler",
            "Paul Opitz",
            "Stefan Engl",
            "Sebastian Stabinger"
        ],
        "arxiv_summary": "Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based\nSentiment Analysis (ABSA), which has many applications e.g. in e-commerce,\nwhere data and insights from reviews can be leveraged to create value for\nbusinesses and customers. Recently, deep transfer-learning methods have been\napplied successfully to a myriad of Natural Language Processing (NLP) tasks,\nincluding ATSC. Building on top of the prominent BERT language model, we\napproach ATSC using a two-step procedure: self-supervised domain-specific BERT\nlanguage model finetuning, followed by supervised task-specific finetuning. Our\nfindings on how to best exploit domain-specific language model finetuning\nenable us to produce new state-of-the-art performance on the SemEval 2014 Task\n4 restaurants dataset. In addition, to explore the real-world robustness of our\nmodels, we perform cross-domain evaluation. We show that a cross-domain adapted\nBERT language model performs significantly better than strong baseline models\nlike vanilla BERT-base and XLNet-base. Finally, we conduct a case study to\ninterpret model prediction errors.",
        "arxiv_firstAuthor": "Alexander Rietzler",
        "arxiv_title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
        "arxiv_num": "1908.11860",
        "arxiv_published": "2019-08-30T17:44:30Z",
        "arxiv_updated": "2019-11-19T10:17:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_2001_07685_fixmatch_simplify",
        "tag": [
            "http://www.semanlink.net/tag/google_research",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/semi_supervised_learning"
        ],
        "comment": "[github](https://github.com/google-research/fixmatch)\r\n\r\n> we demonstrate the **power of a\r\nsimple combination of two common Semi-Supervised Learning methods**: consistency\r\nregularization and pseudo-labeling.\r\n\r\n1. First generates pseudo-labels using the model\u2019s\r\npredictions on weakly-augmented unlabeled images. For a\r\ngiven image, the pseudo-label is only retained if the model\r\nproduces a high-confidence prediction. \r\n2. The model is then\r\ntrained to predict the pseudo-label when fed a strongly augmented\r\nversion of the same image.",
        "title": "[2001.07685] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
        "relatedDoc": [],
        "creationTime": "2020-01-22T18:11:37Z",
        "creationDate": "2020-01-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.07685"
        ],
        "arxiv_author": [
            "Kihyuk Sohn",
            "Alex Kurakin",
            "Han Zhang",
            "Zizhao Zhang",
            "Chun-Liang Li",
            "Nicholas Carlini",
            "Colin Raffel",
            "David Berthelot",
            "Ekin D. Cubuk"
        ],
        "arxiv_summary": "Semi-supervised learning (SSL) provides an effective means of leveraging\nunlabeled data to improve a model's performance. In this paper, we demonstrate\nthe power of a simple combination of two common SSL methods: consistency\nregularization and pseudo-labeling. Our algorithm, FixMatch, first generates\npseudo-labels using the model's predictions on weakly-augmented unlabeled\nimages. For a given image, the pseudo-label is only retained if the model\nproduces a high-confidence prediction. The model is then trained to predict the\npseudo-label when fed a strongly-augmented version of the same image. Despite\nits simplicity, we show that FixMatch achieves state-of-the-art performance\nacross a variety of standard semi-supervised learning benchmarks, including\n94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just\n4 labels per class. Since FixMatch bears many similarities to existing SSL\nmethods that achieve worse performance, we carry out an extensive ablation\nstudy to tease apart the experimental factors that are most important to\nFixMatch's success. We make our code available at\nhttps://github.com/google-research/fixmatch.",
        "arxiv_firstAuthor": "Kihyuk Sohn",
        "arxiv_title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
        "arxiv_num": "2001.07685",
        "arxiv_published": "2020-01-21T18:32:27Z",
        "arxiv_updated": "2020-01-21T18:32:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/05/_1905_12149_satnet_bridging_d",
        "tag": [
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/constraint_satisfaction_problem"
        ],
        "comment": "",
        "title": "[1905.12149] SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
        "relatedDoc": [],
        "creationTime": "2019-05-31T10:38:41Z",
        "creationDate": "2019-05-31",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.12149"
        ],
        "arxiv_author": [
            "Po-Wei Wang",
            "Zico Kolter",
            "Priya L. Donti",
            "Bryan Wilder"
        ],
        "arxiv_summary": "Integrating logical reasoning within deep learning architectures has been a\nmajor goal of modern AI systems. In this paper, we propose a new direction\ntoward this goal by introducing a differentiable (smoothed) maximum\nsatisfiability (MAXSAT) solver that can be integrated into the loop of larger\ndeep learning systems. Our (approximate) solver is based upon a fast coordinate\ndescent approach to solving the semidefinite program (SDP) associated with the\nMAXSAT problem. We show how to analytically differentiate through the solution\nto this SDP and efficiently solve the associated backward pass. We demonstrate\nthat by integrating this solver into end-to-end learning systems, we can learn\nthe logical structure of challenging problems in a minimally supervised\nfashion. In particular, we show that we can learn the parity function using\nsingle-bit supervision (a traditionally hard task for deep networks) and learn\nhow to play 9x9 Sudoku solely from examples. We also solve a \"visual Sudok\"\nproblem that maps images of Sudoku puzzles to their associated logical\nsolutions by combining our MAXSAT solver with a traditional convolutional\narchitecture. Our approach thus shows promise in integrating logical structures\nwithin deep learning.",
        "arxiv_firstAuthor": "Po-Wei Wang",
        "arxiv_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
        "arxiv_num": "1905.12149",
        "arxiv_published": "2019-05-29T00:47:35Z",
        "arxiv_updated": "2019-05-29T00:47:35Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2107_00676_a_primer_on_pretra",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/low_resource_languages",
            "http://www.semanlink.net/tag/bosch",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/multilingual_language_models"
        ],
        "comment": "> MLLMs are useful for bilingual tasks, particularly\r\nin low resource scenarios.\r\n>\r\n> The surprisingly good performance of\r\nMLLMs in crosslingual transfer as well as\r\nbilingual tasks motivates the hypothesis that\r\nMLLMs are learning universal patterns. However,\r\nour survey of the studies in this space indicates that\r\nthere is no consensus yet.",
        "title": "[2107.00676] A Primer on Pretrained Multilingual Language Models",
        "relatedDoc": [],
        "creationTime": "2021-07-13T13:33:29Z",
        "creationDate": "2021-07-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/2107.00676"
        ],
        "arxiv_author": [
            "Pratyush Kumar",
            "Sumanth Doddapaneni",
            "Gowtham Ramesh",
            "Anoop Kunchukuttan",
            "Mitesh M. Khapra"
        ],
        "arxiv_summary": "Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \\textit{etc.}\nhave emerged as a viable option for bringing the power of pretraining to a\nlarge number of languages. Given their success in zero shot transfer learning,\nthere has emerged a large body of work in (i) building bigger MLLMs covering a\nlarge number of languages (ii) creating exhaustive benchmarks covering a wider\nvariety of tasks and languages for evaluating MLLMs (iii) analysing the\nperformance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks\n(iv) understanding the universal language patterns (if any) learnt by MLLMs and\n(v) augmenting the (often) limited capacity of MLLMs to improve their\nperformance on seen or even unseen languages. In this survey, we review the\nexisting literature covering the above broad areas of research pertaining to\nMLLMs. Based on our survey, we recommend some promising directions of future\nresearch.",
        "arxiv_firstAuthor": "Sumanth Doddapaneni",
        "arxiv_title": "A Primer on Pretrained Multilingual Language Models",
        "arxiv_num": "2107.00676",
        "arxiv_published": "2021-07-01T18:01:46Z",
        "arxiv_updated": "2021-07-01T18:01:46Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/2009_12030_autoeter_automate",
        "tag": [
            "http://www.semanlink.net/tag/emnlp_2020",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_type_representation"
        ],
        "comment": "head_type +relation = tail_type (Hum, mais pour une relation entre 2 entit\u00e9s de m\u00eame type ?)",
        "title": "[2009.12030] AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding",
        "relatedDoc": [],
        "creationTime": "2021-05-17T16:47:20Z",
        "creationDate": "2021-05-17",
        "bookmarkOf": [
            "https://arxiv.org/abs/2009.12030"
        ],
        "arxiv_author": [
            "Yongfei Zhang",
            "Bo Li",
            "Shiliang Pu",
            "Guanglin Niu",
            "Jingyang Li"
        ],
        "arxiv_summary": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing\nentities and relations in continuous vector spaces. Some traditional KGE models\nleveraging additional type information can improve the representation of\nentities which however totally rely on the explicit types or neglect the\ndiverse type representations specific to various relations. Besides, none of\nthe existing methods is capable of inferring all the relation patterns of\nsymmetry, inversion and composition as well as the complex properties of 1-N,\nN-1 and N-N relations, simultaneously. To explore the type information for any\nKG, we develop a novel KGE framework with Automated Entity TypE Representation\n(AutoETER), which learns the latent type embedding of each entity by regarding\neach relation as a translation operation between the types of two entities with\na relation-aware projection mechanism. Particularly, our designed automated\ntype representation learning mechanism is a pluggable module which can be\neasily incorporated with any KGE model. Besides, our approach could model and\ninfer all the relation patterns and complex relations. Experiments on four\ndatasets demonstrate the superior performance of our model compared to\nstate-of-the-art baselines on link prediction tasks, and the visualization of\ntype clustering provides clearly the explanation of type embeddings and\nverifies the effectiveness of our model.",
        "arxiv_firstAuthor": "Guanglin Niu",
        "arxiv_title": "AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding",
        "arxiv_num": "2009.12030",
        "arxiv_published": "2020-09-25T04:27:35Z",
        "arxiv_updated": "2020-10-06T13:52:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/09/_1909_01380_the_bottom_up_evol",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "[blog post](http://www.semanlink.net/doc/2019/09/evolution_of_representations_in)",
        "title": "[1909.01380] The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/09/evolution_of_representations_in"
        ],
        "creationTime": "2019-09-16T23:50:52Z",
        "creationDate": "2019-09-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.01380"
        ],
        "arxiv_author": [
            "Elena Voita",
            "Rico Sennrich",
            "Ivan Titov"
        ],
        "arxiv_summary": "We seek to understand how the representations of individual tokens and the\nstructure of the learned feature space evolve between layers in deep neural\nnetworks under different learning objectives. We focus on the Transformers for\nour analysis as they have been shown effective on various tasks, including\nmachine translation (MT), standard left-to-right language models (LM) and\nmasked language modeling (MLM). Previous work used black-box probing tasks to\nshow that the representations learned by the Transformer differ significantly\ndepending on the objective. In this work, we use canonical correlation analysis\nand mutual information estimators to study how information flows across\nTransformer layers and how this process depends on the choice of learning\nobjective. For example, as you go from bottom to top layers, information about\nthe past in left-to-right language models gets vanished and predictions about\nthe future get formed. In contrast, for MLM, representations initially acquire\ninformation about the context around the token, partially forgetting the token\nidentity and producing a more generalized token representation. The token\nidentity then gets recreated at the top MLM layers.",
        "arxiv_firstAuthor": "Elena Voita",
        "arxiv_title": "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives",
        "arxiv_num": "1909.01380",
        "arxiv_published": "2019-09-03T18:06:03Z",
        "arxiv_updated": "2019-09-03T18:06:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1812_06280_wikipedia2vec_an_",
        "tag": [
            "http://www.semanlink.net/tag/wikipedia2vec",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ikuya_yamada"
        ],
        "comment": "",
        "title": "[1812.06280] Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia",
        "relatedDoc": [],
        "creationTime": "2020-09-02T16:44:44Z",
        "creationDate": "2020-09-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1812.06280"
        ],
        "arxiv_author": [
            "Hiroyuki Shindo",
            "Yoshiyasu Takefuji",
            "Jin Sakuma",
            "Akari Asai",
            "Yuji Matsumoto",
            "Hideaki Takeda",
            "Ikuya Yamada"
        ],
        "arxiv_summary": "The embeddings of entities in a large knowledge base (e.g., Wikipedia) are\nhighly beneficial for solving various natural language tasks that involve real\nworld knowledge. In this paper, we present Wikipedia2Vec, a Python-based\nopen-source tool for learning the embeddings of words and entities from\nWikipedia. The proposed tool enables users to learn the embeddings efficiently\nby issuing a single command with a Wikipedia dump file as an argument. We also\nintroduce a web-based demonstration of our tool that allows users to visualize\nand explore the learned embeddings. In our experiments, our tool achieved a\nstate-of-the-art result on the KORE entity relatedness dataset, and competitive\nresults on various standard benchmark datasets. Furthermore, our tool has been\nused as a key component in various recent studies. We publicize the source\ncode, demonstration, and the pretrained embeddings for 12 languages at\nhttps://wikipedia2vec.github.io/.",
        "arxiv_firstAuthor": "Ikuya Yamada",
        "arxiv_title": "Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia",
        "arxiv_num": "1812.06280",
        "arxiv_published": "2018-12-15T12:51:39Z",
        "arxiv_updated": "2020-01-30T10:58:05Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_1902_10909_bert_for_joint_int",
        "tag": [
            "http://www.semanlink.net/tag/alibaba",
            "http://www.semanlink.net/tag/intent_classification_and_slot_filling",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bert"
        ],
        "comment": "> Experimental results show that our\r\nproposed joint BERT model outperforms BERT\r\nmodels modeling intent classification and slot filling\r\nseparately, demonstrating the efficacy of exploiting\r\nthe relationship between the two tasks.\r\n\r\nAdding a CRF on top of the model doesn't improve the results.",
        "title": "[1902.10909] BERT for Joint Intent Classification and Slot Filling",
        "relatedDoc": [],
        "creationTime": "2020-01-09T01:13:39Z",
        "creationDate": "2020-01-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/1902.10909"
        ],
        "arxiv_author": [
            "Zhu Zhuo",
            "Qian Chen",
            "Wen Wang"
        ],
        "arxiv_summary": "Intent classification and slot filling are two essential tasks for natural\nlanguage understanding. They often suffer from small-scale human-labeled\ntraining data, resulting in poor generalization capability, especially for rare\nwords. Recently a new language representation model, BERT (Bidirectional\nEncoder Representations from Transformers), facilitates pre-training deep\nbidirectional representations on large-scale unlabeled corpora, and has created\nstate-of-the-art models for a wide variety of natural language processing tasks\nafter simple fine-tuning. However, there has not been much effort on exploring\nBERT for natural language understanding. In this work, we propose a joint\nintent classification and slot filling model based on BERT. Experimental\nresults demonstrate that our proposed model achieves significant improvement on\nintent classification accuracy, slot filling F1, and sentence-level semantic\nframe accuracy on several public benchmark datasets, compared to the\nattention-based recurrent neural network models and slot-gated models.",
        "arxiv_firstAuthor": "Qian Chen",
        "arxiv_title": "BERT for Joint Intent Classification and Slot Filling",
        "arxiv_num": "1902.10909",
        "arxiv_published": "2019-02-28T05:54:16Z",
        "arxiv_updated": "2019-02-28T05:54:16Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/07/2004_07202_entities_as_expert",
        "tag": [
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/memory_networks",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/not_encoding_knowledge_in_language_model",
            "http://www.semanlink.net/tag/knowledge_graph_augmented_language_models",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/question_answering"
        ],
        "comment": ">  We focus on the problem of **capturing declarative knowledge in the learned parameters of a language model**...\r\n\r\n> Entities as Experts (EaE) can access distinct memories of the entities mentioned in a piece of text;\r\n\r\n> To understand the motivation for distinct and\r\nindependent entity representations: A traditional Transformer would need to build an internal representation\r\nof Charles Darwin from the words \u201cCharles\u201d\r\nand \u201cDarwin\u201d... Conversely, EAE can access\r\na dedicated representation of \u201cCharles Darwin\u201d,\r\nwhich is a memory of all of the contexts in which\r\nthis entity has previously been mentioned.... Having retrieved\r\nand re-integrated this memory it is much easier for\r\nEAE to relate the question to the answer\r\n\r\n> EaE's entity representations are learned directly from text. Correct identification, and representation, of entities is essential to EaE's performance\r\n\r\nBased on transformer architecture\r\n\r\nExtension: [Facts as Experts](doc:2020/07/2007_00849_facts_as_experts_)",
        "title": "[2004.07202] Entities as Experts: Sparse Memory Access with Entity Supervision",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/07/2007_00849_facts_as_experts_"
        ],
        "creationTime": "2020-07-11T15:09:10Z",
        "creationDate": "2020-07-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.07202"
        ],
        "arxiv_author": [
            "Nicholas FitzGerald",
            "Eunsol Choi",
            "Livio Baldini Soares",
            "Tom Kwiatkowski",
            "Thibault F\u00e9vry"
        ],
        "arxiv_summary": "We focus on the problem of capturing declarative knowledge in the learned\nparameters of a language model. We introduce a new model, Entities as Experts\n(EaE), that can access distinct memories of the entities mentioned in a piece\nof text. Unlike previous efforts to integrate entity knowledge into sequence\nmodels, EaE's entity representations are learned directly from text. These\nrepresentations capture sufficient knowledge to answer TriviaQA questions such\nas \"Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley,\nEric Roberts?\". EaE outperforms a Transformer model with $30\\times$ the\nparameters on this task. According to the Lama knowledge probes, EaE also\ncontains more factual knowledge than a similar sized Bert. We show that\nassociating parameters with specific entities means that EaE only needs to\naccess a fraction of its parameters at inference time, and we show that the\ncorrect identification, and representation, of entities is essential to EaE's\nperformance. We also argue that the discrete and independent entity\nrepresentations in EaE make it more modular and interpretable than the\nTransformer architecture on which it is based.",
        "arxiv_firstAuthor": "Thibault F\u00e9vry",
        "arxiv_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
        "arxiv_num": "2004.07202",
        "arxiv_published": "2020-04-15T17:00:05Z",
        "arxiv_updated": "2020-04-15T17:00:05Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/11/2010_01057_luke_deep_context",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ikuya_yamada",
            "http://www.semanlink.net/tag/entity_embeddings",
            "http://www.semanlink.net/tag/bert_kb",
            "http://www.semanlink.net/tag/emnlp_2020",
            "http://www.semanlink.net/tag/text_aware_kg_embedding",
            "http://www.semanlink.net/tag/self_attention"
        ],
        "comment": "> LUKE is based on bidirectional Transformer, treats words and entities in a text as independent tokens, and outputs contextualized representations of them. The representations can be used to address downstream tasks similarly to BERT. [src](https://twitter.com/ikuyamada/status/1312947499141750786)\r\n\r\n> LUKE is trained using a novel pretraining task that involves predicting randomly masked words (equivalent to BERT\u2019s masked language model) and entities in an entity-annotated corpus obtained from Wikipedia.\r\n\r\n(Hum, \u00e7a me rappelle quelque chose // TODO find where)\r\n\r\n> LUKE also uses a new *entity-aware* self-attention mechanism that considers the types of tokens (words or entities) when computing attention scores.\r\n\r\n[github](https://github.com/studio-ousia/luke), [at Hugging Face](https://twitter.com/AkariAsai/status/1389428550298525696), [doc](https://huggingface.co/transformers/model_doc/luke.html), [tweet](https://twitter.com/ikuyamada/status/1392742990586683392?s=20)",
        "title": "[2010.01057] LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
        "relatedDoc": [],
        "creationTime": "2020-11-26T16:21:30Z",
        "creationDate": "2020-11-26",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.01057"
        ],
        "arxiv_author": [
            "Yuji Matsumoto",
            "Akari Asai",
            "Hideaki Takeda",
            "Ikuya Yamada",
            "Hiroyuki Shindo"
        ],
        "arxiv_summary": "Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.",
        "arxiv_firstAuthor": "Ikuya Yamada",
        "arxiv_title": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
        "arxiv_num": "2010.01057",
        "arxiv_published": "2020-10-02T15:38:03Z",
        "arxiv_updated": "2020-10-02T15:38:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1602.04938",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/lime"
        ],
        "comment": "technique that explains the predictions of any classifier by learning an interpretable model locally around the prediction",
        "title": "[1602.04938] \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
        "relatedDoc": [],
        "creationTime": "2018-09-09T15:22:41Z",
        "creationDate": "2018-09-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Sameer Singh",
            "Marco Tulio Ribeiro",
            "Carlos Guestrin"
        ],
        "arxiv_summary": "Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.",
        "arxiv_firstAuthor": "Marco Tulio Ribeiro",
        "arxiv_title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
        "arxiv_num": "1602.04938",
        "arxiv_published": "2016-02-16T08:20:14Z",
        "arxiv_updated": "2016-08-09T17:54:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/1903_11279_graph_convolution_",
        "tag": [
            "http://www.semanlink.net/tag/information_extraction",
            "http://www.semanlink.net/tag/2d_nlp",
            "http://www.semanlink.net/tag/alibaba",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/visually_rich_documents"
        ],
        "comment": "Problem addressed in this paper: extracting\r\nthe values of pre-defined entities from Visually Rich Documents (VRDs).\r\n\r\n> Graph embeddings are trained to\r\nsummarize the context of a text segment in the\r\ndocument, and further combined with text embeddings\r\nfor entity extraction\r\n\r\n> Node embedding encodes textual\r\nfeatures, while edge embedding primarily represents\r\nvisual features\r\n\r\n> Graph convolution is applied to compute visual\r\ntext embeddings of text segments in the graph,\r\nas shown in Figure 3. Different from existing\r\nworks, we define convolution on the node-edge-node\r\ntriplets instead of on the node\r\nalone\r\n\r\n> In our model, graph convolution is defined\r\nbased on the self-attention mechanism. The idea is to compute the output hidden representation of\r\neach node by attending to its neighbors\r\n\r\n> We combine graph embeddings with token embeddings\r\nand feed them into standard BiLSTM-CRF\r\nfor entity extraction\r\n\r\n> We build an annotation system to facilitate the labeling\r\nof the ground truth data.",
        "title": "[1903.11279] Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
        "relatedDoc": [],
        "creationTime": "2020-06-16T09:27:40Z",
        "creationDate": "2020-06-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/1903.11279"
        ],
        "arxiv_author": [
            "Xiaojing Liu",
            "Qiong Zhang",
            "Feiyu Gao",
            "Huasha Zhao"
        ],
        "arxiv_summary": "Visually rich documents (VRDs) are ubiquitous in daily business and life.\nExamples are purchase receipts, insurance policy documents, custom declaration\nforms and so on. In VRDs, visual and layout information is critical for\ndocument understanding, and texts in such documents cannot be serialized into\nthe one-dimensional sequence without losing information. Classic information\nextraction models such as BiLSTM-CRF typically operate on text sequences and do\nnot incorporate visual features. In this paper, we introduce a graph\nconvolution based model to combine textual and visual information presented in\nVRDs. Graph embeddings are trained to summarize the context of a text segment\nin the document, and further combined with text embeddings for entity\nextraction. Extensive experiments have been conducted to show that our method\noutperforms BiLSTM-CRF baselines by significant margins, on two real-world\ndatasets. Additionally, ablation studies are also performed to evaluate the\neffectiveness of each component of our model.",
        "arxiv_firstAuthor": "Xiaojing Liu",
        "arxiv_title": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
        "arxiv_num": "1903.11279",
        "arxiv_published": "2019-03-27T07:47:12Z",
        "arxiv_updated": "2019-03-27T07:47:12Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1810.09164",
        "tag": [
            "http://www.semanlink.net/tag/wikidata",
            "http://www.semanlink.net/tag/entity_linking",
            "http://www.semanlink.net/tag/nlp_short_texts",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Evaluation of different deep learning **techniques to create\r\na context vector from graphs, aimed at high-accuracy NED**. (neural\r\napproach for entity disambiguation using graphs as background\r\nknowledge)\r\n\r\n> We tackle Named Entity Disambiguation (NED) by comparing entities\r\nin short sentences with Wikidata graphs. Creating a context vector\r\nfrom graphs through deep learning is a challenging problem that has\r\nnever been applied to NED. Our main contribution is to present an\r\nexperimental study of recent neural techniques, as well as a discussion\r\nabout which graph features are most important for the disambiguation\r\ntask...\r\n\r\n[published paper](https://rd.springer.com/chapter/10.1007/978-3-030-15719-7_10)\r\n\r\n\r\nIn NED, the system\r\nmust be able to generate a context for an entity in a text and an entity\r\nin a knowledge base, then correctly link the two.\r\n\r\nExplore whether representing graphs\r\nas triplets is more useful than using the full topological information of the graph \r\n",
        "title": "[1810.09164] Named Entity Disambiguation using Deep Learning on Graphs",
        "relatedDoc": [],
        "creationTime": "2019-04-26T17:37:17Z",
        "creationDate": "2019-04-26",
        "bookmarkOf": [],
        "arxiv_author": [
            "Marc Sloan",
            "Andrew D. O'Harney",
            "Alberto Cetoli",
            "Mohammad Akbari",
            "Stefano Bragaglia"
        ],
        "arxiv_summary": "We tackle \\ac{NED} by comparing entities in short sentences with \\wikidata{}\ngraphs. Creating a context vector from graphs through deep learning is a\nchallenging problem that has never been applied to \\ac{NED}. Our main\ncontribution is to present an experimental study of recent neural techniques,\nas well as a discussion about which graph features are most important for the\ndisambiguation task. In addition, a new dataset (\\wikidatadisamb{}) is created\nto allow a clean and scalable evaluation of \\ac{NED} with \\wikidata{} entries,\nand to be used as a reference in future research. In the end our results show\nthat a \\ac{Bi-LSTM} encoding of the graph triplets performs best, improving\nupon the baseline models and scoring an \\rm{F1} value of $91.6\\%$ on the\n\\wikidatadisamb{} test set",
        "arxiv_firstAuthor": "Alberto Cetoli",
        "arxiv_title": "Named Entity Disambiguation using Deep Learning on Graphs",
        "arxiv_num": "1810.09164",
        "arxiv_published": "2018-10-22T10:16:07Z",
        "arxiv_updated": "2018-10-22T10:16:07Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1803.11175",
        "tag": [
            "http://www.semanlink.net/tag/dans_deep_averaging_neural_networks",
            "http://www.semanlink.net/tag/google_research",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/ray_kurzweil",
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/transfer_learning"
        ],
        "comment": "models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks.\r\n\r\n> With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task\r\n\r\nmixes an unsupervised task using a large corpus together with the supervised SNLI task, leveraging the [#Transformer](/tag/attention_is_all_you_need) architecture",
        "title": "[1803.11175] Universal Sentence Encoder",
        "relatedDoc": [],
        "creationTime": "2018-05-29T16:50:18Z",
        "creationDate": "2018-05-29",
        "bookmarkOf": [],
        "arxiv_author": [
            "Ray Kurzweil",
            "Chris Tar",
            "Yinfei Yang",
            "Brian Strope",
            "Nicole Limtiaco",
            "Sheng-yi Kong",
            "Rhomni St. John",
            "Yun-Hsuan Sung",
            "Daniel Cer",
            "Mario Guajardo-Cespedes",
            "Steve Yuan",
            "Noah Constant",
            "Nan Hua"
        ],
        "arxiv_summary": "We present models for encoding sentences into embedding vectors that\nspecifically target transfer learning to other NLP tasks. The models are\nefficient and result in accurate performance on diverse transfer tasks. Two\nvariants of the encoding models allow for trade-offs between accuracy and\ncompute resources. For both variants, we investigate and report the\nrelationship between model complexity, resource consumption, the availability\nof transfer task training data, and task performance. Comparisons are made with\nbaselines that use word level transfer learning via pretrained word embeddings\nas well as baselines do not use any transfer learning. We find that transfer\nlearning using sentence embeddings tends to outperform word level transfer.\nWith transfer learning via sentence embeddings, we observe surprisingly good\nperformance with minimal amounts of supervised training data for a transfer\ntask. We obtain encouraging results on Word Embedding Association Tests (WEAT)\ntargeted at detecting model bias. Our pre-trained sentence encoding models are\nmade freely available for download and on TF Hub.",
        "arxiv_firstAuthor": "Daniel Cer",
        "arxiv_title": "Universal Sentence Encoder",
        "arxiv_num": "1803.11175",
        "arxiv_published": "2018-03-29T17:43:03Z",
        "arxiv_updated": "2018-04-12T17:03:44Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/07/_1907_05242_large_memory_layer",
        "tag": [
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/ludovic_denoyer",
            "http://www.semanlink.net/tag/guillaume_lample",
            "http://www.semanlink.net/tag/memory_networks",
            "http://www.semanlink.net/tag/memory_in_deep_learning",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> **a structured memory which can be easily integrated into a neural network.** The memory is very large by design and therefore significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on **product keys**, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time.\r\n\r\n> a key-value memory layer that can increase model capacity for a negligible computational cost. A 12-layer transformer with a memory outperforms a 24-layer transformer, and is 2x faster! \r\n\r\n[Implementation](/doc/2019/08/product_key_memory_pkm_minima)\r\n\r\nTODO: compare with [[2007.00849] Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge](doc:2020/07/2007_00849_facts_as_experts_)",
        "title": "[1907.05242] Large Memory Layers with Product Keys",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/07/2007_00849_facts_as_experts_",
            "http://www.semanlink.net/doc/2019/08/product_key_memory_pkm_minima"
        ],
        "creationTime": "2019-07-13T19:32:44Z",
        "creationDate": "2019-07-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/1907.05242"
        ],
        "arxiv_author": [
            "Marc'Aurelio Ranzato",
            "Herv\u00e9 J\u00e9gou",
            "Ludovic Denoyer",
            "Alexandre Sablayrolles",
            "Guillaume Lample"
        ],
        "arxiv_summary": "This paper introduces a structured memory which can be easily integrated into\na neural network. The memory is very large by design and significantly\nincreases the capacity of the architecture, by up to a billion parameters with\na negligible computational overhead. Its design and access pattern is based on\nproduct keys, which enable fast and exact nearest neighbor search. The ability\nto increase the number of parameters while keeping the same computational\nbudget lets the overall system strike a better trade-off between prediction\naccuracy and computation efficiency both at training and test time. This memory\nlayer allows us to tackle very large scale language modeling tasks. In our\nexperiments we consider a dataset with up to 30 billion words, and we plug our\nmemory layer in a state-of-the-art transformer-based architecture. In\nparticular, we found that a memory augmented model with only 12 layers\noutperforms a baseline transformer model with 24 layers, while being twice\nfaster at inference time. We release our code for reproducibility purposes.",
        "arxiv_firstAuthor": "Guillaume Lample",
        "arxiv_title": "Large Memory Layers with Product Keys",
        "arxiv_num": "1907.05242",
        "arxiv_published": "2019-07-10T14:52:12Z",
        "arxiv_updated": "2019-12-16T03:46:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1901.03136",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ip_ir_ml_ia"
        ],
        "comment": "[github](https://github.com/helmersl/patent_similarity_search)\r\n\r\nmouais\r\n\r\n",
        "title": "[1901.03136] Automating the search for a patent's prior art with a full text similarity search",
        "relatedDoc": [],
        "creationTime": "2019-02-15T15:57:01Z",
        "creationDate": "2019-02-15",
        "bookmarkOf": [],
        "arxiv_author": [
            "Tim Oppermann",
            "Franziska Biegler",
            "Franziska Horn",
            "Klaus-Robert M\u00fcller",
            "Lea Helmers"
        ],
        "arxiv_summary": "More than ever, technical inventions are the symbol of our society's advance.\nPatents guarantee their creators protection against infringement. For an\ninvention being patentable, its novelty and inventiveness have to be assessed.\nTherefore, a search for published work that describes similar inventions to a\ngiven patent application needs to be performed. Currently, this so-called\nsearch for prior art is executed with semi-automatically composed keyword\nqueries, which is not only time consuming, but also prone to errors. In\nparticular, errors may systematically arise by the fact that different keywords\nfor the same technical concepts may exist across disciplines. In this paper, a\nnovel approach is proposed, where the full text of a given patent application\nis compared to existing patents using machine learning and natural language\nprocessing techniques to automatically detect inventions that are similar to\nthe one described in the submitted document. Various state-of-the-art\napproaches for feature extraction and document comparison are evaluated. In\naddition to that, the quality of the current search process is assessed based\non ratings of a domain expert. The evaluation results show that our automated\napproach, besides accelerating the search process, also improves the search\nresults for prior art with respect to their quality.",
        "arxiv_firstAuthor": "Lea Helmers",
        "arxiv_title": "Automating the search for a patent's prior art with a full text similarity search",
        "arxiv_num": "1901.03136",
        "arxiv_published": "2019-01-10T13:04:25Z",
        "arxiv_updated": "2019-03-04T19:45:29Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1904.08398",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/nlp_text_classification"
        ],
        "comment": "",
        "title": "[1904.08398] DocBERT: BERT for Document Classification",
        "relatedDoc": [],
        "creationTime": "2019-04-18T17:26:35Z",
        "creationDate": "2019-04-18",
        "bookmarkOf": [],
        "arxiv_author": [
            "Raphael Tang",
            "Ashutosh Adhikari",
            "Achyudh Ram",
            "Jimmy Lin"
        ],
        "arxiv_summary": "We present, to our knowledge, the first application of BERT to document\nclassification. A few characteristics of the task might lead one to think that\nBERT is not the most appropriate model: syntactic structures matter less for\ncontent categories, documents can often be longer than typical BERT input, and\ndocuments often have multiple labels. Nevertheless, we show that a\nstraightforward classification model using BERT is able to achieve the state of\nthe art across four popular datasets. To address the computational expense\nassociated with BERT inference, we distill knowledge from BERT-large to small\nbidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x\nfewer parameters. The primary contribution of our paper is improved baselines\nthat can provide the foundation for future work.",
        "arxiv_firstAuthor": "Ashutosh Adhikari",
        "arxiv_title": "DocBERT: BERT for Document Classification",
        "arxiv_num": "1904.08398",
        "arxiv_published": "2019-04-17T17:55:18Z",
        "arxiv_updated": "2019-08-22T05:09:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1909_04164_knowledge_enhanced",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_augmented_language_models",
            "http://www.semanlink.net/tag/multiple_knowledge_bases",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowbert",
            "http://www.semanlink.net/tag/knowledge_graph_augmented_language_models",
            "http://www.semanlink.net/tag/emnlp_2019",
            "http://www.semanlink.net/tag/knowledge_driven_embeddings",
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/nlp_using_knowledge_graphs",
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/contextualised_word_representations",
            "http://www.semanlink.net/tag/grounded_language_learning",
            "http://www.semanlink.net/tag/kd_mkb_biblio"
        ],
        "comment": "General method to **embed multiple knowledge bases into pre-trained language models** (KB in the \r\nsense as fixed collection of entity nodes)\r\n\r\n> The key idea is to explicitly model\r\nentity spans in the input text and use an **entity\r\nlinker** to retrieve relevant entity embeddings from\r\na KB to form knowledge enhanced entity-span\r\nrepresentations.\r\n> Then,  update contextual word representations via a form of **word-to-entity attention**. \r\n> In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that **combines a small amount of entity linking supervision with a large amount of raw text**.",
        "title": "[1909.04164] Knowledge Enhanced Contextual Word Representations",
        "relatedDoc": [],
        "creationTime": "2020-05-13T01:44:51Z",
        "creationDate": "2020-05-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.04164",
            "https://www.aclweb.org/anthology/D19-1005.pdf",
            "https://www.aclweb.org/anthology/D19-1005/"
        ],
        "arxiv_author": [
            "Sameer Singh",
            "Vidur Joshi",
            "Matthew E. Peters",
            "Robert L. Logan IV",
            "Mark Neumann",
            "Roy Schwartz",
            "Noah A. Smith"
        ],
        "arxiv_summary": "Contextual word representations, typically trained on unstructured, unlabeled\ntext, do not contain any explicit grounding to real world entities and are\noften unable to remember facts about those entities. We propose a general\nmethod to embed multiple knowledge bases (KBs) into large scale models, and\nthereby enhance their representations with structured, human-curated knowledge.\nFor each KB, we first use an integrated entity linker to retrieve relevant\nentity embeddings, then update contextual word representations via a form of\nword-to-entity attention. In contrast to previous approaches, the entity\nlinkers and self-supervised language modeling objective are jointly trained\nend-to-end in a multitask setting that combines a small amount of entity\nlinking supervision with a large amount of raw text. After integrating WordNet\nand a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert)\ndemonstrates improved perplexity, ability to recall facts as measured in a\nprobing task and downstream performance on relationship extraction, entity\ntyping, and word sense disambiguation. KnowBert's runtime is comparable to\nBERT's and it scales to large KBs.",
        "arxiv_firstAuthor": "Matthew E. Peters",
        "arxiv_title": "Knowledge Enhanced Contextual Word Representations",
        "arxiv_num": "1909.04164",
        "arxiv_published": "2019-09-09T21:18:50Z",
        "arxiv_updated": "2019-10-31T00:14:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/2009_07938_type_augmented_rel",
        "tag": [
            "http://www.semanlink.net/tag/link_prediction",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ai_ibm",
            "http://www.semanlink.net/tag/knowledge_graph_completion"
        ],
        "comment": "",
        "title": "[2009.07938] Type-augmented Relation Prediction in Knowledge Graphs",
        "relatedDoc": [],
        "creationTime": "2020-09-19T10:00:31Z",
        "creationDate": "2020-09-19",
        "bookmarkOf": [
            "https://arxiv.org/abs/2009.07938"
        ],
        "arxiv_author": [
            "Tian Gao",
            "Pavan Kapanipathi",
            "Qiang Ji",
            "Kartik Talamadupula",
            "Zijun Cui"
        ],
        "arxiv_summary": "Knowledge graphs (KGs) are of great importance to many real world\napplications, but they generally suffer from incomplete information in the form\nof missing relations between entities. Knowledge graph completion (also known\nas relation prediction) is the task of inferring missing facts given existing\nones. Most of the existing work is proposed by maximizing the likelihood of\nobserved instance-level triples. Not much attention, however, is paid to the\nontological information, such as type information of entities and relations. In\nthis work, we propose a type-augmented relation prediction (TaRP) method, where\nwe apply both the type information and instance-level information for relation\nprediction. In particular, type information and instance-level information are\nencoded as prior probabilities and likelihoods of relations respectively, and\nare combined by following Bayes' rule. Our proposed TaRP method achieves\nsignificantly better performance than state-of-the-art methods on three\nbenchmark datasets: FB15K, YAGO26K-906, and DB111K-174. In addition, we show\nthat TaRP achieves significantly improved data efficiency. More importantly,\nthe type information extracted from a specific dataset can generalize well to\nother datasets through the proposed TaRP model.",
        "arxiv_firstAuthor": "Zijun Cui",
        "arxiv_title": "Type-augmented Relation Prediction in Knowledge Graphs",
        "arxiv_num": "2009.07938",
        "arxiv_published": "2020-09-16T21:14:18Z",
        "arxiv_updated": "2020-09-16T21:14:18Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/2004_09095_the_state_and_fate",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/low_resource_languages"
        ],
        "comment": "",
        "title": "[2004.09095] The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
        "relatedDoc": [],
        "creationTime": "2021-10-03T11:50:06Z",
        "creationDate": "2021-10-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.09095"
        ],
        "arxiv_author": [
            "Kalika Bali",
            "Pratik Joshi",
            "Monojit Choudhury",
            "Sebastin Santy",
            "Amar Budhiraja"
        ],
        "arxiv_summary": "Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind.",
        "arxiv_firstAuthor": "Pratik Joshi",
        "arxiv_title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
        "arxiv_num": "2004.09095",
        "arxiv_published": "2020-04-20T07:19:22Z",
        "arxiv_updated": "2021-01-27T03:39:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/11/2011_02260_graph_neural_netwo",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/recommender_systems",
            "http://www.semanlink.net/tag/graph_neural_networks",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2011.02260] Graph Neural Networks in Recommender Systems: A Survey",
        "relatedDoc": [],
        "creationTime": "2020-11-11T11:04:40Z",
        "creationDate": "2020-11-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2011.02260"
        ],
        "arxiv_author": [
            "Fei Sun",
            "Wentao Zhang",
            "Shiwen Wu",
            "Bin Cui"
        ],
        "arxiv_summary": "With the explosive growth of online information, recommender systems play a\nkey role to alleviate such information overload. Due to the important\napplication value of recommender system, there have always been emerging works\nin this field. In recent years, graph neural network (GNN) techniques have\ngained considerable interests which can naturally integrate node information\nand topological structure. Owing to the outperformance of GNN in learning on\ngraph data, GNN methods have been widely applied in many fields. In recommender\nsystems, the main challenge is to learn the efficient user/item embeddings from\ntheir interactions and side information if available. Since most of the\ninformation essentially has graph structure and GNNs have superiority in\nrepresentation learning, the field of utilizing graph neural network in\nrecommender systems is flourishing. This article aims to provide a\ncomprehensive review of recent research efforts on graph neural network based\nrecommender systems. Specifically, we provide a taxonomy of graph neural\nnetwork based recommendation models and state new perspectives pertaining to\nthe development of this field.",
        "arxiv_firstAuthor": "Shiwen Wu",
        "arxiv_title": "Graph Neural Networks in Recommender Systems: A Survey",
        "arxiv_num": "2011.02260",
        "arxiv_published": "2020-11-04T12:57:47Z",
        "arxiv_updated": "2020-11-04T12:57:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1911_02685_a_comprehensive_su",
        "tag": [
            "http://www.semanlink.net/tag/transfer_learning",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1911.02685] A Comprehensive Survey on Transfer Learning",
        "relatedDoc": [],
        "creationTime": "2020-09-24T18:41:06Z",
        "creationDate": "2020-09-24",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.02685"
        ],
        "arxiv_author": [
            "Zhiyuan Qi",
            "Yongchun Zhu",
            "Hui Xiong",
            "Keyu Duan",
            "Qing He",
            "Dongbo Xi",
            "Hengshu Zhu",
            "Fuzhen Zhuang"
        ],
        "arxiv_summary": "Transfer learning aims at improving the performance of target learners on\ntarget domains by transferring the knowledge contained in different but related\nsource domains. In this way, the dependence on a large number of target domain\ndata can be reduced for constructing target learners. Due to the wide\napplication prospects, transfer learning has become a popular and promising\narea in machine learning. Although there are already some valuable and\nimpressive surveys on transfer learning, these surveys introduce approaches in\na relatively isolated way and lack the recent advances in transfer learning.\nDue to the rapid expansion of the transfer learning area, it is both necessary\nand challenging to comprehensively review the relevant studies. This survey\nattempts to connect and systematize the existing transfer learning researches,\nas well as to summarize and interpret the mechanisms and the strategies of\ntransfer learning in a comprehensive way, which may help readers have a better\nunderstanding of the current research status and ideas. Unlike previous\nsurveys, this survey paper reviews more than forty representative transfer\nlearning approaches, especially homogeneous transfer learning approaches, from\nthe perspectives of data and model. The applications of transfer learning are\nalso briefly introduced. In order to show the performance of different transfer\nlearning models, over twenty representative transfer learning models are used\nfor experiments. The models are performed on three different datasets, i.e.,\nAmazon Reviews, Reuters-21578, and Office-31. And the experimental results\ndemonstrate the importance of selecting appropriate transfer learning models\nfor different applications in practice.",
        "arxiv_firstAuthor": "Fuzhen Zhuang",
        "arxiv_title": "A Comprehensive Survey on Transfer Learning",
        "arxiv_num": "1911.02685",
        "arxiv_published": "2019-11-07T00:15:02Z",
        "arxiv_updated": "2020-06-23T15:52:46Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/11/1911_06136_kepler_a_unified_",
        "tag": [
            "http://www.semanlink.net/tag/good_related_work_section",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/text_kg_and_embeddings",
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/discute_avec_raphael"
        ],
        "comment": "A unified model for knowledge embedding (KE, ie relational facts) and pre-trained language representation (PLM)\r\n\r\n> can not only better integrate\r\nfactual knowledge into PLMs but also\r\neffectively learn KE through the abundant\r\ninformation in text\r\n\r\n> Inspired by [Xie et al. (2016)](doc:2020/10/representation_learning_of_know) ([DKRL](tag:dkrl)), we take **entity descriptions**\r\nto bridge the gap between KE and PLM.\r\n\r\n> We encode the texts\r\nand entities into a unified semantic space with the\r\nsame PLM as the encoder, and jointly optimize the\r\nKE and the masked language modeling (MLM) objectives\r\nduring pre-training. For the KE objective,\r\nwe encode the entity descriptions as their corresponding\r\nentity embeddings, and then learn them\r\nin the same way as conventional KE methods. For\r\nthe MLM objective, we follow the approach of existing\r\nPLMs\r\n\r\n\r\n",
        "title": "[1911.06136] KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/10/representation_learning_of_know"
        ],
        "creationTime": "2020-11-03T16:41:30Z",
        "creationDate": "2020-11-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.06136"
        ],
        "arxiv_author": [
            "Xiaozhi Wang",
            "Tianyu Gao",
            "Zhaocheng Zhu",
            "Zhiyuan Liu",
            "Juanzi Li",
            "Jian Tang"
        ],
        "arxiv_summary": "Pre-trained language representation models (PLMs) cannot well capture factual\nknowledge from text. In contrast, knowledge embedding (KE) methods can\neffectively represent the relational facts in knowledge graphs (KGs) with\ninformative entity embeddings, but conventional KE models do not utilize the\nrich text data. In this paper, we propose a unified model for Knowledge\nEmbedding and Pre-trained LanguagE Representation (KEPLER), which can not only\nbetter integrate factual knowledge into PLMs but also effectively learn KE\nthrough the abundant information in text. In KEPLER, we encode textual\ndescriptions of entities with a PLM as their embeddings, and then jointly\noptimize the KE and language modeling objectives. Experimental results show\nthat KEPLER achieves state-of-the-art performance on various NLP tasks, and\nalso works remarkably well as an inductive KE model on the link prediction\ntask. Furthermore, for pre-training KEPLER and evaluating the KE performance,\nwe construct Wikidata5M, a large-scale KG dataset with aligned entity\ndescriptions, and benchmark state-of-the-art KE methods on it. It shall serve\nas a new KE benchmark and facilitate the research on large KG, inductive KE,\nand KG with text. The dataset can be obtained from\nhttps://deepgraphlearning.github.io/project/wikidata5m.",
        "arxiv_firstAuthor": "Xiaozhi Wang",
        "arxiv_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
        "arxiv_num": "1911.06136",
        "arxiv_published": "2019-11-13T05:21:45Z",
        "arxiv_updated": "2020-02-19T07:46:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1902.05196v1",
        "tag": [
            "http://www.semanlink.net/tag/categorical_variables",
            "http://www.semanlink.net/tag/category_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_text_classification"
        ],
        "comment": "> We observe that **current representation methods for categorical metadata... are not as effective as claimed** in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category",
        "title": "[1902.05196] Categorical Metadata Representation for Customized Text Classification",
        "relatedDoc": [],
        "creationTime": "2019-02-18T08:20:43Z",
        "creationDate": "2019-02-18",
        "bookmarkOf": [],
        "arxiv_author": [
            "Seung-won Hwang",
            "Sua Sung",
            "Jihyeok Kim",
            "Kyungjae Lee",
            "Reinald Kim Amplayo",
            "Minji Seo"
        ],
        "arxiv_summary": "The performance of text classification has improved tremendously using\nintelligently engineered neural-based models, especially those injecting\ncategorical metadata as additional information, e.g., using user/product\ninformation for sentiment classification. These information have been used to\nmodify parts of the model (e.g., word embeddings, attention mechanisms) such\nthat results can be customized according to the metadata. We observe that\ncurrent representation methods for categorical metadata, which are devised for\nhuman consumption, are not as effective as claimed in popular classification\nmethods, outperformed even by simple concatenation of categorical features in\nthe final layer of the sentence encoder. We conjecture that categorical\nfeatures are harder to represent for machine use, as available context only\nindirectly describes the category, and even such context is often scarce (for\ntail category). To this end, we propose to use basis vectors to effectively\nincorporate categorical metadata on various parts of a neural-based model. This\nadditionally decreases the number of parameters dramatically, especially when\nthe number of categorical features is large. Extensive experiments on various\ndatasets with different properties are performed and show that through our\nmethod, we can represent categorical metadata more effectively to customize\nparts of the model, including unexplored ones, and increase the performance of\nthe model greatly.",
        "arxiv_firstAuthor": "Jihyeok Kim",
        "arxiv_title": "Categorical Metadata Representation for Customized Text Classification",
        "arxiv_num": "1902.05196",
        "arxiv_published": "2019-02-14T03:07:53Z",
        "arxiv_updated": "2019-02-14T03:07:53Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/1906_01195_learning_attention",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_knowledge_graphs",
            "http://www.semanlink.net/tag/acl_2019"
        ],
        "comment": "[GitHub](https://github.com/deepakn97/relationPrediction) [Blog post](/doc/2020/04/deepak_nathani_%7C_pay_attention_)",
        "title": "[1906.01195] Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/04/deepak_nathani_%7C_pay_attention_"
        ],
        "creationTime": "2020-04-30T12:59:24Z",
        "creationDate": "2020-04-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.01195"
        ],
        "arxiv_author": [
            "Manohar Kaul",
            "Deepak Nathani",
            "Charu Sharma",
            "Jatin Chauhan"
        ],
        "arxiv_summary": "The recent proliferation of knowledge graphs (KGs) coupled with incomplete or\npartial information, in the form of missing relations (links) between entities,\nhas fueled a lot of research on knowledge base completion (also known as\nrelation prediction). Several recent works suggest that convolutional neural\nnetwork (CNN) based models generate richer and more expressive feature\nembeddings and hence also perform well on relation prediction. However, we\nobserve that these KG embeddings treat triples independently and thus fail to\ncover the complex and hidden information that is inherently implicit in the\nlocal neighborhood surrounding a triple. To this effect, our paper proposes a\nnovel attention based feature embedding that captures both entity and relation\nfeatures in any given entity's neighborhood. Additionally, we also encapsulate\nrelation clusters and multihop relations in our model. Our empirical study\noffers insights into the efficacy of our attention based model and we show\nmarked performance gains in comparison to state of the art methods on all\ndatasets.",
        "arxiv_firstAuthor": "Deepak Nathani",
        "arxiv_title": "Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs",
        "arxiv_num": "1906.01195",
        "arxiv_published": "2019-06-04T04:59:08Z",
        "arxiv_updated": "2019-06-04T04:59:08Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2103_11811_masakhaner_named_",
        "tag": [
            "http://www.semanlink.net/tag/nlp_4_africa",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/masakhane",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "",
        "title": "[2103.11811] MasakhaNER: Named Entity Recognition for African Languages",
        "relatedDoc": [],
        "creationTime": "2021-07-06T13:08:36Z",
        "creationDate": "2021-07-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/2103.11811"
        ],
        "arxiv_author": [
            "Bonaventure F. P. Dossou",
            "Tendai Marengereke",
            "Henok Tilaye",
            "Adewale Akinfaderin",
            "Daniel D'souza",
            "Stephen Mayhew",
            "Degaga Wolde",
            "Orevaoghene Ahia",
            "Joyce Nakatumba-Nabende",
            "Tajuddeen Gwadabe",
            "Mouhamadane MBOUP",
            "Iroro Orife",
            "Samba Ngom",
            "Victor Akinode",
            "Paul Rayson",
            "Shamsuddeen Muhammad",
            "Tobius Saul Bateesa",
            "Chiamaka Chukwuneke",
            "Rubungo Andre Niyongabo",
            "Mofetoluwa Adeyemi",
            "Verrah Otiende",
            "Maurice Katusiime",
            "Catherine Gitau",
            "Jade Abbott",
            "Derguene Mbaye",
            "Israel Abebe Azime",
            "Chester Palen-Michel",
            "Shruti Rijhwani",
            "Emmanuel Anebi",
            "David Ifeoluwa Adelani",
            "Tosin Adewumi",
            "Clemencia Siro",
            "Eric Peter Wairagala",
            "Deborah Nabagereka",
            "Anuoluwapo Aremu",
            "Temilola Oloyede",
            "Sebastian Ruder",
            "Dibora Gebreyohannes",
            "Seid Muhie Yimam",
            "Davis David",
            "Kelechi Ogueji",
            "Abdoulaye Faye",
            "Yvonne Wambui",
            "Kelechi Nwaike",
            "Abdoulaye Diallo",
            "Jesujoba Alabi",
            "Graham Neubig",
            "Gerald Muriuki",
            "Happy Buzaaba",
            "Julia Kreutzer",
            "Jonathan Mukiibi",
            "Samuel Oyerinde",
            "Chris Chinenye Emezue",
            "Ayodele Awokoya",
            "Salomey Osei",
            "Blessing Sibanda",
            "Nkiruka Odu",
            "Constantine Lignos",
            "Ignatius Ezeani",
            "Thierno Ibrahima DIOP",
            "Perez Ogayo"
        ],
        "arxiv_summary": "We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.",
        "arxiv_firstAuthor": "David Ifeoluwa Adelani",
        "arxiv_title": "MasakhaNER: Named Entity Recognition for African Languages",
        "arxiv_num": "2103.11811",
        "arxiv_published": "2021-03-22T13:12:44Z",
        "arxiv_updated": "2021-07-05T15:14:32Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_2003_02320_knowledge_graphs",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/aidan_hogan",
            "http://www.semanlink.net/tag/axel_polleres"
        ],
        "comment": "Draws together many topics & perspectives regarding Knowledge Graphs. 18 co-authors, lead by Aidan Hogan. (Regarding language models for embedding, they refer to [Wang et al. Knowledge Graph Embedding: A Survey of Approaches and Applications](/doc/2019/05/knowledge_graph_embedding_a_su))",
        "title": "[2003.02320] Knowledge Graphs",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/05/knowledge_graph_embedding_a_su"
        ],
        "creationTime": "2020-03-07T09:20:34Z",
        "creationDate": "2020-03-07",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.02320"
        ],
        "arxiv_author": [
            "Lukas Schmelzeisen",
            "Claudia d'Amato",
            "Antoine Zimmermann",
            "Eva Blomqvist",
            "Gerard de Melo",
            "Sebastian Neumaier",
            "Axel Polleres",
            "Juan Sequeda",
            "Sabbir M. Rashid",
            "Anisa Rula",
            "Steffen Staab",
            "Sabrina Kirrane",
            "Claudio Gutierrez",
            "Axel-Cyrille Ngonga Ngomo",
            "Roberto Navigli",
            "Michael Cochez",
            "Aidan Hogan",
            "Jos\u00e9 Emilio Labra Gayo"
        ],
        "arxiv_summary": "In this paper we provide a comprehensive introduction to knowledge graphs,\nwhich have recently garnered significant attention from both industry and\nacademia in scenarios that require exploiting diverse, dynamic, large-scale\ncollections of data. After a general introduction, we motivate and contrast\nvarious graph-based data models and query languages that are used for knowledge\ngraphs. We discuss the roles of schema, identity, and context in knowledge\ngraphs. We explain how knowledge can be represented and extracted using a\ncombination of deductive and inductive techniques. We summarise methods for the\ncreation, enrichment, quality assessment, refinement, and publication of\nknowledge graphs. We provide an overview of prominent open knowledge graphs and\nenterprise knowledge graphs, their applications, and how they use the\naforementioned techniques. We conclude with high-level future research\ndirections for knowledge graphs.",
        "arxiv_firstAuthor": "Aidan Hogan",
        "arxiv_title": "Knowledge Graphs",
        "arxiv_num": "2003.02320",
        "arxiv_published": "2020-03-04T20:20:32Z",
        "arxiv_updated": "2020-04-17T00:07:00Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_1711_00046_replace_or_retriev",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/string_searching_algorithm",
            "http://www.semanlink.net/tag/aho_corasick_algorithm",
            "http://www.semanlink.net/tag/regex",
            "http://www.semanlink.net/tag/flashtext_algorithm"
        ],
        "comment": "FlashText algorithm for replacing keywords or finding keywords in a given text.\r\n\r\nFor a document of size N (characters) and a dictionary of M keywords, the time complexity is O(N) (compared to O(MxN) with regex). FlashText is designed to only match complete words (words with boundary characters on both sides). **Different from Aho Corasick Algorithm, as it doesn't match substrings**. This algorithm is also **designed to go for the longest match** first. For an input dictionary {Machine, Learning, Machine learning} on a string 'I like Machine learning', it will only consider the longest match, which is Machine Learning\r\n\r\n[Github](https://github.com/vi3k6i5/flashtext) (python)",
        "title": "[1711.00046] Replace or Retrieve Keywords In Documents at Scale",
        "relatedDoc": [],
        "creationTime": "2020-01-09T16:26:49Z",
        "creationDate": "2020-01-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/1711.00046"
        ],
        "arxiv_author": [
            "Vikash Singh"
        ],
        "arxiv_summary": "In this paper we introduce, the FlashText algorithm for replacing keywords or\nfinding keywords in a given text. FlashText can search or replace keywords in\none pass over a document. The time complexity of this algorithm is not\ndependent on the number of terms being searched or replaced. For a document of\nsize N (characters) and a dictionary of M keywords, the time complexity will be\nO(N). This algorithm is much faster than Regex, because regex time complexity\nis O(MxN). It is also different from Aho Corasick Algorithm, as it doesn't\nmatch substrings. FlashText is designed to only match complete words (words\nwith boundary characters on both sides). For an input dictionary of {Apple},\nthis algorithm won't match it to 'I like Pineapple'. This algorithm is also\ndesigned to go for the longest match first. For an input dictionary {Machine,\nLearning, Machine learning} on a string 'I like Machine learning', it will only\nconsider the longest match, which is Machine Learning. We have made python\nimplementation of this algorithm available as open-source on GitHub, released\nunder the permissive MIT License.",
        "arxiv_firstAuthor": "Vikash Singh",
        "arxiv_title": "Replace or Retrieve Keywords In Documents at Scale",
        "arxiv_num": "1711.00046",
        "arxiv_published": "2017-10-31T18:34:03Z",
        "arxiv_updated": "2017-11-09T18:56:44Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1810_04882_towards_understand",
        "tag": [
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1810.04882] Towards Understanding Linear Word Analogies",
        "relatedDoc": [],
        "creationTime": "2019-06-24T08:33:44Z",
        "creationDate": "2019-06-24",
        "bookmarkOf": [
            "https://arxiv.org/abs/1810.04882"
        ],
        "arxiv_author": [
            "Graeme Hirst",
            "David Duvenaud",
            "Kawin Ethayarajh"
        ],
        "arxiv_summary": "A surprising property of word vectors is that word analogies can often be\nsolved with vector arithmetic. However, it is unclear why arithmetic operators\ncorrespond to non-linear embedding models such as skip-gram with negative\nsampling (SGNS). We provide a formal explanation of this phenomenon without\nmaking the strong assumptions that past theories have made about the vector\nspace and word distribution. Our theory has several implications. Past work has\nconjectured that linear substructures exist in vector spaces because relations\ncan be represented as ratios; we prove that this holds for SGNS. We provide\nnovel justification for the addition of SGNS word vectors by showing that it\nautomatically down-weights the more frequent word, as weighting schemes do ad\nhoc. Lastly, we offer an information theoretic interpretation of Euclidean\ndistance in vector spaces, justifying its use in capturing word dissimilarity.",
        "arxiv_firstAuthor": "Kawin Ethayarajh",
        "arxiv_title": "Towards Understanding Linear Word Analogies",
        "arxiv_num": "1810.04882",
        "arxiv_published": "2018-10-11T08:08:40Z",
        "arxiv_updated": "2019-08-12T04:04:15Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1512.00765",
        "tag": [
            "http://www.semanlink.net/tag/nlp_short_texts",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/tf_idf"
        ],
        "comment": "In order to pair short text\r\nfragments\u2014as a concatenation of separate words\u2014an adequate\r\ndistributed sentence representation is needed. Main contribution: a first step towards a hybrid method that\r\ncombines the strength of dense distributed representations\u2014\r\nas opposed to sparse term matching\u2014with the strength of\r\ntf-idf based methods. The combination of word embeddings and tf-idf\r\ninformation might lead to a better model for semantic content\r\nwithin very short text fragments.",
        "title": "[1512.00765] Learning Semantic Similarity for Very Short Texts",
        "relatedDoc": [],
        "creationTime": "2017-06-09T14:51:21Z",
        "creationDate": "2017-06-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Thomas Demeester",
            "Steven Van Canneyt",
            "Steven Bohez",
            "Cedric De Boom",
            "Bart Dhoedt"
        ],
        "arxiv_summary": "Levering data on social media, such as Twitter and Facebook, requires\ninformation retrieval algorithms to become able to relate very short text\nfragments to each other. Traditional text similarity methods such as tf-idf\ncosine-similarity, based on word overlap, mostly fail to produce good results\nin this case, since word overlap is little or non-existent. Recently,\ndistributed word representations, or word embeddings, have been shown to\nsuccessfully allow words to match on the semantic level. In order to pair short\ntext fragments - as a concatenation of separate words - an adequate distributed\nsentence representation is needed, in existing literature often obtained by\nnaively combining the individual word representations. We therefore\ninvestigated several text representations as a combination of word embeddings\nin the context of semantic pair matching. This paper investigates the\neffectiveness of several such naive techniques, as well as traditional tf-idf\nsimilarity, for fragments of different lengths. Our main contribution is a\nfirst step towards a hybrid method that combines the strength of dense\ndistributed representations - as opposed to sparse term matching - with the\nstrength of tf-idf based methods to automatically reduce the impact of less\ninformative terms. Our new approach outperforms the existing techniques in a\ntoy experimental set-up, leading to the conclusion that the combination of word\nembeddings and tf-idf information might lead to a better model for semantic\ncontent within very short text fragments.",
        "arxiv_firstAuthor": "Cedric De Boom",
        "arxiv_title": "Learning Semantic Similarity for Very Short Texts",
        "arxiv_num": "1512.00765",
        "arxiv_published": "2015-12-02T16:31:20Z",
        "arxiv_updated": "2015-12-02T16:31:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/2104_14690_entailment_as_few_",
        "tag": [
            "http://www.semanlink.net/tag/few_shot_learning",
            "http://www.semanlink.net/tag/data_augmentation",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entailment"
        ],
        "comment": "> a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples\r\n>\r\n> For instance, we can reformulate a sentiment classification task as a textual entailment one\r\nwith an input sentence S1 as\r\nxin = [CLS]S1[SEP]S2[EOS]; where S2 = This indicates positive user sentiment, \r\nand let the language modelMto determine the if input sentence S1 entails the label description S2",
        "title": "[2104.14690] Entailment as Few-Shot Learner",
        "relatedDoc": [],
        "creationTime": "2021-05-03T23:05:39Z",
        "creationDate": "2021-05-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2104.14690"
        ],
        "arxiv_author": [
            "Madian Khabsa",
            "Hao Ma",
            "Sinong Wang",
            "Hanzi Mao",
            "Han Fang"
        ],
        "arxiv_summary": "Large pre-trained language models (LMs) have demonstrated remarkable ability\nas few-shot learners. However, their success hinges largely on scaling model\nparameters to a degree that makes it challenging to train and serve. In this\npaper, we propose a new approach, named as EFL, that can turn small LMs into\nbetter few-shot learners. The key idea of this approach is to reformulate\npotential NLP task into an entailment one, and then fine-tune the model with as\nlittle as 8 examples. We further demonstrate our proposed method can be: (i)\nnaturally combined with an unsupervised contrastive learning-based data\naugmentation method; (ii) easily extended to multilingual few-shot learning. A\nsystematic evaluation on 18 standard NLP tasks demonstrates that this approach\nimproves the various existing SOTA few-shot learning methods by 12\\%, and\nyields competitive few-shot performance with 500 times larger models, such as\nGPT-3.",
        "arxiv_firstAuthor": "Sinong Wang",
        "arxiv_title": "Entailment as Few-Shot Learner",
        "arxiv_num": "2104.14690",
        "arxiv_published": "2021-04-29T22:52:26Z",
        "arxiv_updated": "2021-04-29T22:52:26Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1908_10084_sentence_bert_sen",
        "tag": [
            "http://www.semanlink.net/tag/sentence_similarity",
            "http://www.semanlink.net/tag/nearest_neighbor_search",
            "http://www.semanlink.net/tag/sbert",
            "http://www.semanlink.net/tag/emnlp_2019",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/huggingface_transformers",
            "http://www.semanlink.net/tag/siamese_network"
        ],
        "comment": "> Sentence-BERT\r\n(SBERT), a modification of the pretrained\r\nBERT network that use siamese and triplet network\r\nstructures to derive **semantically meaningful\r\nsentence embeddings** that can be compared\r\nusing cosine-similarity.\r\n\r\nImportant because \r\n\r\n- BERT ist unsuitable for semantic similarity\r\nsearch as well as for unsupervised tasks\r\nlike clustering.\r\n- simple methods such as using the CLS token give low quality sentence embeddings\r\n\r\nHowever, the purpose of SBERT sentence embeddings\r\nare **not to be used for transfer learning for other\r\ntasks**.\r\n\r\n[Related blog post](/doc/2020/01/richer_sentence_embeddings_usin); [Github](https://github.com/UKPLab/sentence-transformers)",
        "title": "[1908.10084] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/07/ukplab_sentence_transformers_s",
            "http://www.semanlink.net/doc/2020/01/richer_sentence_embeddings_usin"
        ],
        "creationTime": "2019-08-28T22:41:55Z",
        "creationDate": "2019-08-28",
        "bookmarkOf": [
            "https://arxiv.org/abs/1908.10084"
        ],
        "arxiv_author": [
            "Iryna Gurevych",
            "Nils Reimers"
        ],
        "arxiv_summary": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new\nstate-of-the-art performance on sentence-pair regression tasks like semantic\ntextual similarity (STS). However, it requires that both sentences are fed into\nthe network, which causes a massive computational overhead: Finding the most\nsimilar pair in a collection of 10,000 sentences requires about 50 million\ninference computations (~65 hours) with BERT. The construction of BERT makes it\nunsuitable for semantic similarity search as well as for unsupervised tasks\nlike clustering.\nIn this publication, we present Sentence-BERT (SBERT), a modification of the\npretrained BERT network that use siamese and triplet network structures to\nderive semantically meaningful sentence embeddings that can be compared using\ncosine-similarity. This reduces the effort for finding the most similar pair\nfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while\nmaintaining the accuracy from BERT.\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning\ntasks, where it outperforms other state-of-the-art sentence embeddings methods.",
        "arxiv_firstAuthor": "Nils Reimers",
        "arxiv_title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "arxiv_num": "1908.10084",
        "arxiv_published": "2019-08-27T08:50:17Z",
        "arxiv_updated": "2019-08-27T08:50:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1709.08568",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning_attention",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/yoshua_bengio",
            "http://www.semanlink.net/tag/representation_learning",
            "http://www.semanlink.net/tag/thought_vector",
            "http://www.semanlink.net/tag/consciousness_prior",
            "http://www.semanlink.net/tag/conscience_artificielle",
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/human_level_ai"
        ],
        "comment": "\"consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., **consciousness as awareness at a particular time instant**\": the projection of a big vector (all the things conscious and unconscious in brain). Attention: additional mechanism describing what mind chooses to focus on.\r\n\r\n[YouTube video](/doc/?uri=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYr1mOzC93xs)",
        "title": "[1709.08568] The Consciousness Prior",
        "relatedDoc": [
            "https://www.youtube.com/watch?v=Yr1mOzC93xs"
        ],
        "creationTime": "2017-09-29T14:44:19Z",
        "creationDate": "2017-09-29",
        "bookmarkOf": [],
        "arxiv_author": [
            "Yoshua Bengio"
        ],
        "arxiv_summary": "A new prior is proposed for learning representations of high-level concepts\nof the kind we manipulate with language. This prior can be combined with other\npriors in order to help disentangling abstract factors from each other. It is\ninspired by cognitive neuroscience theories of consciousness, seen as a\nbottleneck through which just a few elements, after having been selected by\nattention from a broader pool, are then broadcast and condition further\nprocessing, both in perception and decision-making. The set of recently\nselected elements one becomes aware of is seen as forming a low-dimensional\nconscious state. This conscious state is combining the few concepts\nconstituting a conscious thought, i.e., what one is immediately conscious of at\na particular moment. We claim that this architectural and\ninformation-processing constraint corresponds to assumptions about the joint\ndistribution between high-level concepts. To the extent that these assumptions\nare generally true (and the form of natural language seems consistent with\nthem), they can form a useful prior for representation learning. A\nlow-dimensional thought or conscious state is analogous to a sentence: it\ninvolves only a few variables and yet can make a statement with very high\nprobability of being true. This is consistent with a joint distribution (over\nhigh-level concepts) which has the form of a sparse factor graph, i.e., where\nthe dependencies captured by each factor of the factor graph involve only very\nfew variables while creating a strong dip in the overall energy function. The\nconsciousness prior also makes it natural to map conscious states to natural\nlanguage utterances or to express classical AI knowledge in a form similar to\nfacts and rules, albeit capturing uncertainty as well as efficient search\nmechanisms implemented by attention mechanisms.",
        "arxiv_firstAuthor": "Yoshua Bengio",
        "arxiv_title": "The Consciousness Prior",
        "arxiv_num": "1709.08568",
        "arxiv_published": "2017-09-25T15:59:11Z",
        "arxiv_updated": "2019-12-02T22:53:39Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/1903_04197_structured_knowled",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/pixelwise_dense_prediction"
        ],
        "comment": "",
        "title": "[1903.04197] Structured Knowledge Distillation for Dense Prediction",
        "relatedDoc": [],
        "creationTime": "2020-04-16T14:13:03Z",
        "creationDate": "2020-04-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/1903.04197"
        ],
        "arxiv_author": [
            "Jingdong Wang",
            "Chunhua Shen",
            "Changyong Shun",
            "Yifan Liu"
        ],
        "arxiv_summary": "In this paper, we consider transferring the structure information from large\nnetworks to small ones for dense prediction tasks. Previous knowledge\ndistillation strategies used for dense prediction tasks often directly borrow\nthe distillation scheme for image classification and perform knowledge\ndistillation for each pixel separately, leading to sub-optimal performance.\nHere we propose to distill structured knowledge from large networks to small\nnetworks, taking into account the fact that dense prediction is a structured\nprediction problem. Specifically, we study two structured distillation schemes:\ni)pair-wise distillation that distills the pairwise similarities by building a\nstatic graph, and ii)holistic distillation that uses adversarial training to\ndistill holistic knowledge. The effectiveness of our knowledge distillation\napproaches is demonstrated by extensive experiments on three dense prediction\ntasks: semantic segmentation, depth estimation, and object detection.",
        "arxiv_firstAuthor": "Yifan Liu",
        "arxiv_title": "Structured Knowledge Distillation for Dense Prediction",
        "arxiv_num": "1903.04197",
        "arxiv_published": "2019-03-11T10:05:09Z",
        "arxiv_updated": "2020-02-20T23:52:50Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1912_08422_distilling_structu",
        "tag": [
            "http://www.semanlink.net/tag/ai_facebook",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/recommender_systems",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/kd_mkb_related",
            "http://www.semanlink.net/tag/explainable_ai"
        ],
        "comment": "distilling structured knowledge from a differentiable path-based recommendation model.\r\n\r\n> proposed framework can achieve state-of-the-art recommendation performance and meanwhile provide interpretable recommendation reasons",
        "title": "[1912.08422] Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation",
        "relatedDoc": [],
        "creationTime": "2020-05-12T11:11:16Z",
        "creationDate": "2020-05-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/1912.08422"
        ],
        "arxiv_author": [
            "Hanning Zhou",
            "Xiaoran Xu",
            "Yuan Zhang",
            "Yan Zhang"
        ],
        "arxiv_summary": "Recently, the embedding-based recommendation models (e.g., matrix\nfactorization and deep models) have been prevalent in both academia and\nindustry due to their effectiveness and flexibility. However, they also have\nsuch intrinsic limitations as lacking explainability and suffering from data\nsparsity. In this paper, we propose an end-to-end joint learning framework to\nget around these limitations without introducing any extra overhead by\ndistilling structured knowledge from a differentiable path-based recommendation\nmodel. Through extensive experiments, we show that our proposed framework can\nachieve state-of-the-art recommendation performance and meanwhile provide\ninterpretable recommendation reasons.",
        "arxiv_firstAuthor": "Yuan Zhang",
        "arxiv_title": "Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation",
        "arxiv_num": "1912.08422",
        "arxiv_published": "2019-12-18T07:43:52Z",
        "arxiv_updated": "2019-12-18T07:43:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2102_11107_towards_causal_rep",
        "tag": [
            "http://www.semanlink.net/tag/machine_learning",
            "http://www.semanlink.net/tag/causal_inference",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/yoshua_bengio"
        ],
        "comment": "This article reviews fundamental concepts of causal inference and relates them to crucial open problems of machine learning, including transfer learning and generalization, thereby assaying how causality can contribute to modern machine learning research\r\n\r\nRelated: [Making sense of raw input](doc:2021/05/making_sense_of_raw_input)",
        "title": "[2102.11107] Towards Causal Representation Learning",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/05/making_sense_of_raw_input"
        ],
        "creationTime": "2021-07-15T00:29:21Z",
        "creationDate": "2021-07-15",
        "bookmarkOf": [
            "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363924",
            "https://arxiv.org/abs/2102.11107"
        ],
        "arxiv_author": [
            "Yoshua Bengio",
            "Nal Kalchbrenner",
            "Stefan Bauer",
            "Anirudh Goyal",
            "Nan Rosemary Ke",
            "Bernhard Sch\u00f6lkopf",
            "Francesco Locatello"
        ],
        "arxiv_summary": "The two fields of machine learning and graphical causality arose and\ndeveloped separately. However, there is now cross-pollination and increasing\ninterest in both fields to benefit from the advances of the other. In the\npresent paper, we review fundamental concepts of causal inference and relate\nthem to crucial open problems of machine learning, including transfer and\ngeneralization, thereby assaying how causality can contribute to modern machine\nlearning research. This also applies in the opposite direction: we note that\nmost work in causality starts from the premise that the causal variables are\ngiven. A central problem for AI and causality is, thus, causal representation\nlearning, the discovery of high-level causal variables from low-level\nobservations. Finally, we delineate some implications of causality for machine\nlearning and propose key research areas at the intersection of both\ncommunities.",
        "arxiv_firstAuthor": "Bernhard Sch\u00f6lkopf",
        "arxiv_title": "Towards Causal Representation Learning",
        "arxiv_num": "2102.11107",
        "arxiv_published": "2021-02-22T15:26:57Z",
        "arxiv_updated": "2021-02-22T15:26:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/2103_12953_supporting_cluster",
        "tag": [
            "http://www.semanlink.net/tag/short_text_clustering",
            "http://www.semanlink.net/tag/contrastive_learning",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "leverages contrastive learning to promote better separation between clusters\r\n\r\n(refers to [Hadifar 2019](doc:2021/05/a_self_training_approach_for_sh))",
        "title": "[2103.12953] Supporting Clustering with Contrastive Learning",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/05/a_self_training_approach_for_sh"
        ],
        "creationTime": "2021-05-20T16:55:29Z",
        "creationDate": "2021-05-20",
        "bookmarkOf": [
            "https://arxiv.org/abs/2103.12953"
        ],
        "arxiv_author": [
            "Bing Xiang",
            "Xiaokai Wei",
            "Feng Nan",
            "Kathleen McKeown",
            "Ramesh Nallapati",
            "Shangwen Li",
            "Dejiao Zhang",
            "Henghui Zhu",
            "Andrew Arnold"
        ],
        "arxiv_summary": "Unsupervised clustering aims at discovering the semantic categories of data\naccording to some distance measured in the representation space. However,\ndifferent categories often overlap with each other in the representation space\nat the beginning of the learning process, which poses a significant challenge\nfor distance-based clustering in achieving good separation between different\ncategories. To this end, we propose Supporting Clustering with Contrastive\nLearning (SCCL) -- a novel framework to leverage contrastive learning to\npromote better separation. We assess the performance of SCCL on short text\nclustering and show that SCCL significantly advances the state-of-the-art\nresults on most benchmark datasets with 3%-11% improvement on Accuracy and\n4%-15% improvement on Normalized Mutual Information. Furthermore, our\nquantitative analysis demonstrates the effectiveness of SCCL in leveraging the\nstrengths of both bottom-up instance discrimination and top-down clustering to\nachieve better intra-cluster and inter-cluster distances when evaluated with\nthe ground truth cluster labels",
        "arxiv_firstAuthor": "Dejiao Zhang",
        "arxiv_title": "Supporting Clustering with Contrastive Learning",
        "arxiv_num": "2103.12953",
        "arxiv_published": "2021-03-24T03:05:17Z",
        "arxiv_updated": "2021-03-24T03:05:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1902.05309v1",
        "tag": [
            "http://www.semanlink.net/tag/sequence_labeling",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/transfer_learning"
        ],
        "comment": "use-case ex: NER when the target data contains new categories",
        "title": "[1902.05309] Transfer Learning for Sequence Labeling Using Source Model and Target Data",
        "relatedDoc": [],
        "creationTime": "2019-02-18T08:30:22Z",
        "creationDate": "2019-02-18",
        "bookmarkOf": [],
        "arxiv_author": [
            "Lingzhen Chen",
            "Alessandro Moschitti"
        ],
        "arxiv_summary": "In this paper, we propose an approach for transferring the knowledge of a\nneural model for sequence labeling, learned from the source domain, to a new\nmodel trained on a target domain, where new label categories appear. Our\ntransfer learning (TL) techniques enable to adapt the source model using the\ntarget data and new categories, without accessing to the source data. Our\nsolution consists in adding new neurons in the output layer of the target model\nand transferring parameters from the source model, which are then fine-tuned\nwith the target data. Additionally, we propose a neural adapter to learn the\ndifference between the source and the target label distribution, which provides\nadditional important information to the target model. Our experiments on Named\nEntity Recognition show that (i) the learned knowledge in the source model can\nbe effectively transferred when the target data contains new categories and\n(ii) our neural adapter further improves such transfer.",
        "arxiv_firstAuthor": "Lingzhen Chen",
        "arxiv_title": "Transfer Learning for Sequence Labeling Using Source Model and Target Data",
        "arxiv_num": "1902.05309",
        "arxiv_published": "2019-02-14T11:40:58Z",
        "arxiv_updated": "2019-02-14T11:40:58Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/2001_04451_reformer_the_effi",
        "tag": [
            "http://www.semanlink.net/tag/google_research",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/reformer"
        ],
        "comment": "",
        "title": "[2001.04451] Reformer: The Efficient Transformer",
        "relatedDoc": [],
        "creationTime": "2020-06-29T19:04:03Z",
        "creationDate": "2020-06-29",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.04451"
        ],
        "arxiv_author": [
            "Nikita Kitaev",
            "Anselm Levskaya",
            "\u0141ukasz Kaiser"
        ],
        "arxiv_summary": "Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O($L^2$) to\nO($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of $N$ times,\nwhere $N$ is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences.",
        "arxiv_firstAuthor": "Nikita Kitaev",
        "arxiv_title": "Reformer: The Efficient Transformer",
        "arxiv_num": "2001.04451",
        "arxiv_published": "2020-01-13T18:38:28Z",
        "arxiv_updated": "2020-02-18T16:01:18Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2006_07264_low_resource_langu",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/low_resource_languages",
            "http://www.semanlink.net/tag/survey"
        ],
        "comment": "bof",
        "title": "[2006.07264] Low-resource Languages: A Review of Past Work and Future Challenges",
        "relatedDoc": [],
        "creationTime": "2021-07-06T13:07:39Z",
        "creationDate": "2021-07-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/2006.07264"
        ],
        "arxiv_author": [
            "Vincent Carles",
            "Evan Heetderks",
            "Alexandre Magueresse"
        ],
        "arxiv_summary": "A current problem in NLP is massaging and processing low-resource languages\nwhich lack useful training attributes such as supervised data, number of native\nspeakers or experts, etc. This review paper concisely summarizes previous\ngroundbreaking achievements made towards resolving this problem, and analyzes\npotential improvements in the context of the overall future research direction.",
        "arxiv_firstAuthor": "Alexandre Magueresse",
        "arxiv_title": "Low-resource Languages: A Review of Past Work and Future Challenges",
        "arxiv_num": "2006.07264",
        "arxiv_published": "2020-06-12T15:21:57Z",
        "arxiv_updated": "2020-06-12T15:21:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_1902_10197_rotate_knowledge_",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/rotate",
            "http://www.semanlink.net/tag/kd_mkb_biblio",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge_graph_completion"
        ],
        "comment": "> We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.",
        "title": "[1902.10197] RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
        "relatedDoc": [],
        "creationTime": "2020-03-03T13:27:48Z",
        "creationDate": "2020-03-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/1902.10197"
        ],
        "arxiv_author": [
            "Zhi-Hong Deng",
            "Zhiqing Sun",
            "Jian Tang",
            "Jian-Yun Nie"
        ],
        "arxiv_summary": "We study the problem of learning representations of entities and relations in\nknowledge graphs for predicting missing links. The success of such a task\nheavily relies on the ability of modeling and inferring the patterns of (or\nbetween) the relations. In this paper, we present a new approach for knowledge\ngraph embedding called RotatE, which is able to model and infer various\nrelation patterns including: symmetry/antisymmetry, inversion, and composition.\nSpecifically, the RotatE model defines each relation as a rotation from the\nsource entity to the target entity in the complex vector space. In addition, we\npropose a novel self-adversarial negative sampling technique for efficiently\nand effectively training the RotatE model. Experimental results on multiple\nbenchmark knowledge graphs show that the proposed RotatE model is not only\nscalable, but also able to infer and model various relation patterns and\nsignificantly outperform existing state-of-the-art models for link prediction.",
        "arxiv_firstAuthor": "Zhiqing Sun",
        "arxiv_title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
        "arxiv_num": "1902.10197",
        "arxiv_published": "2019-02-26T20:15:09Z",
        "arxiv_updated": "2019-02-26T20:15:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1807.07984",
        "tag": [
            "http://www.semanlink.net/tag/attention_in_graphs",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> An attention mechanism aids a model by\r\nallowing it to \"focus on the most relevant parts of the input to make decisions\"\r\n",
        "title": "[1807.07984] Attention Models in Graphs: A Survey",
        "relatedDoc": [],
        "creationTime": "2018-11-14T02:13:13Z",
        "creationDate": "2018-11-14",
        "bookmarkOf": [],
        "arxiv_author": [
            "Eunyee Koh",
            "Nesreen K. Ahmed",
            "Sungchul Kim",
            "Ryan A. Rossi",
            "John Boaz Lee"
        ],
        "arxiv_summary": "Graph-structured data arise naturally in many different application domains.\nBy representing data as graphs, we can capture entities (i.e., nodes) as well\nas their relationships (i.e., edges) with each other. Many useful insights can\nbe derived from graph-structured data as demonstrated by an ever-growing body\nof work focused on graph mining. However, in the real-world, graphs can be both\nlarge - with many complex patterns - and noisy which can pose a problem for\neffective graph mining. An effective way to deal with this issue is to\nincorporate \"attention\" into graph mining solutions. An attention mechanism\nallows a method to focus on task-relevant parts of the graph, helping it to\nmake better decisions. In this work, we conduct a comprehensive and focused\nsurvey of the literature on the emerging field of graph attention models. We\nintroduce three intuitive taxonomies to group existing work. These are based on\nproblem setting (type of input and output), the type of attention mechanism\nused, and the task (e.g., graph classification, link prediction, etc.). We\nmotivate our taxonomies through detailed examples and use each to survey\ncompeting approaches from a unique standpoint. Finally, we highlight several\nchallenges in the area and discuss promising directions for future work.",
        "arxiv_firstAuthor": "John Boaz Lee",
        "arxiv_title": "Attention Models in Graphs: A Survey",
        "arxiv_num": "1807.07984",
        "arxiv_published": "2018-07-20T18:11:07Z",
        "arxiv_updated": "2018-07-20T18:11:07Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2001_03765_learning_cross_con",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_embeddings",
            "http://www.semanlink.net/tag/entity_type_representation"
        ],
        "comment": "",
        "title": "[2001.03765] Learning Cross-Context Entity Representations from Text",
        "relatedDoc": [],
        "creationTime": "2021-06-22T13:42:19Z",
        "creationDate": "2021-06-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.03765"
        ],
        "arxiv_author": [
            "David Weiss",
            "Nicholas FitzGerald",
            "Livio Baldini Soares",
            "Tom Kwiatkowski",
            "Thibault F\u00e9vry",
            "Jeffrey Ling",
            "Zifei Shan"
        ],
        "arxiv_summary": "Language modeling tasks, in which words, or word-pieces, are predicted on the\nbasis of a local context, have been very effective for learning word embeddings\nand context dependent representations of phrases. Motivated by the observation\nthat efforts to code world knowledge into machine readable knowledge bases or\nhuman readable encyclopedias tend to be entity-centric, we investigate the use\nof a fill-in-the-blank task to learn context independent representations of\nentities from the text contexts in which those entities were mentioned. We show\nthat large scale training of neural models allows us to learn high quality\nentity representations, and we demonstrate successful results on four domains:\n(1) existing entity-level typing benchmarks, including a 64% error reduction\nover previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot\ncategory reconstruction task; (3) existing entity linking benchmarks, where we\nmatch the state-of-the-art on CoNLL-Aida without linking-specific features and\nobtain a score of 89.8% on TAC-KBP 2010 without using any alias table, external\nknowledge base or in domain training data and (4) answering trivia questions,\nwhich uniquely identify entities. Our global entity representations encode\nfine-grained type categories, such as Scottish footballers, and can answer\ntrivia questions such as: Who was the last inmate of Spandau jail in Berlin?",
        "arxiv_firstAuthor": "Jeffrey Ling",
        "arxiv_title": "Learning Cross-Context Entity Representations from Text",
        "arxiv_num": "2001.03765",
        "arxiv_published": "2020-01-11T15:30:56Z",
        "arxiv_updated": "2020-01-11T15:30:56Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1811.06031",
        "tag": [
            "http://www.semanlink.net/tag/multi_task_learning",
            "http://www.semanlink.net/tag/sebastian_ruder",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embeddings"
        ],
        "comment": "[Blog post](https://medium.com/dair-ai/hmtl-multi-task-learning-for-state-of-the-art-nlp-245572bbb601), [GitHub repo](https://github.com/huggingface/hmtl)\r\n",
        "title": "[1811.06031] A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks",
        "relatedDoc": [
            "https://medium.com/dair-ai/hmtl-multi-task-learning-for-state-of-the-art-nlp-245572bbb601"
        ],
        "creationTime": "2018-11-17T10:24:49Z",
        "creationDate": "2018-11-17",
        "bookmarkOf": [],
        "arxiv_author": [
            "Victor Sanh",
            "Sebastian Ruder",
            "Thomas Wolf"
        ],
        "arxiv_summary": "Much effort has been devoted to evaluate whether multi-task learning can be\nleveraged to learn rich representations that can be used in various Natural\nLanguage Processing (NLP) down-stream applications. However, there is still a\nlack of understanding of the settings in which multi-task learning has a\nsignificant effect. In this work, we introduce a hierarchical model trained in\na multi-task learning setup on a set of carefully selected semantic tasks. The\nmodel is trained in a hierarchical fashion to introduce an inductive bias by\nsupervising a set of low level tasks at the bottom layers of the model and more\ncomplex tasks at the top layers of the model. This model achieves\nstate-of-the-art results on a number of tasks, namely Named Entity Recognition,\nEntity Mention Detection and Relation Extraction without hand-engineered\nfeatures or external NLP tools like syntactic parsers. The hierarchical\ntraining supervision induces a set of shared semantic representations at lower\nlayers of the model. We show that as we move from the bottom to the top layers\nof the model, the hidden states of the layers tend to represent more complex\nsemantic information.",
        "arxiv_firstAuthor": "Victor Sanh",
        "arxiv_title": "A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks",
        "arxiv_num": "1811.06031",
        "arxiv_published": "2018-11-14T19:42:03Z",
        "arxiv_updated": "2018-11-26T06:15:05Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1707.00418",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/multi_label_classification"
        ],
        "comment": "Uses [Deep Canonical Correlation Analysis](/tag/deep_canonical_correlation_analysis) and autoencoder structures to **learn a latent subspace from both feature and label domains** for multi-label classification.\r\n\r\n(several implementations on github)\r\n\r\n\r\n",
        "title": "[1707.00418] Learning Deep Latent Spaces for Multi-Label Classification",
        "relatedDoc": [],
        "creationTime": "2018-03-16T23:37:58Z",
        "creationDate": "2018-03-16",
        "bookmarkOf": [],
        "arxiv_author": [
            "Chih-Kuan Yeh",
            "Wei-Jen Ko",
            "Yu-Chiang Frank Wang",
            "Wei-Chieh Wu"
        ],
        "arxiv_summary": "Multi-label classification is a practical yet challenging task in machine\nlearning related fields, since it requires the prediction of more than one\nlabel category for each input instance. We propose a novel deep neural networks\n(DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this\ntask. Aiming at better relating feature and label domain data for improved\nclassification, we uniquely perform joint feature and label embedding by\nderiving a deep latent space, followed by the introduction of label-correlation\nsensitive loss function for recovering the predicted label outputs. Our C2AE is\nachieved by integrating the DNN architectures of canonical correlation analysis\nand autoencoder, which allows end-to-end learning and prediction with the\nability to exploit label dependency. Moreover, our C2AE can be easily extended\nto address the learning problem with missing labels. Our experiments on\nmultiple datasets with different scales confirm the effectiveness and\nrobustness of our proposed method, which is shown to perform favorably against\nstate-of-the-art methods for multi-label classification.",
        "arxiv_firstAuthor": "Chih-Kuan Yeh",
        "arxiv_title": "Learning Deep Latent Spaces for Multi-Label Classification",
        "arxiv_num": "1707.00418",
        "arxiv_published": "2017-07-03T06:37:01Z",
        "arxiv_updated": "2017-07-03T06:37:01Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2004_04906_dense_passage_retr",
        "tag": [
            "http://www.semanlink.net/tag/emnlp_2020",
            "http://www.semanlink.net/tag/open_domain_question_answering",
            "http://www.semanlink.net/tag/dense_passage_retrieval",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Uses two BERT models to encode text: one for encoding queries and one for encoding documents. The two models are trained simultaneously in a two-tower configuration to maximize the dot product for passages likely to answer the question\r\n\r\n[Github](https://github.com/facebookresearch/DPR)",
        "title": "[2004.04906] Dense Passage Retrieval for Open-Domain Question Answering",
        "relatedDoc": [],
        "creationTime": "2021-06-03T11:06:07Z",
        "creationDate": "2021-06-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.04906"
        ],
        "arxiv_author": [
            "Danqi Chen",
            "Wen-tau Yih",
            "Vladimir Karpukhin",
            "Patrick Lewis",
            "Sewon Min",
            "Sergey Edunov",
            "Barlas O\u011fuz",
            "Ledell Wu"
        ],
        "arxiv_summary": "Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, where traditional sparse vector space models, such\nas TF-IDF or BM25, are the de facto method. In this work, we show that\nretrieval can be practically implemented using dense representations alone,\nwhere embeddings are learned from a small number of questions and passages by a\nsimple dual-encoder framework. When evaluated on a wide range of open-domain QA\ndatasets, our dense retriever outperforms a strong Lucene-BM25 system largely\nby 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.",
        "arxiv_firstAuthor": "Vladimir Karpukhin",
        "arxiv_title": "Dense Passage Retrieval for Open-Domain Question Answering",
        "arxiv_num": "2004.04906",
        "arxiv_published": "2020-04-10T04:53:17Z",
        "arxiv_updated": "2020-09-30T21:27:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/11/2010_03496_inductive_entity_r",
        "tag": [
            "http://www.semanlink.net/tag/link_prediction",
            "http://www.semanlink.net/tag/text_aware_kg_embedding",
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/thewebconf_2021",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_knowledge_graphs",
            "http://www.semanlink.net/tag/entity_embeddings",
            "http://www.semanlink.net/tag/ckb"
        ],
        "comment": "BLP \"BERT for Link Prediction\": using the textual descriptions of entities when computing entity representations (hence not failing with entities unknown in training)\r\n\r\n> a method for **learning representations\r\nof entities**, that uses a **pre-trained Transformer** based\r\narchitecture as an entity encoder, and\r\n**link prediction training on a knowledge graph\r\nwith textual entity descriptions**.\r\n\r\n> using entity descriptions,\r\nan entity encoder is trained for link prediction in\r\na knowledge graph. The encoder can then be used\r\nwithout fine-tuning to obtain features for entity classification\r\nand information retrieval\r\n\r\nCites [Xie et al](doc:2020/10/representation_learning_of_know) and [Kepler](doc:2020/11/1911_06136_kepler_a_unified_). They claim that their\r\nobjective targeted exclusively for link prediction (and not an objective that combines language modeling\r\nand link prediction as Kepler)\r\nperforms better than Kepler's more complex one.",
        "title": "[2010.03496] Inductive Entity Representations from Text via Link Prediction",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/10/representation_learning_of_know",
            "http://www.semanlink.net/doc/2020/11/1911_06136_kepler_a_unified_"
        ],
        "creationTime": "2020-11-03T16:38:59Z",
        "creationDate": "2020-11-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.03496"
        ],
        "arxiv_author": [
            "Michael Cochez",
            "Paul Groth",
            "Daniel Daza"
        ],
        "arxiv_summary": "We present a method for learning representations of entities, that uses a\nTransformer-based architecture as an entity encoder, and link prediction\ntraining on a knowledge graph with textual entity descriptions. We demonstrate\nthat our approach can be applied effectively for link prediction in different\ninductive settings involving entities not seen during training, outperforming\nrelated state-of-the-art methods (22% MRR improvement on average). We provide\nevidence that the learned representations transfer to other tasks that do not\nrequire fine-tuning the entity encoder. In an entity classification task we\nobtain an average improvement of 16% accuracy compared with baselines that also\nemploy pre-trained models. For an information retrieval task, significant\nimprovements of up to 8.8% in NDCG@10 were obtained for natural language\nqueries.",
        "arxiv_firstAuthor": "Daniel Daza",
        "arxiv_title": "Inductive Entity Representations from Text via Link Prediction",
        "arxiv_num": "2010.03496",
        "arxiv_published": "2020-10-07T16:04:06Z",
        "arxiv_updated": "2020-10-07T16:04:06Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1811.05370",
        "tag": [
            "http://www.semanlink.net/tag/ulmfit",
            "http://www.semanlink.net/tag/elmo",
            "http://www.semanlink.net/tag/amazon_alexa",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/transfer_learning"
        ],
        "comment": "> We apply ELMo, ULMFiT (unsupervised transfer) with supervised transfer to reduce labeled data required for launching domains in Alexa by 10-15x",
        "title": "[1811.05370] Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents",
        "relatedDoc": [],
        "creationTime": "2018-11-20T00:14:11Z",
        "creationDate": "2018-11-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Aditya Siddhant",
            "Anuj Goyal",
            "Angeliki Metallinou"
        ],
        "arxiv_summary": "User interaction with voice-powered agents generates large amounts of\nunlabeled utterances. In this paper, we explore techniques to efficiently\ntransfer the knowledge from these unlabeled utterances to improve model\nperformance on Spoken Language Understanding (SLU) tasks. We use Embeddings\nfrom Language Model (ELMo) to take advantage of unlabeled data by learning\ncontextualized word representations. Additionally, we propose ELMo-Light\n(ELMoL), a faster and simpler unsupervised pre-training method for SLU. Our\nfindings suggest unsupervised pre-training on a large corpora of unlabeled\nutterances leads to significantly better SLU performance compared to training\nfrom scratch and it can even outperform conventional supervised transfer.\nAdditionally, we show that the gains from unsupervised transfer techniques can\nbe further improved by supervised transfer. The improvements are more\npronounced in low resource settings and when using only 1000 labeled in-domain\nsamples, our techniques match the performance of training from scratch on\n10-15x more labeled in-domain data.",
        "arxiv_firstAuthor": "Aditya Siddhant",
        "arxiv_title": "Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents",
        "arxiv_num": "1811.05370",
        "arxiv_published": "2018-11-13T15:44:31Z",
        "arxiv_updated": "2018-11-13T15:44:31Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1103.0398",
        "tag": [
            "http://www.semanlink.net/tag/multi_task_learning",
            "http://www.semanlink.net/tag/multitask_learning_in_nlp",
            "http://www.semanlink.net/tag/deep_nlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/frequently_cited_paper",
            "http://www.semanlink.net/tag/ronan_collobert",
            "http://www.semanlink.net/tag/nlp"
        ],
        "comment": "seminal work\r\n\r\nAbstract:\r\n\r\n> a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements\r\n",
        "title": "[1103.0398] Natural Language Processing (almost) from Scratch",
        "relatedDoc": [],
        "creationTime": "2018-01-17T18:40:10Z",
        "creationDate": "2018-01-17",
        "bookmarkOf": [],
        "arxiv_author": [
            "Leon Bottou",
            "Michael Karlen",
            "Ronan Collobert",
            "Koray Kavukcuoglu",
            "Pavel Kuksa",
            "Jason Weston"
        ],
        "arxiv_summary": "We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.",
        "arxiv_firstAuthor": "Ronan Collobert",
        "arxiv_title": "Natural Language Processing (almost) from Scratch",
        "arxiv_num": "1103.0398",
        "arxiv_published": "2011-03-02T11:34:50Z",
        "arxiv_updated": "2011-03-02T11:34:50Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1601.03764",
        "tag": [
            "http://www.semanlink.net/tag/lexical_ambiguity",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/emnlp_2018",
            "http://www.semanlink.net/tag/sparse_dictionary_learning",
            "http://www.semanlink.net/tag/sanjeev_arora"
        ],
        "comment": "> Here it is shown that multiple word senses reside\r\nin linear superposition within the word\r\nembedding and simple sparse coding can recover\r\nvectors that approximately capture the\r\nsenses\r\n\r\n> Each extracted word sense is accompanied by one of about  2000 \u201cdiscourse atoms\u201d that gives a succinct description of which other words co-occur with that word sense.\r\n\r\n> The success of the approach is mathematically explained using a variant of\r\nthe random walk on discourses model\r\n\r\n(\"random walk\": a generative model for language). Under the assumptions of this model,  there\r\nexists a linear relationship between the vector of a\r\nword w and the vectors of the words in its contexts (It is not the average of the words in w's context, but in a given corpus the matrix of the linear relationship does not depend on w. It can be estimated, and so we can compute the embedding of a word from the contexts it belongs to)\r\n\r\n[Related blog post](/doc/?uri=https%3A%2F%2Fwww.offconvex.org%2F2016%2F07%2F10%2Fembeddingspolysemy%2F)\r\n",
        "title": "[1601.03764] Linear Algebraic Structure of Word Senses, with Applications to Polysemy",
        "relatedDoc": [
            "https://www.offconvex.org/2016/07/10/embeddingspolysemy/"
        ],
        "creationTime": "2018-08-28T11:00:08Z",
        "creationDate": "2018-08-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Yingyu Liang",
            "Andrej Risteski",
            "Tengyu Ma",
            "Yuanzhi Li",
            "Sanjeev Arora"
        ],
        "arxiv_summary": "Word embeddings are ubiquitous in NLP and information retrieval, but it is\nunclear what they represent when the word is polysemous. Here it is shown that\nmultiple word senses reside in linear superposition within the word embedding\nand simple sparse coding can recover vectors that approximately capture the\nsenses. The success of our approach, which applies to several embedding\nmethods, is mathematically explained using a variant of the random walk on\ndiscourses model (Arora et al., 2016). A novel aspect of our technique is that\neach extracted word sense is accompanied by one of about 2000 \"discourse atoms\"\nthat gives a succinct description of which other words co-occur with that word\nsense. Discourse atoms can be of independent interest, and make the method\npotentially more useful. Empirical tests are used to verify and support the\ntheory.",
        "arxiv_firstAuthor": "Sanjeev Arora",
        "arxiv_title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy",
        "arxiv_num": "1601.03764",
        "arxiv_published": "2016-01-14T22:02:18Z",
        "arxiv_updated": "2018-12-07T17:30:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/01/2012_04740_river_machine_lea",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/max_halford",
            "http://www.semanlink.net/tag/raphaelsty"
        ],
        "comment": "[Github](doc:2020/01/creme_ml_creme_online_machine_)",
        "title": "[2012.04740] River: machine learning for streaming data in Python",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/01/creme_ml_creme_online_machine_"
        ],
        "creationTime": "2021-01-05T16:15:12Z",
        "creationDate": "2021-01-05",
        "bookmarkOf": [
            "https://arxiv.org/abs/2012.04740"
        ],
        "arxiv_author": [
            "Jesse Read",
            "Albert Bifet",
            "Saulo Martiello Mastelini",
            "Talel Abdessalem",
            "Jacob Montiel",
            "Geoffrey Bolmier",
            "Raphael Sourty",
            "Adil Zouitine",
            "Max Halford",
            "Robin Vaysse",
            "Heitor Murilo Gomes"
        ],
        "arxiv_summary": "River is a machine learning library for dynamic data streams and continual\nlearning. It provides multiple state-of-the-art learning methods, data\ngenerators/transformers, performance metrics and evaluators for different\nstream learning problems. It is the result from the merger of the two most\npopular packages for stream learning in Python: Creme and scikit-multiflow.\nRiver introduces a revamped architecture based on the lessons learnt from the\nseminal packages. River's ambition is to be the go-to library for doing machine\nlearning on streaming data. Additionally, this open source package brings under\nthe same umbrella a large community of practitioners and researchers. The\nsource code is available at https://github.com/online-ml/river.",
        "arxiv_firstAuthor": "Jacob Montiel",
        "arxiv_title": "River: machine learning for streaming data in Python",
        "arxiv_num": "2012.04740",
        "arxiv_published": "2020-12-08T21:04:44Z",
        "arxiv_updated": "2020-12-08T21:04:44Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1601.00670",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/variational_bayesian_methods",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/statistics"
        ],
        "comment": "",
        "title": "[1601.00670] Variational Inference: A Review for Statisticians",
        "relatedDoc": [],
        "creationTime": "2018-08-07T10:37:09Z",
        "creationDate": "2018-08-07",
        "bookmarkOf": [],
        "arxiv_author": [
            "David M. Blei",
            "Jon D. McAuliffe",
            "Alp Kucukelbir"
        ],
        "arxiv_summary": "One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.",
        "arxiv_firstAuthor": "David M. Blei",
        "arxiv_title": "Variational Inference: A Review for Statisticians",
        "arxiv_num": "1601.00670",
        "arxiv_published": "2016-01-04T21:28:04Z",
        "arxiv_updated": "2018-05-09T20:52:28Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1808.07699",
        "tag": [
            "http://www.semanlink.net/tag/entity_linking",
            "http://www.semanlink.net/tag/end_to_end_entity_linking",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> We presented the first **neural end-to-end entity linking**\r\nmodel and show the benefit of jointly optimizing\r\nentity recognition and linking. Leveraging key\r\ncomponents, namely word, entity and mention embeddings,\r\nwe prove that engineered features can\r\nbe almost completely replaced by modern neural\r\nnetworks.",
        "title": "[1808.07699] End-to-End Neural Entity Linking",
        "relatedDoc": [],
        "creationTime": "2019-04-23T19:12:16Z",
        "creationDate": "2019-04-23",
        "bookmarkOf": [],
        "arxiv_author": [
            "Octavian-Eugen Ganea",
            "Nikolaos Kolitsas",
            "Thomas Hofmann"
        ],
        "arxiv_summary": "Entity Linking (EL) is an essential task for semantic text understanding and\ninformation extraction. Popular methods separately address the Mention\nDetection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging\ntheir mutual dependency. We here propose the first neural end-to-end EL system\nthat jointly discovers and links entities in a text document. The main idea is\nto consider all possible spans as potential mentions and learn contextual\nsimilarity scores over their entity candidates that are useful for both MD and\nED decisions. Key components are context-aware mention embeddings, entity\nembeddings and a probabilistic mention - entity map, without demanding other\nengineered features. Empirically, we show that our end-to-end method\nsignificantly outperforms popular systems on the Gerbil platform when enough\ntraining data is available. Conversely, if testing datasets follow different\nannotation conventions compared to the training set (e.g. queries/ tweets vs\nnews documents), our ED model coupled with a traditional NER system offers the\nbest or second best EL accuracy.",
        "arxiv_firstAuthor": "Nikolaos Kolitsas",
        "arxiv_title": "End-to-End Neural Entity Linking",
        "arxiv_num": "1808.07699",
        "arxiv_published": "2018-08-23T11:16:57Z",
        "arxiv_updated": "2018-08-29T17:44:38Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1611.04228",
        "tag": [
            "http://www.semanlink.net/tag/neuroscience_and_ai",
            "http://www.semanlink.net/tag/brain_vs_deep_learning",
            "http://www.semanlink.net/tag/hebbian_theory",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "The \"fire together, wire together\" Hebbian model is a central principle for learning in neuroscience, but surprisingly, it has found limited applicability in modern machine learning. In this paper, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning",
        "title": "[1611.04228] Learning Sparse, Distributed Representations using the Hebbian Principle",
        "relatedDoc": [],
        "creationTime": "2017-04-28T22:52:38Z",
        "creationDate": "2017-04-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Upamanyu Madhow",
            "Aseem Wadhwa"
        ],
        "arxiv_summary": "The \"fire together, wire together\" Hebbian model is a central principle for\nlearning in neuroscience, but surprisingly, it has found limited applicability\nin modern machine learning. In this paper, we take a first step towards\nbridging this gap, by developing flavors of competitive Hebbian learning which\nproduce sparse, distributed neural codes using online adaptation with minimal\ntuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning\n(AHL). We illustrate the distributed nature of the learned representations via\noutput entropy computations for synthetic data, and demonstrate superior\nperformance, compared to standard alternatives such as autoencoders, in\ntraining a deep convolutional net on standard image datasets.",
        "arxiv_firstAuthor": "Aseem Wadhwa",
        "arxiv_title": "Learning Sparse, Distributed Representations using the Hebbian Principle",
        "arxiv_num": "1611.04228",
        "arxiv_published": "2016-11-14T02:28:13Z",
        "arxiv_updated": "2016-11-14T02:28:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/09/_1909_04939_inceptiontime_fin",
        "tag": [
            "http://www.semanlink.net/tag/time_series",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1909.04939] InceptionTime: Finding AlexNet for Time Series Classification",
        "relatedDoc": [],
        "creationTime": "2019-09-28T10:23:53Z",
        "creationDate": "2019-09-28",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.04939"
        ],
        "arxiv_author": [
            "Daniel F. Schmidt",
            "Jonathan Weber",
            "Germain Forestier",
            "Lhassane Idoumghar",
            "Charlotte Pelletier",
            "Pierre-Alain Muller",
            "Geoffrey I. Webb",
            "Hassan Ismail Fawaz",
            "Fran\u00e7ois Petitjean",
            "Benjamin Lucas"
        ],
        "arxiv_summary": "Time series classification (TSC) is the area of machine learning interested\nin learning how to assign labels to time series. The last few decades of work\nin this area have led to significant progress in the accuracy of classifiers,\nwith the state of the art now represented by the HIVE-COTE algorithm. While\nextremely accurate, HIVE-COTE is infeasible to use in many applications because\nof its very high training time complexity in O(N^2*T^4) for a dataset with N\ntime series of length T. For example, it takes HIVE-COTE more than 72,000s to\nlearn from a small dataset with N=700 time series of short length T=46. Deep\nlearning, on the other hand, has now received enormous attention because of its\nhigh scalability and state-of-the-art accuracy in computer vision and natural\nlanguage processing tasks. Deep learning for TSC has only very recently started\nto be explored, with the first few architectures developed over the last 3\nyears only. The accuracy of deep learning for TSC has been raised to a\ncompetitive level, but has not quite reached the level of HIVE-COTE. This is\nwhat this paper achieves: outperforming HIVE-COTE's accuracy together with\nscalability. We take an important step towards finding the AlexNet network for\nTSC by presenting InceptionTime---an ensemble of deep Convolutional Neural\nNetwork (CNN) models, inspired by the Inception-v4 architecture. Our\nexperiments show that InceptionTime slightly outperforms HIVE-COTE with a\nwin/draw/loss on the UCR archive of 40/6/39. Not only is InceptionTime more\naccurate, but it is much faster: InceptionTime learns from that same dataset\nwith 700 time series in 2,300s but can also learn from a dataset with 8M time\nseries in 13 hours, a quantity of data that is fully out of reach of HIVE-COTE.",
        "arxiv_firstAuthor": "Hassan Ismail Fawaz",
        "arxiv_title": "InceptionTime: Finding AlexNet for Time Series Classification",
        "arxiv_num": "1909.04939",
        "arxiv_published": "2019-09-11T09:32:40Z",
        "arxiv_updated": "2019-09-13T14:28:15Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1803.05651",
        "tag": [
            "http://www.semanlink.net/tag/word2vec",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "We show that high quality quantized word vectors using 1-2 bits per parameter can be learned by introducing a quantization function into Word2Vec. We furthermore show that training with the quantization function acts as a regularizer",
        "title": "[1803.05651] Word2Bits - Quantized Word Vectors",
        "relatedDoc": [],
        "creationTime": "2018-03-20T17:36:21Z",
        "creationDate": "2018-03-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Maximilian Lam"
        ],
        "arxiv_summary": "Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.",
        "arxiv_firstAuthor": "Maximilian Lam",
        "arxiv_title": "Word2Bits - Quantized Word Vectors",
        "arxiv_num": "1803.05651",
        "arxiv_published": "2018-03-15T09:21:34Z",
        "arxiv_updated": "2018-03-31T08:45:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/2004_05150_longformer_the_lo",
        "tag": [
            "http://www.semanlink.net/tag/nlp_long_documents",
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> **Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length**. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length. Longformer's attention mechanism is a **drop-in replacement** for the standard self-attention and **combines a local windowed attention with a task motivated global attention**.",
        "title": "[2004.05150] Longformer: The Long-Document Transformer",
        "relatedDoc": [],
        "creationTime": "2020-04-13T11:06:40Z",
        "creationDate": "2020-04-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.05150"
        ],
        "arxiv_author": [
            "Iz Beltagy",
            "Matthew E. Peters",
            "Arman Cohan"
        ],
        "arxiv_summary": "Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA.",
        "arxiv_firstAuthor": "Iz Beltagy",
        "arxiv_title": "Longformer: The Long-Document Transformer",
        "arxiv_num": "2004.05150",
        "arxiv_published": "2020-04-10T17:54:09Z",
        "arxiv_updated": "2020-04-10T17:54:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1511_03643_unifying_distillat",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/machines_teaching_machines",
            "http://www.semanlink.net/tag/ai_facebook"
        ],
        "comment": "A framework to learn from multiple machines and data representations, unifying two techniques that enable machines to learn from other machines: [distillation](tag:knowledge_distillation) ([Hinton et al., 2015](doc:2020/04/1503_02531_distilling_the_kno)) and privileged information (Vapnik & Izmailov, 2015)\r\n",
        "title": "[1511.03643] Unifying distillation and privileged information",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/04/1503_02531_distilling_the_kno"
        ],
        "creationTime": "2020-05-31T10:42:51Z",
        "creationDate": "2020-05-31",
        "bookmarkOf": [
            "https://arxiv.org/abs/1511.03643"
        ],
        "arxiv_author": [
            "David Lopez-Paz",
            "Bernhard Sch\u00f6lkopf",
            "L\u00e9on Bottou",
            "Vladimir Vapnik"
        ],
        "arxiv_summary": "Distillation (Hinton et al., 2015) and privileged information (Vapnik &\nIzmailov, 2015) are two techniques that enable machines to learn from other\nmachines. This paper unifies these two techniques into generalized\ndistillation, a framework to learn from multiple machines and data\nrepresentations. We provide theoretical and causal insight about the inner\nworkings of generalized distillation, extend it to unsupervised, semisupervised\nand multitask learning scenarios, and illustrate its efficacy on a variety of\nnumerical simulations on both synthetic and real-world data.",
        "arxiv_firstAuthor": "David Lopez-Paz",
        "arxiv_title": "Unifying distillation and privileged information",
        "arxiv_num": "1511.03643",
        "arxiv_published": "2015-11-11T20:27:54Z",
        "arxiv_updated": "2016-02-26T02:21:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1901.11504",
        "tag": [
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/multi_task_learning",
            "http://www.semanlink.net/tag/kd_mkb_biblio",
            "http://www.semanlink.net/tag/nlu",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/bert"
        ],
        "comment": "outperforms BERT in nine of eleven benchmark NLP tasks",
        "title": "[1901.11504] Multi-Task Deep Neural Networks for Natural Language Understanding",
        "relatedDoc": [],
        "creationTime": "2019-02-17T12:30:18Z",
        "creationDate": "2019-02-17",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jianfeng Gao",
            "Pengcheng He",
            "Xiaodong Liu",
            "Weizhu Chen"
        ],
        "arxiv_summary": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for\nlearning representations across multiple natural language understanding (NLU)\ntasks. MT-DNN not only leverages large amounts of cross-task data, but also\nbenefits from a regularization effect that leads to more general\nrepresentations in order to adapt to new tasks and domains. MT-DNN extends the\nmodel proposed in Liu et al. (2015) by incorporating a pre-trained\nbidirectional transformer language model, known as BERT (Devlin et al., 2018).\nMT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI,\nSciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7%\n(2.2% absolute improvement). We also demonstrate using the SNLI and SciTail\ndatasets that the representations learned by MT-DNN allow domain adaptation\nwith substantially fewer in-domain labels than the pre-trained BERT\nrepresentations. The code and pre-trained models are publicly available at\nhttps://github.com/namisan/mt-dnn.",
        "arxiv_firstAuthor": "Xiaodong Liu",
        "arxiv_title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
        "arxiv_num": "1901.11504",
        "arxiv_published": "2019-01-31T18:07:25Z",
        "arxiv_updated": "2019-05-30T00:01:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1709_03933_hash_embeddings_fo",
        "tag": [
            "http://www.semanlink.net/tag/feature_hashing",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> A hash embedding may be seen as an interpolation between\r\na standard word embedding and a word embedding created using a random hash\r\nfunction (the hashing trick).\r\n\r\nrecommand\u00e9 par [Rapha\u00ebl Sourty](tag:raphaelsty)",
        "title": "[1709.03933] Hash Embeddings for Efficient Word Representations",
        "relatedDoc": [],
        "creationTime": "2020-05-19T11:14:12Z",
        "creationDate": "2020-05-19",
        "bookmarkOf": [
            "https://arxiv.org/abs/1709.03933"
        ],
        "arxiv_author": [
            "Dan Svenstrup",
            "Jonas Meinertz Hansen",
            "Ole Winther"
        ],
        "arxiv_summary": "We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight\nvector. The final $d$ dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of $B$ embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.",
        "arxiv_firstAuthor": "Dan Svenstrup",
        "arxiv_title": "Hash Embeddings for Efficient Word Representations",
        "arxiv_num": "1709.03933",
        "arxiv_published": "2017-09-12T16:13:10Z",
        "arxiv_updated": "2017-09-12T16:13:10Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/2002_06504_differentiable_top",
        "tag": [
            "http://www.semanlink.net/tag/top_k",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> if the top-k operation is implemented in an algorithmic way, e.g., using bubble algorithm, the resulting model cannot be trained in an end-to-end way using prevalent gradient descent algorithms. This is because these implementations typically involve swapping indices, whose gradient cannot be computed. Moreover, the corresponding mapping from the input scores to the indicator vector of whether this element belongs to the top-k set is essentially discontinuous. To address the issue, we propose a smoothed approximation, namely the SOFT (Scalable Optimal transport-based diFferenTiable) top-k operator\r\n> ...\r\n> We apply the proposed operator to the [k-nearest neighbors](tag:k_nearest_neighbors_algorithm) and [beam search](tag:beam_search) algorithms, and demonstrate improved performance",
        "title": "[2002.06504] Differentiable Top-k Operator with Optimal Transport",
        "relatedDoc": [],
        "creationTime": "2020-06-29T14:04:10Z",
        "creationDate": "2020-06-29",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.06504"
        ],
        "arxiv_author": [
            "Hanjun Dai",
            "Hongyuan Zha",
            "Wei Wei",
            "Minshuo Chen",
            "Tomas Pfister",
            "Bo Dai",
            "Yujia Xie",
            "Tuo Zhao"
        ],
        "arxiv_summary": "The top-k operation, i.e., finding the k largest or smallest elements from a\ncollection of scores, is an important model component, which is widely used in\ninformation retrieval, machine learning, and data mining. However, if the top-k\noperation is implemented in an algorithmic way, e.g., using bubble algorithm,\nthe resulting model cannot be trained in an end-to-end way using prevalent\ngradient descent algorithms. This is because these implementations typically\ninvolve swapping indices, whose gradient cannot be computed. Moreover, the\ncorresponding mapping from the input scores to the indicator vector of whether\nthis element belongs to the top-k set is essentially discontinuous. To address\nthe issue, we propose a smoothed approximation, namely the SOFT (Scalable\nOptimal transport-based diFferenTiable) top-k operator. Specifically, our SOFT\ntop-k operator approximates the output of the top-k operation as the solution\nof an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT\noperator can then be efficiently approximated based on the optimality\nconditions of EOT problem. We apply the proposed operator to the k-nearest\nneighbors and beam search algorithms, and demonstrate improved performance.",
        "arxiv_firstAuthor": "Yujia Xie",
        "arxiv_title": "Differentiable Top-k Operator with Optimal Transport",
        "arxiv_num": "2002.06504",
        "arxiv_published": "2020-02-16T04:57:52Z",
        "arxiv_updated": "2020-02-18T18:56:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1511.07972",
        "tag": [
            "http://www.semanlink.net/tag/memory_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1511.07972] Learning with Memory Embeddings",
        "relatedDoc": [],
        "creationTime": "2017-10-24T14:47:21Z",
        "creationDate": "2017-10-24",
        "bookmarkOf": [],
        "arxiv_author": [
            "Volker Tresp",
            "Yinchong Yang",
            "Crist\u00f3bal Esteban",
            "Stephan Baier",
            "Denis Krompa\u00df"
        ],
        "arxiv_summary": "Embedding learning, a.k.a. representation learning, has been shown to be able\nto model large-scale semantic knowledge graphs. A key concept is a mapping of\nthe knowledge graph to a tensor representation whose entries are predicted by\nmodels using latent representations of generalized entities. Latent variable\nmodels are well suited to deal with the high dimensionality and sparsity of\ntypical knowledge graphs. In recent publications the embedding models were\nextended to also consider time evolutions, time patterns and subsymbolic\nrepresentations. In this paper we map embedding models, which were developed\npurely as solutions to technical problems for modelling temporal knowledge\ngraphs, to various cognitive memory functions, in particular to semantic and\nconcept memory, episodic memory, sensory memory, short-term memory, and working\nmemory. We discuss learning, query answering, the path from sensory input to\nsemantic decoding, and the relationship between episodic memory and semantic\nmemory. We introduce a number of hypotheses on human memory that can be derived\nfrom the developed mathematical models.",
        "arxiv_firstAuthor": "Volker Tresp",
        "arxiv_title": "Learning with Memory Embeddings",
        "arxiv_num": "1511.07972",
        "arxiv_published": "2015-11-25T07:06:09Z",
        "arxiv_updated": "2016-05-07T09:06:15Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/09/2104_06979_tsdae_using_trans",
        "tag": [
            "http://www.semanlink.net/tag/unsupervised_sentence_embedding_learning",
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nils_reimers",
            "http://www.semanlink.net/tag/emnlp_2021"
        ],
        "comment": "> The most\r\nsuccessful previous approaches like InferSent (Conneau\r\net al., 2017), Universial Sentence Encoder\r\n(USE) (Cer et al., 2018) and SBERT (Reimers and\r\nGurevych, 2019) heavily relied on labeled data to\r\ntrain sentence embedding models.\r\n>\r\n> TSDAE can\r\nachieve up to 93.1% of the performance of indomain\r\nsupervised approaches. Further, we\r\nshow that TSDAE is **a strong domain adaptation\r\nand pre-training method for sentence\r\nembeddings**, significantly outperforming other\r\napproaches like Masked Language Model.\r\n\r\n> During training, TSDAE\r\nencodes corrupted sentences into fixed-sized\r\nvectors and requires the decoder to reconstruct the\r\noriginal sentences from this sentence embedding.\r\n\r\n- <https://www.sbert.net/examples/unsupervised_learning/TSDAE/README.html>\r\n- [github](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE)\r\n- [UKPLab/sentence-transformers: Sentence Embeddings with BERT & XLNet](doc:2020/07/ukplab_sentence_transformers_s)\r\n- [twitter](https://twitter.com/KexinWang2049/status/1433361957579538432):\r\n\r\n> **TSDAE can learn domain-specific sentence embeddings with unlabeled sentences**\r\n>\r\n> Most importantly, instead of STS, **we suggest evaluating unsupervised sentence embeddings on the domain-specific tasks&datasets, which is the real use case for them**. Actually, STS scores do not correlate with performance on specific tasks. \r\n\r\n\r\n\r\n",
        "title": "[2104.06979] TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/07/ukplab_sentence_transformers_s"
        ],
        "creationTime": "2021-09-01T16:43:01Z",
        "creationDate": "2021-09-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/2104.06979"
        ],
        "arxiv_author": [
            "Nils Reimers",
            "Kexin Wang",
            "Iryna Gurevych"
        ],
        "arxiv_summary": "Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\nA crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.",
        "arxiv_firstAuthor": "Kexin Wang",
        "arxiv_title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning",
        "arxiv_num": "2104.06979",
        "arxiv_published": "2021-04-14T17:02:18Z",
        "arxiv_updated": "2021-08-30T18:23:40Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1712.09405",
        "tag": [
            "http://www.semanlink.net/tag/fasttext",
            "http://www.semanlink.net/tag/word2vec",
            "http://www.semanlink.net/tag/tomas_mikolov",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks",
        "title": "[1712.09405] Advances in Pre-Training Distributed Word Representations",
        "relatedDoc": [],
        "creationTime": "2017-12-29T20:52:48Z",
        "creationDate": "2017-12-29",
        "bookmarkOf": [],
        "arxiv_author": [
            "Tomas Mikolov",
            "Armand Joulin",
            "Edouard Grave",
            "Christian Puhrsch",
            "Piotr Bojanowski"
        ],
        "arxiv_summary": "Many Natural Language Processing applications nowadays rely on pre-trained\nword representations estimated from large text corpora such as news\ncollections, Wikipedia and Web Crawl. In this paper, we show how to train\nhigh-quality word vector representations by using a combination of known tricks\nthat are however rarely used together. The main result of our work is the new\nset of publicly available pre-trained models that outperform the current state\nof the art by a large margin on a number of tasks.",
        "arxiv_firstAuthor": "Tomas Mikolov",
        "arxiv_title": "Advances in Pre-Training Distributed Word Representations",
        "arxiv_num": "1712.09405",
        "arxiv_published": "2017-12-26T21:00:04Z",
        "arxiv_updated": "2017-12-26T21:00:04Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1602.05314",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/convolutional_neural_network"
        ],
        "comment": "",
        "title": "[1602.05314] PlaNet - Photo Geolocation with Convolutional Neural Networks",
        "relatedDoc": [],
        "creationTime": "2016-02-26T13:00:13Z",
        "creationDate": "2016-02-26",
        "bookmarkOf": [],
        "arxiv_author": [
            "Tobias Weyand",
            "Ilya Kostrikov",
            "James Philbin"
        ],
        "arxiv_summary": "Is it possible to build a system to determine the location where a photo was\ntaken using just its pixels? In general, the problem seems exceptionally\ndifficult: it is trivial to construct situations where no location can be\ninferred. Yet images often contain informative cues such as landmarks, weather\npatterns, vegetation, road markings, and architectural details, which in\ncombination may allow one to determine an approximate location and occasionally\nan exact location. Websites such as GeoGuessr and View from your Window suggest\nthat humans are relatively good at integrating these cues to geolocate images,\nespecially en-masse. In computer vision, the photo geolocation problem is\nusually approached using image retrieval methods. In contrast, we pose the\nproblem as one of classification by subdividing the surface of the earth into\nthousands of multi-scale geographic cells, and train a deep network using\nmillions of geotagged images. While previous approaches only recognize\nlandmarks or perform approximate matching using global image descriptors, our\nmodel is able to use and integrate multiple visible cues. We show that the\nresulting model, called PlaNet, outperforms previous approaches and even\nattains superhuman levels of accuracy in some cases. Moreover, we extend our\nmodel to photo albums by combining it with a long short-term memory (LSTM)\narchitecture. By learning to exploit temporal coherence to geolocate uncertain\nphotos, we demonstrate that this model achieves a 50% performance improvement\nover the single-image model.",
        "arxiv_firstAuthor": "Tobias Weyand",
        "arxiv_title": "PlaNet - Photo Geolocation with Convolutional Neural Networks",
        "arxiv_num": "1602.05314",
        "arxiv_published": "2016-02-17T06:27:55Z",
        "arxiv_updated": "2016-02-17T06:27:55Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1002.2284v2",
        "tag": [
            "http://www.semanlink.net/tag/markets",
            "http://www.semanlink.net/tag/p_np",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Hmm wow",
        "title": "[1002.2284] Markets are efficient if and only if P = NP",
        "relatedDoc": [],
        "creationTime": "2013-05-11T11:18:22Z",
        "creationDate": "2013-05-11",
        "bookmarkOf": [],
        "arxiv_author": [
            "Philip Maymin"
        ],
        "arxiv_summary": "I prove that if markets are weak-form efficient, meaning current prices fully\nreflect all information available in past prices, then P = NP, meaning every\ncomputational problem whose solution can be verified in polynomial time can\nalso be solved in polynomial time. I also prove the converse by showing how we\ncan \"program\" the market to solve NP-complete problems. Since P probably does\nnot equal NP, markets are probably not efficient. Specifically, markets become\nincreasingly inefficient as the time series lengthens or becomes more frequent.\nAn illustration by way of partitioning the excess returns to momentum\nstrategies based on data availability confirms this prediction.",
        "arxiv_firstAuthor": "Philip Maymin",
        "arxiv_title": "Markets are efficient if and only if P = NP",
        "arxiv_num": "1002.2284",
        "arxiv_published": "2010-02-11T05:56:16Z",
        "arxiv_updated": "2010-05-13T07:26:53Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1801.01586",
        "tag": [
            "http://www.semanlink.net/tag/autoencoder",
            "http://www.semanlink.net/tag/tutorial",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1801.01586] A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines",
        "relatedDoc": [],
        "creationTime": "2018-01-09T14:05:31Z",
        "creationDate": "2018-01-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Francisco Herrera",
            "Francisco Charte",
            "Mar\u00eda J. del Jesus",
            "David Charte",
            "Salvador Garc\u00eda"
        ],
        "arxiv_summary": "Many of the existing machine learning algorithms, both supervised and\nunsupervised, depend on the quality of the input characteristics to generate a\ngood model. The amount of these variables is also important, since performance\ntends to decline as the input dimensionality increases, hence the interest in\nusing feature fusion techniques, able to produce feature sets that are more\ncompact and higher level. A plethora of procedures to fuse original variables\nfor producing new ones has been developed in the past decades. The most basic\nones use linear combinations of the original variables, such as PCA (Principal\nComponent Analysis) and LDA (Linear Discriminant Analysis), while others find\nmanifold embeddings of lower dimensionality based on non-linear combinations,\nsuch as Isomap or LLE (Linear Locally Embedding) techniques.\nMore recently, autoencoders (AEs) have emerged as an alternative to manifold\nlearning for conducting nonlinear feature fusion. Dozens of AE models have been\nproposed lately, each with its own specific traits. Although many of them can\nbe used to generate reduced feature sets through the fusion of the original\nones, there also AEs designed with other applications in mind.\nThe goal of this paper is to provide the reader with a broad view of what an\nAE is, how they are used for feature fusion, a taxonomy gathering a broad range\nof models, and how they relate to other classical techniques. In addition, a\nset of didactic guidelines on how to choose the proper AE for a given task is\nsupplied, together with a discussion of the software tools available. Finally,\ntwo case studies illustrate the usage of AEs with datasets of handwritten\ndigits and breast cancer.",
        "arxiv_firstAuthor": "David Charte",
        "arxiv_title": "A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines",
        "arxiv_num": "1801.01586",
        "arxiv_published": "2018-01-04T23:51:05Z",
        "arxiv_updated": "2018-01-04T23:51:05Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_1703_07464_no_fuss_distance_m",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/zero_shot_learning",
            "http://www.semanlink.net/tag/triplet_loss",
            "http://www.semanlink.net/tag/similarity_learning",
            "http://www.semanlink.net/tag/metric_learning",
            "http://www.semanlink.net/tag/google_research"
        ],
        "comment": "> We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity...\r\n> Traditionnaly, supervision is expressed in the form of sets of points that follow\r\nan ordinal relationship \u2013 an anchor point x is similar to\r\na set of positive points Y , and dissimilar to a set of negative\r\npoints Z, and a loss defined over these distances is minimized.\r\n> Triplet-Based methods are challenging to optimize (a main issue is the need for finding informative triplets).\r\n>\r\n> We propose to **optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well**. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss.\r\n\r\nMentioned in this [blog post](/doc/2020/01/training_a_speaker_embedding_fr):\r\n\r\n> \"**Proxy based triplet learning**\": instead of generating triplets, we learn an embedding for each class and use the learnt embedding as a proxy for triplets as part of the training. In other words, we can train end to end without the computationally expensive step of resampling triplets after each network update.\r\n\r\nNear the conclusion:\r\n\r\n> Our formulation of Proxy-NCA loss produces a loss very\r\nsimilar to the standard cross-entropy loss used in classification.\r\nHowever, we arrive at our formulation from a different\r\ndirection: we are not interested in the actual classifier and\r\nindeed discard the proxies once the model has been trained.\r\nInstead, the proxies are auxiliary variables, enabling more\r\neffective optimization of the embedding model parameters.\r\n**As such, our formulation not only enables us to surpass the\r\nstate of the art in zero-shot learning, but also offers an explanation\r\nto the effectiveness of the standard trick of training\r\na classifier, and using its penultimate layer\u2019s output as the\r\nembedding.**",
        "title": "[1703.07464] No Fuss Distance Metric Learning using Proxies",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/01/training_a_speaker_embedding_fr"
        ],
        "creationTime": "2020-02-09T18:44:26Z",
        "creationDate": "2020-02-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/1703.07464"
        ],
        "arxiv_author": [
            "Sergey Ioffe",
            "Yair Movshovitz-Attias",
            "Thomas K. Leung",
            "Alexander Toshev",
            "Saurabh Singh"
        ],
        "arxiv_summary": "We address the problem of distance metric learning (DML), defined as learning\na distance consistent with a notion of semantic similarity. Traditionally, for\nthis problem supervision is expressed in the form of sets of points that follow\nan ordinal relationship -- an anchor point $x$ is similar to a set of positive\npoints $Y$, and dissimilar to a set of negative points $Z$, and a loss defined\nover these distances is minimized. While the specifics of the optimization\ndiffer, in this work we collectively call this type of supervision Triplets and\nall methods that follow this pattern Triplet-Based methods. These methods are\nchallenging to optimize. A main issue is the need for finding informative\ntriplets, which is usually achieved by a variety of tricks such as increasing\nthe batch size, hard or semi-hard triplet mining, etc. Even with these tricks,\nthe convergence rate of such methods is slow. In this paper we propose to\noptimize the triplet loss on a different space of triplets, consisting of an\nanchor data point and similar and dissimilar proxy points which are learned as\nwell. These proxies approximate the original data points, so that a triplet\nloss over the proxies is a tight upper bound of the original loss. This\nproxy-based loss is empirically better behaved. As a result, the proxy-loss\nimproves on state-of-art results for three standard zero-shot learning\ndatasets, by up to 15% points, while converging three times as fast as other\ntriplet-based losses.",
        "arxiv_firstAuthor": "Yair Movshovitz-Attias",
        "arxiv_title": "No Fuss Distance Metric Learning using Proxies",
        "arxiv_num": "1703.07464",
        "arxiv_published": "2017-03-21T23:11:56Z",
        "arxiv_updated": "2017-08-01T19:52:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1603.01360",
        "tag": [
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/guillaume_lample",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "Neural architectures for NER that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. \r\n\r\n> Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora",
        "title": "[1603.01360] Neural Architectures for Named Entity Recognition",
        "relatedDoc": [],
        "creationTime": "2018-03-05T18:40:55Z",
        "creationDate": "2018-03-05",
        "bookmarkOf": [],
        "arxiv_author": [
            "Miguel Ballesteros",
            "Chris Dyer",
            "Guillaume Lample",
            "Sandeep Subramanian",
            "Kazuya Kawakami"
        ],
        "arxiv_summary": "State-of-the-art named entity recognition systems rely heavily on\nhand-crafted features and domain-specific knowledge in order to learn\neffectively from the small, supervised training corpora that are available. In\nthis paper, we introduce two new neural architectures---one based on\nbidirectional LSTMs and conditional random fields, and the other that\nconstructs and labels segments using a transition-based approach inspired by\nshift-reduce parsers. Our models rely on two sources of information about\nwords: character-based word representations learned from the supervised corpus\nand unsupervised word representations learned from unannotated corpora. Our\nmodels obtain state-of-the-art performance in NER in four languages without\nresorting to any language-specific knowledge or resources such as gazetteers.",
        "arxiv_firstAuthor": "Guillaume Lample",
        "arxiv_title": "Neural Architectures for Named Entity Recognition",
        "arxiv_num": "1603.01360",
        "arxiv_published": "2016-03-04T06:36:29Z",
        "arxiv_updated": "2016-04-07T15:09:36Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/1904_01947_extracting_tables_",
        "tag": [
            "http://www.semanlink.net/tag/pdf_extract",
            "http://www.semanlink.net/tag/generative_adversarial_network",
            "http://www.semanlink.net/tag/genetic_algorithm",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1904.01947] Extracting Tables from Documents using Conditional Generative Adversarial Networks and Genetic Algorithms",
        "relatedDoc": [],
        "creationTime": "2020-04-02T15:48:47Z",
        "creationDate": "2020-04-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1904.01947"
        ],
        "arxiv_author": [
            "Matthew Zeigenfuse",
            "Mark Rowan",
            "Nataliya Le Vine"
        ],
        "arxiv_summary": "Extracting information from tables in documents presents a significant\nchallenge in many industries and in academic research. Existing methods which\ntake a bottom-up approach of integrating lines into cells and rows or columns\nneglect the available prior information relating to table structure. Our\nproposed method takes a top-down approach, first using a generative adversarial\nnetwork to map a table image into a standardised `skeleton' table form denoting\nthe approximate row and column borders without table content, then fitting\nrenderings of candidate latent table structures to the skeleton structure using\na distance measure optimised by a genetic algorithm.",
        "arxiv_firstAuthor": "Nataliya Le Vine",
        "arxiv_title": "Extracting Tables from Documents using Conditional Generative Adversarial Networks and Genetic Algorithms",
        "arxiv_num": "1904.01947",
        "arxiv_published": "2019-04-03T12:12:03Z",
        "arxiv_updated": "2019-04-03T12:12:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1306_6802_evaluation_measures",
        "tag": [
            "http://www.semanlink.net/tag/ml_evaluation",
            "http://www.semanlink.net/tag/hierarchical_classification",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/classification_relations_between_classes",
            "http://www.semanlink.net/tag/hierarchical_classification_evaluation"
        ],
        "comment": "How to properly evaluate hierarchical classification algorithms?\r\n\r\n> Classification errors in the upper levels of the hierarchy (e.g. when wrongly\r\nclassifying a document of the class music into the class food) are more severe\r\nthan those in deeper levels (e.g. when classifying a document from progressive\r\nrock as alternative rock).",
        "title": "[1306.6802] Evaluation Measures for Hierarchical Classification: a unified view and novel approaches",
        "relatedDoc": [],
        "creationTime": "2020-09-01T23:46:48Z",
        "creationDate": "2020-09-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/1306.6802"
        ],
        "arxiv_author": [
            "Eric Gaussier",
            "Ion Androutsopoulos",
            "Ioannis Partalas",
            "Georgios Paliouras",
            "Aris Kosmopoulos"
        ],
        "arxiv_summary": "Hierarchical classification addresses the problem of classifying items into a\nhierarchy of classes. An important issue in hierarchical classification is the\nevaluation of different classification algorithms, which is complicated by the\nhierarchical relations among the classes. Several evaluation measures have been\nproposed for hierarchical classification using the hierarchy in different ways.\nThis paper studies the problem of evaluation in hierarchical classification by\nanalyzing and abstracting the key components of the existing performance\nmeasures. It also proposes two alternative generic views of hierarchical\nevaluation and introduces two corresponding novel measures. The proposed\nmeasures, along with the state-of-the art ones, are empirically tested on three\nlarge datasets from the domain of text classification. The empirical results\nillustrate the undesirable behavior of existing approaches and how the proposed\nmethods overcome most of these methods across a range of cases.",
        "arxiv_firstAuthor": "Aris Kosmopoulos",
        "arxiv_title": "Evaluation Measures for Hierarchical Classification: a unified view and novel approaches",
        "arxiv_num": "1306.6802",
        "arxiv_published": "2013-06-28T11:49:53Z",
        "arxiv_updated": "2013-07-01T17:33:58Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1503_02406_deep_learning_and_",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/naftali_tishby",
            "http://www.semanlink.net/tag/information_bottleneck_method",
            "http://www.semanlink.net/tag/information_theory_and_deep_learning"
        ],
        "comment": "> Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN.\r\n",
        "title": "[1503.02406] Deep Learning and the Information Bottleneck Principle",
        "relatedDoc": [],
        "creationTime": "2019-08-15T17:07:31Z",
        "creationDate": "2019-08-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/1503.02406"
        ],
        "arxiv_author": [
            "Naftali Tishby",
            "Noga Zaslavsky"
        ],
        "arxiv_summary": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the\ninformation bottleneck (IB) principle. We first show that any DNN can be\nquantified by the mutual information between the layers and the input and\noutput variables. Using this representation we can calculate the optimal\ninformation theoretic limits of the DNN and obtain finite sample generalization\nbounds. The advantage of getting closer to the theoretical limit is\nquantifiable both by the generalization bound and by the network's simplicity.\nWe argue that both the optimal architecture, number of layers and\nfeatures/connections at each layer, are related to the bifurcation points of\nthe information bottleneck tradeoff, namely, relevant compression of the input\nlayer with respect to the output layer. The hierarchical representations at the\nlayered network naturally correspond to the structural phase transitions along\nthe information curve. We believe that this new insight can lead to new\noptimality bounds and deep learning algorithms.",
        "arxiv_firstAuthor": "Naftali Tishby",
        "arxiv_title": "Deep Learning and the Information Bottleneck Principle",
        "arxiv_num": "1503.02406",
        "arxiv_published": "2015-03-09T09:39:41Z",
        "arxiv_updated": "2015-03-09T09:39:41Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1902.11269",
        "tag": [
            "http://www.semanlink.net/tag/elmo",
            "http://www.semanlink.net/tag/contextualised_word_representations",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "**how to accelerate contextual representation learning**.\r\n\r\n> Contextual representation models are difficult to train due to the large parameter sizes and high computational complexity\r\n\r\n> We find that the softmax layer (the output layer) causes significant inefficiency due to the large vocabulary size.\r\nTherefore, we redesign the learning objectiv.\r\n> Specifically, the proposed approach bypasses the softmax layer by performing language modeling with dimension reduction, and allows the models to leverage pre-trained word embeddings.\r\nOur framework reduces the time spent on the output layer to a negligible level, eliminates almost all the trainable parameters of the softmax layer and performs language modeling without truncating the vocabulary.\r\nWhen applied to ELMo, our method achieves a 4 times speedup and eliminates 80% trainable parameters while achieving competitive performance on downstream tasks.\r\n\r\n**decouples learning contexts and words**\r\n\r\n> Instead of using\r\na softmax layer to predict the distribution of the\r\nmissing word, we utilize and extend the SEMFIT\r\nlayer (Kumar and Tsvetkov, 2018) to **predict the\r\nembedding of the missing word**.",
        "title": "[1902.11269] Efficient Contextual Representation Learning Without Softmax Layer",
        "relatedDoc": [],
        "creationTime": "2019-03-02T08:47:19Z",
        "creationDate": "2019-03-02",
        "bookmarkOf": [],
        "arxiv_author": [
            "Cho-Jui Hsieh",
            "Patrick H. Chen",
            "Kai-Wei Chang",
            "Liunian Harold Li"
        ],
        "arxiv_summary": "Contextual representation models have achieved great success in improving\nvarious downstream tasks. However, these language-model-based encoders are\ndifficult to train due to the large parameter sizes and high computational\ncomplexity. By carefully examining the training procedure, we find that the\nsoftmax layer (the output layer) causes significant inefficiency due to the\nlarge vocabulary size. Therefore, we redesign the learning objective and\npropose an efficient framework for training contextual representation models.\nSpecifically, the proposed approach bypasses the softmax layer by performing\nlanguage modeling with dimension reduction, and allows the models to leverage\npre-trained word embeddings. Our framework reduces the time spent on the output\nlayer to a negligible level, eliminates almost all the trainable parameters of\nthe softmax layer and performs language modeling without truncating the\nvocabulary. When applied to ELMo, our method achieves a 4 times speedup and\neliminates 80% trainable parameters while achieving competitive performance on\ndownstream tasks.",
        "arxiv_firstAuthor": "Liunian Harold Li",
        "arxiv_title": "Efficient Contextual Representation Learning Without Softmax Layer",
        "arxiv_num": "1902.11269",
        "arxiv_published": "2019-02-28T18:19:14Z",
        "arxiv_updated": "2019-02-28T18:19:14Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1903.05823",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/graph_convolutional_networks",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/patent_landscaping"
        ],
        "comment": "a **transformer encoder**\r\nfor analyzing textual data present in patent documents\r\nand a **graph convolutional network** for analyzing\r\npatent metadata.\r\n\r\nA benchmarking dataset for patent landscaping\r\nbased on patent trends reports published by the\r\nKorean Patent Office. Data acquisition using Google's BigQuery public datasets.\r\n\r\n10% improvement comparing to Google\u2019s proposed Automated Patent Landscaping.\r\n\r\nEmpirical analysis of the importance of features (text vs metadata, citations vs classification)\r\n\r\n",
        "title": "[1903.05823] Deep Patent Landscaping Model Using Transformer and Graph Embedding",
        "relatedDoc": [],
        "creationTime": "2019-03-18T10:20:46Z",
        "creationDate": "2019-03-18",
        "bookmarkOf": [],
        "arxiv_author": [
            "Eunjeong Lucy Park",
            "Seokkyu Choi",
            "Hyeonju Lee",
            "Sungchul Choi"
        ],
        "arxiv_summary": "Patent landscaping is a method used for searching related patents during a\nresearch and development (R&D) project. To avoid the risk of patent\ninfringement and to follow current trends in technology, patent landscaping is\na crucial task required during the early stages of an R&D project. As the\nprocess of patent landscaping requires advanced resources and can be tedious,\nthe demand for automated patent landscaping has been gradually increasing.\nHowever, a shortage of well-defined benchmark datasets and comparable models\nmakes it difficult to find related research studies. In this paper, we propose\nan automated patent landscaping model based on deep learning. To analyze the\ntext of patents, the proposed model uses a modified transformer structure. To\nanalyze the metadata of patents, we propose a graph embedding method that uses\na diffusion graph called Diff2Vec. Furthermore, we introduce four benchmark\ndatasets for comparing related research studies in patent landscaping. The\ndatasets are produced by querying Google BigQuery, based on a search formula\nfrom a Korean patent attorney. The obtained results indicate that the proposed\nmodel and datasets can attain state-of-the-art performance, as compared with\ncurrent patent landscaping models.",
        "arxiv_firstAuthor": "Seokkyu Choi",
        "arxiv_title": "Deep Patent Landscaping Model Using Transformer and Graph Embedding",
        "arxiv_num": "1903.05823",
        "arxiv_published": "2019-03-14T05:53:22Z",
        "arxiv_updated": "2019-11-22T00:54:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/07/2007_04612_concept_bottleneck",
        "tag": [
            "http://www.semanlink.net/tag/ai_stanford",
            "http://www.semanlink.net/tag/concept_bottleneck_models",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/human_in_the_loop"
        ],
        "comment": "> We seek to **learn models that we can interact with using high-level concepts**... \r\n>\r\n> We revisit the **classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label**. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction... These models allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.",
        "title": "[2007.04612] Concept Bottleneck Models",
        "relatedDoc": [],
        "creationTime": "2020-07-10T09:48:19Z",
        "creationDate": "2020-07-10",
        "bookmarkOf": [
            "https://arxiv.org/abs/2007.04612"
        ],
        "arxiv_author": [
            "Thao Nguyen",
            "Pang Wei Koh",
            "Stephen Mussmann",
            "Emma Pierson",
            "Yew Siang Tang",
            "Been Kim",
            "Percy Liang"
        ],
        "arxiv_summary": "We seek to learn models that we can interact with using high-level concepts:\nif the model did not think there was a bone spur in the x-ray, would it still\npredict severe arthritis? State-of-the-art models today do not typically\nsupport the manipulation of concepts like \"the existence of bone spurs\", as\nthey are trained end-to-end to go directly from raw input (e.g., pixels) to\noutput (e.g., arthritis severity). We revisit the classic idea of first\npredicting concepts that are provided at training time, and then using these\nconcepts to predict the label. By construction, we can intervene on these\n\\emph{concept bottleneck models} by editing their predicted concept values and\npropagating these changes to the final prediction. On x-ray grading and bird\nidentification, concept bottleneck models achieve competitive accuracy with\nstandard end-to-end models, while enabling interpretation in terms of\nhigh-level clinical concepts (\"bone spurs\") or bird attributes (\"wing color\").\nThese models also allow for richer human-model interaction: accuracy improves\nsignificantly if we can correct model mistakes on concepts at test time.",
        "arxiv_firstAuthor": "Pang Wei Koh",
        "arxiv_title": "Concept Bottleneck Models",
        "arxiv_num": "2007.04612",
        "arxiv_published": "2020-07-09T07:47:28Z",
        "arxiv_updated": "2020-07-09T07:47:28Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1806_06478_co_training_embedd",
        "tag": [
            "http://www.semanlink.net/tag/using_literal_descriptions_of_entities",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/co_training",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_alignment",
            "http://www.semanlink.net/tag/cross_lingual_nlp"
        ],
        "comment": "",
        "title": "[1806.06478] Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment",
        "relatedDoc": [],
        "creationTime": "2020-09-06T16:59:29Z",
        "creationDate": "2020-09-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/1806.06478"
        ],
        "arxiv_author": [
            "Yingtao Tian",
            "Steven Skiena",
            "Kai-Wei Chang",
            "Muhao Chen",
            "Carlo Zaniolo"
        ],
        "arxiv_summary": "Multilingual knowledge graph (KG) embeddings provide latent semantic\nrepresentations of entities and structured knowledge with cross-lingual\ninferences, which benefit various knowledge-driven cross-lingual NLP tasks.\nHowever, precisely learning such cross-lingual inferences is usually hindered\nby the low coverage of entity alignment in many KGs. Since many multilingual\nKGs also provide literal descriptions of entities, in this paper, we introduce\nan embedding-based approach which leverages a weakly aligned multilingual KG\nfor semi-supervised cross-lingual learning using entity descriptions. Our\napproach performs co-training of two embedding models, i.e. a multilingual KG\nembedding model and a multilingual literal description embedding model. The\nmodels are trained on a large Wikipedia-based trilingual dataset where most\nentity alignment is unknown to training. Experimental results show that the\nperformance of the proposed approach on the entity alignment task improves at\neach iteration of co-training, and eventually reaches a stage at which it\nsignificantly surpasses previous approaches. We also show that our approach has\npromising abilities for zero-shot entity alignment, and cross-lingual KG\ncompletion.",
        "arxiv_firstAuthor": "Muhao Chen",
        "arxiv_title": "Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment",
        "arxiv_num": "1806.06478",
        "arxiv_published": "2018-06-18T02:06:46Z",
        "arxiv_updated": "2018-06-18T02:06:46Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/01/1911_03681_e_bert_efficient_",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_embeddings",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/text_kg_and_embeddings"
        ],
        "comment": "> way of **injecting factual knowledge about entities into the pretrained BERT model**.\r\n\r\n(Feeding entity vectors\r\ninto BERT as if they\r\nwere wordpiece vectors without additional encoder\r\npretrainin)\r\n\r\n>\r\n> **We align [Wikipedia2Vec](tag:wikipedia2vec) entity vectors (Yamada et al., 2016) with BERT's native wordpiece vector space and use the aligned entity vectors as if they were wordpiece vectors**. The resulting entity-enhanced version of BERT (called E-BERT) is similar in spirit to [ERNIE](tag:ernie) (Zhang et al., 2019) and [KnowBert](tag:knowbert) (Peters et al., 2019), but it **requires no expensive further pretraining of the BERT encoder**.\r\n>\r\n> Our vector space alignment strategy is inspired by\r\ncross-lingual word vector alignment\r\n\r\nRelated work on Entity-enhanced BERT:\r\n\r\n> (ERNIE and KnowBert) are based on the design principle\r\nthat BERT be adapted to entity vectors. They introduce\r\nnew encoder layers to feed pretrained entity\r\nvectors into the Transformer, and they require additional\r\npretraining to integrate the new parameters.\r\nIn contrast, E-BERT\u2019s design principle is that entity\r\nvectors be adapted to BERT.\r\n>\r\n> Two other knowledge-enhanced MLMs are [[1911.06136] KEPLER](doc:2020/11/1911_06136_kepler_a_unified_)\r\n(Wang et al., 2019c) and K-Adapter (Wang\r\net al., 2020)... Their factual knowledge\r\ndoes not stem from entity vectors \u2013 instead, they\r\nare trained in a multi-task setting on relation classification\r\nand knowledge base completion.\r\n\r\nNot to be cofounded with [[2009.02835] E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce](doc:2020/12/2009_02835_e_bert_a_phrase_a)",
        "title": "[1911.03681] E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/11/1911_06136_kepler_a_unified_",
            "http://www.semanlink.net/doc/2020/12/2009_02835_e_bert_a_phrase_a"
        ],
        "creationTime": "2021-01-12T18:31:21Z",
        "creationDate": "2021-01-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.03681"
        ],
        "arxiv_author": [
            "Nina Poerner",
            "Hinrich Sch\u00fctze",
            "Ulli Waltinger"
        ],
        "arxiv_summary": "We present a novel way of injecting factual knowledge about entities into the\npretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity\nvectors (Yamada et al., 2016) with BERT's native wordpiece vector space and use\nthe aligned entity vectors as if they were wordpiece vectors. The resulting\nentity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE\n(Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no\nexpensive further pretraining of the BERT encoder. We evaluate E-BERT on\nunsupervised question answering (QA), supervised relation classification (RC)\nand entity linking (EL). On all three tasks, E-BERT outperforms BERT and other\nbaselines. We also show quantitatively that the original BERT model is overly\nreliant on the surface form of entity names (e.g., guessing that someone with\nan Italian-sounding name speaks Italian), and that E-BERT mitigates this\nproblem.",
        "arxiv_firstAuthor": "Nina Poerner",
        "arxiv_title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT",
        "arxiv_num": "1911.03681",
        "arxiv_published": "2019-11-09T13:08:25Z",
        "arxiv_updated": "2020-05-01T09:19:35Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/1807_04905_ultra_fine_entity_",
        "tag": [
            "http://www.semanlink.net/tag/entity_type_prediction",
            "http://www.semanlink.net/tag/acl_2018",
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> a new entity typing task:\r\ngiven a sentence with an entity mention,\r\nthe goal is to predict a set of free-form\r\nphrases (e.g. skyscraper, songwriter, or\r\ncriminal) that describe appropriate types\r\nfor the target entity",
        "title": "[1807.04905] Ultra-Fine Entity Typing",
        "relatedDoc": [],
        "creationTime": "2021-06-22T10:50:58Z",
        "creationDate": "2021-06-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/1807.04905"
        ],
        "arxiv_author": [
            "Omer Levy",
            "Luke Zettlemoyer",
            "Yejin Choi",
            "Eunsol Choi"
        ],
        "arxiv_summary": "We introduce a new entity typing task: given a sentence with an entity\nmention, the goal is to predict a set of free-form phrases (e.g. skyscraper,\nsongwriter, or criminal) that describe appropriate types for the target entity.\nThis formulation allows us to use a new type of distant supervision at large\nscale: head words, which indicate the type of the noun phrases they appear in.\nWe show that these ultra-fine types can be crowd-sourced, and introduce new\nevaluation sets that are much more diverse and fine-grained than existing\nbenchmarks. We present a model that can predict open types, and is trained\nusing a multitask objective that pools our new head-word supervision with prior\nsupervision from entity linking. Experimental results demonstrate that our\nmodel is effective in predicting entity types at varying granularity; it\nachieves state of the art performance on an existing fine-grained entity typing\nbenchmark, and sets baselines for our newly-introduced datasets. Our data and\nmodel can be downloaded from: http://nlp.cs.washington.edu/entity_type",
        "arxiv_firstAuthor": "Eunsol Choi",
        "arxiv_title": "Ultra-Fine Entity Typing",
        "arxiv_num": "1807.04905",
        "arxiv_published": "2018-07-13T04:19:03Z",
        "arxiv_updated": "2018-07-13T04:19:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/08/1812_02956_lnemlc_label_netw",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/multi_label_classification"
        ],
        "comment": "> low-complexity approach to multi-label classification built on top of two intuitions that **embedding a label space** may improve classification quality and that **label networks are a viable source of information** in multi-label problems",
        "title": "[1812.02956] LNEMLC: Label Network Embeddings for Multi-Label Classification",
        "relatedDoc": [],
        "creationTime": "2020-08-12T17:07:25Z",
        "creationDate": "2020-08-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/1812.02956"
        ],
        "arxiv_author": [
            "Piotr Szyma\u0144ski",
            "Nitesh Chawla",
            "Tomasz Kajdanowicz"
        ],
        "arxiv_summary": "Multi-label classification aims to classify instances with discrete\nnon-exclusive labels. Most approaches on multi-label classification focus on\neffective adaptation or transformation of existing binary and multi-class\nlearning approaches but fail in modelling the joint probability of labels or do\nnot preserve generalization abilities for unseen label combinations. To address\nthese issues we propose a new multi-label classification scheme, LNEMLC - Label\nNetwork Embedding for Multi-Label Classification, that embeds the label network\nand uses it to extend input space in learning and inference of any base\nmulti-label classifier. The approach allows capturing of labels' joint\nprobability at low computational complexity providing results comparable to the\nbest methods reported in the literature. We demonstrate how the method reveals\nstatistically significant improvements over the simple kNN baseline classifier.\nWe also provide hints for selecting the robust configuration that works\nsatisfactorily across data domains.",
        "arxiv_firstAuthor": "Piotr Szyma\u0144ski",
        "arxiv_title": "LNEMLC: Label Network Embeddings for Multi-Label Classification",
        "arxiv_num": "1812.02956",
        "arxiv_published": "2018-12-07T09:30:18Z",
        "arxiv_updated": "2019-01-01T21:11:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1905_07854_kgat_knowledge_gr",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_knowledge_graphs",
            "http://www.semanlink.net/tag/recommender_systems"
        ],
        "comment": "",
        "title": "[1905.07854] KGAT: Knowledge Graph Attention Network for Recommendation",
        "relatedDoc": [],
        "creationTime": "2019-08-23T00:33:53Z",
        "creationDate": "2019-08-23",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.07854"
        ],
        "arxiv_author": [
            "Xiangnan He",
            "Xiang Wang",
            "Yixin Cao",
            "Tat-Seng Chua",
            "Meng Liu"
        ],
        "arxiv_summary": "To provide more accurate, diverse, and explainable recommendation, it is\ncompulsory to go beyond modeling user-item interactions and take side\ninformation into account. Traditional methods like factorization machine (FM)\ncast it as a supervised learning problem, which assumes each interaction as an\nindependent instance with side information encoded. Due to the overlook of the\nrelations among instances or items (e.g., the director of a movie is also an\nactor of another movie), these methods are insufficient to distill the\ncollaborative signal from the collective behaviors of users. In this work, we\ninvestigate the utility of knowledge graph (KG), which breaks down the\nindependent interaction assumption by linking items with their attributes. We\nargue that in such a hybrid structure of KG and user-item graph, high-order\nrelations --- which connect two items with one or multiple linked attributes\n--- are an essential factor for successful recommendation. We propose a new\nmethod named Knowledge Graph Attention Network (KGAT) which explicitly models\nthe high-order connectivities in KG in an end-to-end fashion. It recursively\npropagates the embeddings from a node's neighbors (which can be users, items,\nor attributes) to refine the node's embedding, and employs an attention\nmechanism to discriminate the importance of the neighbors. Our KGAT is\nconceptually advantageous to existing KG-based recommendation methods, which\neither exploit high-order relations by extracting paths or implicitly modeling\nthem with regularization. Empirical results on three public benchmarks show\nthat KGAT significantly outperforms state-of-the-art methods like Neural FM and\nRippleNet. Further studies verify the efficacy of embedding propagation for\nhigh-order relation modeling and the interpretability benefits brought by the\nattention mechanism.",
        "arxiv_firstAuthor": "Xiang Wang",
        "arxiv_title": "KGAT: Knowledge Graph Attention Network for Recommendation",
        "arxiv_num": "1905.07854",
        "arxiv_published": "2019-05-20T03:08:11Z",
        "arxiv_updated": "2019-06-08T02:49:37Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1506.08422",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/latent_dirichlet_allocation",
            "http://www.semanlink.net/tag/topic_embeddings"
        ],
        "comment": "Topic2Vec aims at learning topic representations along with word representations. Considering the simplicity and efficient solution, we just follow the optimization scheme that used in Word2Vec",
        "title": "[1506.08422] Topic2Vec: Learning Distributed Representations of Topics",
        "relatedDoc": [],
        "creationTime": "2017-12-03T17:36:27Z",
        "creationDate": "2017-12-03",
        "bookmarkOf": [],
        "arxiv_author": [
            "Li-Qiang Niu",
            "Xin-Yu Dai"
        ],
        "arxiv_summary": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents\nplays an important role in nature language processing and machine learning\nareas. However, the probability distribution from LDA only describes the\nstatistical relationship of occurrences in the corpus and usually in practice,\nprobability is not the best choice for feature representations. Recently,\nembedding methods have been proposed to represent words and documents by\nlearning essential concepts and representations, such as Word2Vec and Doc2Vec.\nThe embedded representations have shown more effectiveness than LDA-style\nrepresentations in many tasks. In this paper, we propose the Topic2Vec approach\nwhich can learn topic representations in the same semantic vector space with\nwords, as an alternative to probability. The experimental results show that\nTopic2Vec achieves interesting and meaningful results.",
        "arxiv_firstAuthor": "Li-Qiang Niu",
        "arxiv_title": "Topic2Vec: Learning Distributed Representations of Topics",
        "arxiv_num": "1506.08422",
        "arxiv_published": "2015-06-28T16:17:40Z",
        "arxiv_updated": "2015-06-28T16:17:40Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/12/2009_02835_e_bert_a_phrase_a",
        "tag": [
            "http://www.semanlink.net/tag/aspect_detection",
            "http://www.semanlink.net/tag/domain_specific_bert",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/e_commerce_data",
            "http://www.semanlink.net/tag/bert_kb"
        ],
        "comment": "E-BERT, pre-training framework for product data.\r\n\r\n1. to benefit from phrase-level knowledge: Adaptive Hybrid Masking, a new masking strategy, which allows the model to adaptively switch from learning preliminary word knowledge to learning complex phrases\r\n2. leveraging product-level knowledge: training E-BERT to\r\npredict a product\u2019s associated neighbors (product association)\r\n\r\nResources used:\r\n\r\n- description of millions of products from the amazon dataset (title, description, reviews)\r\n- e-commerce phrases: extracted from above dataset using [AutoPhrase](doc:2020/12/autophrase_automated_phrase_mi)\r\n- product association graph: pairs of substitutable and complementary products extracted from amazon dataset\r\n\r\nNot to be confounded with [[1911.03681] E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT](doc:2021/01/1911_03681_e_bert_efficient_)",
        "title": "[2009.02835] E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/12/autophrase_automated_phrase_mi",
            "http://www.semanlink.net/doc/2021/01/1911_03681_e_bert_efficient_"
        ],
        "creationTime": "2020-12-14T11:10:29Z",
        "creationDate": "2020-12-14",
        "bookmarkOf": [
            "https://arxiv.org/abs/2009.02835"
        ],
        "arxiv_author": [
            "Zuohui Fu",
            "Fuzhen Zhuang",
            "Yanchi Liu",
            "Zixuan Yuan",
            "Hui Xiong",
            "Pengyang Wang",
            "Denghui Zhang",
            "Haifeng Chen"
        ],
        "arxiv_summary": "Pre-trained language models such as BERT have achieved great success in a\nbroad range of natural language processing tasks. However, BERT cannot well\nsupport E-commerce related tasks due to the lack of two levels of domain\nknowledge, i.e., phrase-level and product-level. On one hand, many E-commerce\ntasks require an accurate understanding of domain phrases, whereas such\nfine-grained phrase-level knowledge is not explicitly modeled by BERT's\ntraining objective. On the other hand, product-level knowledge like product\nassociations can enhance the language modeling of E-commerce, but they are not\nfactual knowledge thus using them indiscriminately may introduce noise. To\ntackle the problem, we propose a unified pre-training framework, namely,\nE-BERT. Specifically, to preserve phrase-level knowledge, we introduce Adaptive\nHybrid Masking, which allows the model to adaptively switch from learning\npreliminary word knowledge to learning complex phrases, based on the fitting\nprogress of two modes. To utilize product-level knowledge, we introduce\nNeighbor Product Reconstruction, which trains E-BERT to predict a product's\nassociated neighbors with a denoising cross attention layer. Our investigation\nreveals promising results in four downstream tasks, i.e., review-based question\nanswering, aspect extraction, aspect sentiment classification, and product\nclassification.",
        "arxiv_firstAuthor": "Denghui Zhang",
        "arxiv_title": "E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce",
        "arxiv_num": "2009.02835",
        "arxiv_published": "2020-09-07T00:15:36Z",
        "arxiv_updated": "2020-09-10T23:00:16Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1710.06632",
        "tag": [
            "http://www.semanlink.net/tag/lexical_ambiguity",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/sense_embeddings",
            "http://www.semanlink.net/tag/using_word_embedding"
        ],
        "comment": "",
        "title": "[1710.06632] Towards a Seamless Integration of Word Senses into Downstream NLP Applications",
        "relatedDoc": [],
        "creationTime": "2018-10-09T15:08:40Z",
        "creationDate": "2018-10-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Roberto Navigli",
            "Nigel Collier",
            "Mohammad Taher Pilehvar",
            "Jose Camacho-Collados"
        ],
        "arxiv_summary": "Lexical ambiguity can impede NLP systems from accurate understanding of\nsemantics. Despite its potential benefits, the integration of sense-level\ninformation into NLP systems has remained understudied. By incorporating a\nnovel disambiguation algorithm into a state-of-the-art classification model, we\ncreate a pipeline to integrate sense-level information into downstream NLP\napplications. We show that a simple disambiguation of the input text can lead\nto consistent performance improvement on multiple topic categorization and\npolarity detection datasets, particularly when the fine granularity of the\nunderlying sense inventory is reduced and the document is sufficiently large.\nOur results also point to the need for sense representation research to focus\nmore on in vivo evaluations which target the performance in downstream NLP\napplications rather than artificial benchmarks.",
        "arxiv_firstAuthor": "Mohammad Taher Pilehvar",
        "arxiv_title": "Towards a Seamless Integration of Word Senses into Downstream NLP Applications",
        "arxiv_num": "1710.06632",
        "arxiv_published": "2017-10-18T09:13:06Z",
        "arxiv_updated": "2017-10-18T09:13:06Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/1802_05930_learning_beyond_da",
        "tag": [
            "http://www.semanlink.net/tag/text_kg_and_embeddings",
            "http://www.semanlink.net/tag/deep_learning_attention",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> we propose to enhance learning models with world knowledge in the form of **Knowledge Graph fact triples for NLP tasks**. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism.\r\n\r\nRelated [blog post](https://medium.com/@anshumanmourya/learning-beyond-datasets-knowledge-graph-augmented-neural-networks-for-natural-language-b937ba49f2e5)",
        "title": "[1802.05930] Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing",
        "relatedDoc": [],
        "creationTime": "2020-10-02T01:01:15Z",
        "creationDate": "2020-10-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1802.05930"
        ],
        "arxiv_author": [
            "Ambedkar Dukkipati",
            "Somnath Basu Roy Chowdhury",
            "K M Annervaz"
        ],
        "arxiv_summary": "Machine Learning has been the quintessential solution for many AI problems,\nbut learning is still heavily dependent on the specific training data. Some\nlearning models can be incorporated with a prior knowledge in the Bayesian set\nup, but these learning models do not have the ability to access any organised\nworld knowledge on demand. In this work, we propose to enhance learning models\nwith world knowledge in the form of Knowledge Graph (KG) fact triples for\nNatural Language Processing (NLP) tasks. Our aim is to develop a deep learning\nmodel that can extract relevant prior support facts from knowledge graphs\ndepending on the task using attention mechanism. We introduce a\nconvolution-based model for learning representations of knowledge graph entity\nand relation clusters in order to reduce the attention space. We show that the\nproposed method is highly scalable to the amount of prior information that has\nto be processed and can be applied to any generic NLP task. Using this method\nwe show significant improvement in performance for text classification with\nNews20, DBPedia datasets and natural language inference with Stanford Natural\nLanguage Inference (SNLI) dataset. We also demonstrate that a deep learning\nmodel can be trained well with substantially less amount of labeled training\ndata, when it has access to organised world knowledge in the form of knowledge\ngraph.",
        "arxiv_firstAuthor": "K M Annervaz",
        "arxiv_title": "Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing",
        "arxiv_num": "1802.05930",
        "arxiv_published": "2018-02-16T13:38:00Z",
        "arxiv_updated": "2018-05-21T03:44:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1709.03856",
        "tag": [
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/starspace"
        ],
        "comment": "",
        "title": "[1709.03856] StarSpace: Embed All The Things!",
        "relatedDoc": [],
        "creationTime": "2018-05-13T17:51:48Z",
        "creationDate": "2018-05-13",
        "bookmarkOf": [],
        "arxiv_author": [
            "Keith Adams",
            "Antoine Bordes",
            "Adam Fisch",
            "Jason Weston",
            "Sumit Chopra",
            "Ledell Wu"
        ],
        "arxiv_summary": "We present StarSpace, a general-purpose neural embedding model that can solve\na wide variety of problems: labeling tasks such as text classification, ranking\ntasks such as information retrieval/web search, collaborative filtering-based\nor content-based recommendation, embedding of multi-relational graphs, and\nlearning word, sentence or document level embeddings. In each case the model\nworks by embedding those entities comprised of discrete features and comparing\nthem against each other -- learning similarities dependent on the task.\nEmpirical results on a number of tasks show that StarSpace is highly\ncompetitive with existing methods, whilst also being generally applicable to\nnew cases where those methods are not.",
        "arxiv_firstAuthor": "Ledell Wu",
        "arxiv_title": "StarSpace: Embed All The Things!",
        "arxiv_num": "1709.03856",
        "arxiv_published": "2017-09-12T14:16:56Z",
        "arxiv_updated": "2017-11-21T02:59:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/04/1910_02227_making_sense_of_se",
        "tag": [
            "http://www.semanlink.net/tag/artificial_general_intelligence",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/google_deepmind"
        ],
        "comment": "> what does it mean to \u201cmake sense\u201d\r\nof a sensory sequence? Our answer is that making sense means constructing a symbolic theory containing a set\r\nof objects that persist over time, with attributes that change over time, according to general laws. This theory\r\nmust both explain the sensory input, and satisfy unity conditions [the\r\nconstituents of our theory \u2013 objects, properties, and atoms \u2013 must be integrated into a coherent whole]\r\n\r\nSequel: [Making sense of raw input](doc:2021/05/making_sense_of_raw_input)",
        "title": "[1910.02227] Making sense of sensory input",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/05/making_sense_of_raw_input"
        ],
        "creationTime": "2021-04-10T19:09:06Z",
        "creationDate": "2021-04-10",
        "bookmarkOf": [
            "https://arxiv.org/abs/1910.02227"
        ],
        "arxiv_author": [
            "Pushmeet Kohli",
            "Marek Sergot",
            "Jose Hernandez-Orallo",
            "Richard Evans",
            "Johannes Welbl"
        ],
        "arxiv_summary": "This paper attempts to answer a central question in unsupervised learning:\nwhat does it mean to \"make sense\" of a sensory sequence? In our formalization,\nmaking sense involves constructing a symbolic causal theory that both explains\nthe sensory sequence and also satisfies a set of unity conditions. The unity\nconditions insist that the constituents of the causal theory -- objects,\nproperties, and laws -- must be integrated into a coherent whole. On our\naccount, making sense of sensory input is a type of program synthesis, but it\nis unsupervised program synthesis.\nOur second contribution is a computer implementation, the Apperception\nEngine, that was designed to satisfy the above requirements. Our system is able\nto produce interpretable human-readable causal theories from very small amounts\nof data, because of the strong inductive bias provided by the unity conditions.\nA causal theory produced by our system is able to predict future sensor\nreadings, as well as retrodict earlier readings, and impute (fill in the blanks\nof) missing sensory readings, in any combination.\nWe tested the engine in a diverse variety of domains, including cellular\nautomata, rhythms and simple nursery tunes, multi-modal binding problems,\nocclusion tasks, and sequence induction intelligence tests. In each domain, we\ntest our engine's ability to predict future sensor values, retrodict earlier\nsensor values, and impute missing sensory data. The engine performs well in all\nthese domains, significantly out-performing neural net baselines. We note in\nparticular that in the sequence induction intelligence tests, our system\nachieved human-level performance. This is notable because our system is not a\nbespoke system designed specifically to solve intelligence tests, but a\ngeneral-purpose system that was designed to make sense of any sensory sequence.",
        "arxiv_firstAuthor": "Richard Evans",
        "arxiv_title": "Making sense of sensory input",
        "arxiv_num": "1910.02227",
        "arxiv_published": "2019-10-05T07:48:55Z",
        "arxiv_updated": "2020-07-14T03:16:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/2004_10151_experience_grounds",
        "tag": [
            "http://www.semanlink.net/tag/meaning_in_nlp",
            "http://www.semanlink.net/tag/grounded_language_learning",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/yoshua_bengio",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2004.10151] Experience Grounds Language",
        "relatedDoc": [],
        "creationTime": "2020-04-22T16:52:37Z",
        "creationDate": "2020-04-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.10151"
        ],
        "arxiv_author": [
            "Mirella Lapata",
            "Angeliki Lazaridou",
            "Joyce Chai",
            "Yoshua Bengio",
            "Jonathan May",
            "Joseph Turian",
            "Yonatan Bisk",
            "Aleksandr Nisnevich",
            "Jesse Thomason",
            "Ari Holtzman",
            "Nicolas Pinto",
            "Jacob Andreas"
        ],
        "arxiv_summary": "Successful linguistic communication relies on a shared experience of the\nworld, and it is this shared experience that makes utterances meaningful.\nDespite the incredible effectiveness of language processing models trained on\ntext alone, today's best systems still make mistakes that arise from a failure\nto relate language to the physical world it describes and to the social\ninteractions it facilitates.\nNatural Language Processing is a diverse field, and progress throughout its\ndevelopment has come from new representational theories, modeling techniques,\ndata collection paradigms, and tasks. We posit that the present success of\nrepresentation learning approaches trained on large text corpora can be deeply\nenriched from the parallel tradition of research on the contextual and social\nnature of language.\nIn this article, we consider work on the contextual foundations of language:\ngrounding, embodiment, and social interaction. We describe a brief history and\npossible progression of how contextual information can factor into our\nrepresentations, with an eye towards how this integration can move the field\nforward and where it is currently being pioneered. We believe this framing will\nserve as a roadmap for truly contextual language understanding.",
        "arxiv_firstAuthor": "Yonatan Bisk",
        "arxiv_title": "Experience Grounds Language",
        "arxiv_num": "2004.10151",
        "arxiv_published": "2020-04-21T16:56:27Z",
        "arxiv_updated": "2020-04-21T16:56:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "http://www.semanlink.net/2001/00/semanlink-schema#ArxivDoc",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1412.6623",
        "tag": [
            "http://www.semanlink.net/tag/gaussian_embedding",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/andrew_mccallum"
        ],
        "comment": "> Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages\r\n\r\n> Novel word embedding algorithms that embed words directly as Gaussian distributional potential functions in an infinite dimensional function space. This allows us to map word types not only to vectors but to soft regions in space, modeling uncertainty, inclusion, and entailment, as well as providing a rich geometry of the latent space.",
        "title": "[1412.6623] Word Representations via Gaussian Embedding",
        "relatedDoc": [],
        "creationTime": "2018-01-28T17:27:24Z",
        "creationDate": "2018-01-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Andrew McCallum",
            "Luke Vilnis"
        ],
        "arxiv_summary": "Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation.",
        "arxiv_firstAuthor": "Luke Vilnis",
        "arxiv_title": "Word Representations via Gaussian Embedding",
        "arxiv_num": "1412.6623",
        "arxiv_published": "2014-12-20T07:42:40Z",
        "arxiv_updated": "2015-05-01T10:14:58Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_2002_04688_fastai_a_layered_",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/fast_ai",
            "http://www.semanlink.net/tag/api",
            "http://www.semanlink.net/tag/jeremy_howard"
        ],
        "comment": "Paper describing the fast.ai v2 API",
        "title": "[2002.04688] fastai: A Layered API for Deep Learning",
        "relatedDoc": [],
        "creationTime": "2020-02-13T21:07:29Z",
        "creationDate": "2020-02-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.04688"
        ],
        "arxiv_author": [
            "Jeremy Howard",
            "Sylvain Gugger"
        ],
        "arxiv_summary": "fastai is a deep learning library which provides practitioners with\nhigh-level components that can quickly and easily provide state-of-the-art\nresults in standard deep learning domains, and provides researchers with\nlow-level components that can be mixed and matched to build new approaches. It\naims to do both things without substantial compromises in ease of use,\nflexibility, or performance. This is possible thanks to a carefully layered\narchitecture, which expresses common underlying patterns of many deep learning\nand data processing techniques in terms of decoupled abstractions. These\nabstractions can be expressed concisely and clearly by leveraging the dynamism\nof the underlying Python language and the flexibility of the PyTorch library.\nfastai includes: a new type dispatch system for Python along with a semantic\ntype hierarchy for tensors; a GPU-optimized computer vision library which can\nbe extended in pure Python; an optimizer which refactors out the common\nfunctionality of modern optimizers into two basic pieces, allowing optimization\nalgorithms to be implemented in 4-5 lines of code; a novel 2-way callback\nsystem that can access any part of the data, model, or optimizer and change it\nat any point during training; a new data block API; and much more. We have used\nthis library to successfully create a complete deep learning course, which we\nwere able to write more quickly than using previous approaches, and the code\nwas more clear. The library is already in wide use in research, industry, and\nteaching. NB: This paper covers fastai v2, which is currently in pre-release at\nhttp://dev.fast.ai/",
        "arxiv_firstAuthor": "Jeremy Howard",
        "arxiv_title": "fastai: A Layered API for Deep Learning",
        "arxiv_num": "2002.04688",
        "arxiv_published": "2020-02-11T21:16:48Z",
        "arxiv_updated": "2020-02-16T18:17:51Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_1909_07606_k_bert_enabling_l",
        "tag": [
            "http://www.semanlink.net/tag/language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/nlp_using_knowledge_graphs",
            "http://www.semanlink.net/tag/knowledge_graph_deep_learning"
        ],
        "comment": "a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge",
        "title": "[1909.07606] K-BERT: Enabling Language Representation with Knowledge Graph",
        "relatedDoc": [],
        "creationTime": "2020-03-08T22:54:15Z",
        "creationDate": "2020-03-08",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.07606"
        ],
        "arxiv_author": [
            "Haotang Deng",
            "Zhiruo Wang",
            "Weijie Liu",
            "Zhe Zhao",
            "Qi Ju",
            "Peng Zhou",
            "Ping Wang"
        ],
        "arxiv_summary": "Pre-trained language representation models, such as BERT, capture a general\nlanguage representation from large-scale corpora, but lack domain-specific\nknowledge. When reading a domain text, experts make inferences with relevant\nknowledge. For machines to achieve this capability, we propose a\nknowledge-enabled language representation model (K-BERT) with knowledge graphs\n(KGs), in which triples are injected into the sentences as domain knowledge.\nHowever, too much knowledge incorporation may divert the sentence from its\ncorrect meaning, which is called knowledge noise (KN) issue. To overcome KN,\nK-BERT introduces soft-position and visible matrix to limit the impact of\nknowledge. K-BERT can easily inject domain knowledge into the models by\nequipped with a KG without pre-training by-self because it is capable of\nloading model parameters from the pre-trained BERT. Our investigation reveals\npromising results in twelve NLP tasks. Especially in domain-specific tasks\n(including finance, law, and medicine), K-BERT significantly outperforms BERT,\nwhich demonstrates that K-BERT is an excellent choice for solving the\nknowledge-driven problems that require experts.",
        "arxiv_firstAuthor": "Weijie Liu",
        "arxiv_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
        "arxiv_num": "1909.07606",
        "arxiv_published": "2019-09-17T06:16:04Z",
        "arxiv_updated": "2019-09-17T06:16:04Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1607.07956",
        "tag": [
            "http://www.semanlink.net/tag/category_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_embeddings",
            "http://www.semanlink.net/tag/nlp_hierarchical_text_classification"
        ],
        "comment": "a framework that embeds entities and categories into a semantic space by integrating structured\r\nknowledge and taxonomy hierarchy from large knowledge bases.\r\n\r\ntwo methods:\r\n\r\n1. Category Embedding model): it replaces the entities in the context with their directly\r\nlabeled categories to build categories\u2019 context; \r\n2. Hierarchical Category Embedding: it\r\nfurther incorporates all ancestor categories of the context entities to utilize the hierarchical information.",
        "title": "[1607.07956] Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification (COLING 2016)",
        "relatedDoc": [],
        "creationTime": "2018-05-12T16:41:35Z",
        "creationDate": "2018-05-12",
        "bookmarkOf": [],
        "arxiv_author": [
            "Rahul Iyer",
            "Tian Tian",
            "Ronghuo Zheng",
            "Yuezhang Li",
            "Katia Sycara",
            "Zhiting Hu"
        ],
        "arxiv_summary": "Due to the lack of structured knowledge applied in learning distributed\nrepresentation of cate- gories, existing work cannot incorporate category\nhierarchies into entity information. We propose a framework that embeds\nentities and categories into a semantic space by integrating structured\nknowledge and taxonomy hierarchy from large knowledge bases. The framework\nallows to com- pute meaningful semantic relatedness between entities and\ncategories. Our framework can han- dle both single-word concepts and\nmultiple-word concepts with superior performance on concept categorization and\nyield state of the art results on dataless hierarchical classification.",
        "arxiv_firstAuthor": "Yuezhang Li",
        "arxiv_title": "Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification",
        "arxiv_num": "1607.07956",
        "arxiv_published": "2016-07-27T04:51:17Z",
        "arxiv_updated": "2016-07-27T04:51:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/03/1901_04085_passage_re_ranking",
        "tag": [
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ranking_information_retrieval",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/machine_learned_ranking"
        ],
        "comment": "a simple re-implementation of BERT for query-based passage re-ranking\r\n\r\n[\"Slides of our WSDM 2021 tutorial \"Pretrained Transformers for Text Ranking: BERT and Beyond\"](doc:2021/03/rodrigo_nogueira_sur_twitter_)",
        "title": "[1901.04085] Passage Re-ranking with BERT",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/03/rodrigo_nogueira_sur_twitter_"
        ],
        "creationTime": "2021-03-26T01:49:42Z",
        "creationDate": "2021-03-26",
        "bookmarkOf": [
            "https://arxiv.org/abs/1901.04085"
        ],
        "arxiv_author": [
            "Rodrigo Nogueira",
            "Kyunghyun Cho"
        ],
        "arxiv_summary": "Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert",
        "arxiv_firstAuthor": "Rodrigo Nogueira",
        "arxiv_title": "Passage Re-ranking with BERT",
        "arxiv_num": "1901.04085",
        "arxiv_published": "2019-01-13T23:27:58Z",
        "arxiv_updated": "2020-04-14T14:57:40Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1503.00759",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/statistical_relational_learning"
        ],
        "comment": "",
        "title": "[1503.00759] A Review of Relational Machine Learning for Knowledge Graphs",
        "relatedDoc": [],
        "creationTime": "2017-10-24T14:44:20Z",
        "creationDate": "2017-10-24",
        "bookmarkOf": [],
        "arxiv_author": [
            "Evgeniy Gabrilovich",
            "Kevin Murphy",
            "Volker Tresp",
            "Maximilian Nickel"
        ],
        "arxiv_summary": "Relational machine learning studies methods for the statistical analysis of\nrelational, or graph-structured, data. In this paper, we provide a review of\nhow such statistical models can be \"trained\" on large knowledge graphs, and\nthen used to predict new facts about the world (which is equivalent to\npredicting new edges in the graph). In particular, we discuss two fundamentally\ndifferent kinds of statistical relational models, both of which can scale to\nmassive datasets. The first is based on latent feature models such as tensor\nfactorization and multiway neural networks. The second is based on mining\nobservable patterns in the graph. We also show how to combine these latent and\nobservable models to get improved modeling power at decreased computational\ncost. Finally, we discuss how such statistical models of graphs can be combined\nwith text-based information extraction methods for automatically constructing\nknowledge graphs from the Web. To this end, we also discuss Google's Knowledge\nVault project as an example of such combination.",
        "arxiv_firstAuthor": "Maximilian Nickel",
        "arxiv_title": "A Review of Relational Machine Learning for Knowledge Graphs",
        "arxiv_num": "1503.00759",
        "arxiv_published": "2015-03-02T21:35:41Z",
        "arxiv_updated": "2015-09-28T17:40:35Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1805.03793",
        "tag": [
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1805.03793] hyperdoc2vec: Distributed Representations of Hypertext Documents",
        "relatedDoc": [],
        "creationTime": "2018-05-22T11:22:24Z",
        "creationDate": "2018-05-22",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jialong Han",
            "Yan Song",
            "Shuming Shi",
            "Haisong Zhang",
            "Wayne Xin Zhao"
        ],
        "arxiv_summary": "Hypertext documents, such as web pages and academic papers, are of great\nimportance in delivering information in our daily life. Although being\neffective on plain documents, conventional text embedding methods suffer from\ninformation loss if directly adapted to hyper-documents. In this paper, we\npropose a general embedding approach for hyper-documents, namely, hyperdoc2vec,\nalong with four criteria characterizing necessary information that\nhyper-document embedding models should preserve. Systematic comparisons are\nconducted between hyperdoc2vec and several competitors on two tasks, i.e.,\npaper classification and citation recommendation, in the academic paper domain.\nAnalyses and experiments both validate the superiority of hyperdoc2vec to other\nmodels w.r.t. the four criteria.",
        "arxiv_firstAuthor": "Jialong Han",
        "arxiv_title": "hyperdoc2vec: Distributed Representations of Hypertext Documents",
        "arxiv_num": "1805.03793",
        "arxiv_published": "2018-05-10T02:42:03Z",
        "arxiv_updated": "2018-05-10T02:42:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/09/_1909_04120_span_selection_pre",
        "tag": [
            "http://www.semanlink.net/tag/memory_in_deep_learning",
            "http://www.semanlink.net/tag/knowledge_augmented_language_models",
            "http://www.semanlink.net/tag/not_encoding_knowledge_in_language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/nlp_ibm",
            "http://www.semanlink.net/tag/question_answering",
            "http://www.semanlink.net/tag/language_models_knowledge"
        ],
        "comment": "> a **new pre-training task inspired by reading\r\ncomprehension** and an **effort to avoid encoding general knowledge in the transformer network itself**\r\n\r\nCurrent transformer architectures store general knowledge -> large models, long pre-training time. Better to offload the requirement of general knowledge to a sparsely activated network.\r\n\r\n\"Span selection\" as an additional auxiliary task: the query is a sentence drawn from a corpus\r\nwith a term replaced with a special token: [BLANK]. The term replaced by the blank is the answer term. The passage is\r\nrelevant as determined by a BM25 search, and answer-bearing (containing the answer\r\nterm). Unlike BERT\u2019s cloze task, where the answer must be drawn from the model itself, the answer is found in a passage\r\nusing language understanding.\r\n\r\n> **We hope to progress to a model of general purpose language modeling that uses an indexed long\r\nterm memory to retrieve world knowledge, rather than holding it in the densely activated transformer encoder layers.**",
        "title": "[1909.04120] Span Selection Pre-training for Question Answering",
        "relatedDoc": [],
        "creationTime": "2019-09-18T17:26:33Z",
        "creationDate": "2019-09-18",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.04120"
        ],
        "arxiv_author": [
            "Avirup Sil",
            "G P Shrivatsa Bhargav",
            "Alfio Gliozzo",
            "Dinesh Garg",
            "Rishav Chakravarti",
            "Michael Glass",
            "Anthony Ferritto",
            "Lin Pan"
        ],
        "arxiv_summary": "BERT (Bidirectional Encoder Representations from Transformers) and related\npre-trained Transformers have provided large gains across many language\nunderstanding tasks, achieving a new state-of-the-art (SOTA). BERT is\npre-trained on two auxiliary tasks: Masked Language Model and Next Sentence\nPrediction. In this paper we introduce a new pre-training task inspired by\nreading comprehension and an effort to avoid encoding general knowledge in the\ntransformer network itself. We find significant and consistent improvements\nover both BERT-BASE and BERT-LARGE on multiple reading comprehension (MRC) and\nparaphrasing datasets. Specifically, our proposed model has strong empirical\nevidence as it obtains SOTA results on Natural Questions, a new benchmark MRC\ndataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We\nalso establish a new SOTA in HotpotQA, improving answer prediction F1 by 4 F1\npoints and supporting fact prediction by 1 F1 point. Moreover, we show that our\npre-training approach is particularly effective when training data is limited,\nimproving the learning curve by a large amount.",
        "arxiv_firstAuthor": "Michael Glass",
        "arxiv_title": "Span Selection Pre-training for Question Answering",
        "arxiv_num": "1909.04120",
        "arxiv_published": "2019-09-09T19:32:31Z",
        "arxiv_updated": "2019-09-09T19:32:31Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1706.00957.pdf",
        "tag": [
            "http://www.semanlink.net/tag/vector_space_model",
            "http://www.semanlink.net/tag/semantic_search",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/elasticsearch"
        ],
        "comment": "> The ubiquity of semantic vector space modeling raises the challenge of efficient searching in dense, high-dimensional vector spaces. We would naturally want to take advantage of the design and optimizations behind modern fulltext engines like Elasticsearch so as to meet the scalability and robustness demands of modern IR applications. This is the research challenge addressed in this paper.\r\n> The paper describes novel ways of encoding dense vectors into text documents, allowing the use of traditional inverted index engines.\r\n\r\n[blog post](https://rare-technologies.com/semantic-search-fulltext-engine-acl-2017/)\r\n",
        "title": "[1706.00957] Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines",
        "relatedDoc": [],
        "creationTime": "2017-11-11T22:28:36Z",
        "creationDate": "2017-11-11",
        "bookmarkOf": [],
        "arxiv_author": [
            "Petr Sojka",
            "Radim \u0158eh\u016f\u0159ek",
            "Jan Rygl",
            "Jan Pomik\u00e1lek",
            "Michal R\u016f\u017ei\u010dka",
            "V\u00edt Novotn\u00fd"
        ],
        "arxiv_summary": "Vector representations and vector space modeling (VSM) play a central role in\nmodern machine learning. We propose a novel approach to `vector similarity\nsearching' over dense semantic representations of words and documents that can\nbe deployed on top of traditional inverted-index-based fulltext engines, taking\nadvantage of their robustness, stability, scalability and ubiquity.\nWe show that this approach allows the indexing and querying of dense vectors\nin text domains. This opens up exciting avenues for major efficiency gains,\nalong with simpler deployment, scaling and monitoring.\nThe end result is a fast and scalable vector database with a tunable\ntrade-off between vector search performance and quality, backed by a standard\nfulltext engine such as Elasticsearch.\nWe empirically demonstrate its querying performance and quality by applying\nthis solution to the task of semantic searching over a dense vector\nrepresentation of the entire English Wikipedia.",
        "arxiv_firstAuthor": "Jan Rygl",
        "arxiv_title": "Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines",
        "arxiv_num": "1706.00957",
        "arxiv_published": "2017-06-03T14:21:22Z",
        "arxiv_updated": "2017-06-03T14:21:22Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/0811.3701",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/hypothese_de_riemann",
            "http://www.semanlink.net/tag/jean_paul"
        ],
        "comment": "> In this paper we explore a family of congruences over N* from which a sequence of symmetric matrices related to the Mertens function is built. From the results of numerical experiments we formulate a conjecture, about the growth of the quadratic norm of these matrices, which implies the Riemann hypothesis. This suggests that matrix analysis methods may play a more important role in this classical and difficult problem.\r\n",
        "title": "[0811.3701] Symmetric matrices related to the Mertens function",
        "relatedDoc": [],
        "creationTime": "2009-01-20T21:56:47Z",
        "creationDate": "2009-01-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jean-Paul Cardinal"
        ],
        "arxiv_summary": "In this paper we explore a family of congruences over $\\N^\\ast$ from which\none builds a sequence of symmetric matrices related to the Mertens function.\nFrom the results of numerical experiments, we formulate a conjecture about\nthe growth of the quadratic norm of these matrices, which implies the Riemann\nhypothesis. This suggests that matrix analysis methods may come to play a more\nimportant role in this classical and difficult problem.",
        "arxiv_firstAuthor": "Jean-Paul Cardinal",
        "arxiv_title": "Symmetric matrices related to the Mertens function",
        "arxiv_num": "0811.3701",
        "arxiv_published": "2008-11-22T17:22:06Z",
        "arxiv_updated": "2009-03-09T11:28:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/07/2007_00077_similarity_search_",
        "tag": [
            "http://www.semanlink.net/tag/imbalanced_data",
            "http://www.semanlink.net/tag/facebook_fair",
            "http://www.semanlink.net/tag/ai_stanford",
            "http://www.semanlink.net/tag/nearest_neighbor_search",
            "http://www.semanlink.net/tag/active_learning",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> Similarity search for Efficient Active Learning and Search (SEALS)\r\n\r\nIn [Active Learning](tag:active_learning): instead of searching globally for the optimal examples to label, leverage the fact that data is often heavily skewed and expand the candidate pool with the nearest neighbors of the labeled set.\r\n\r\n> Our work attacks **both the labeling and computational costs of machine learning**...SEALS dramatically reduces the barrier to machine learning, enabling small teams or individuals to\r\nbuild accurate classifiers. **SEALS does, however, introduce another system component, a similarity\r\nsearch index, which adds some additional engineering complexity** to build, tune, and maintain.\r\nFortunately, several highly optimized implementations like Annoy and [Faiss](doc:2020/06/facebookresearch_faiss_a_libra) work reasonably well\r\nout of the box.",
        "title": "[2007.00077] Similarity Search for Efficient Active Learning and Search of Rare Concepts",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/06/facebookresearch_faiss_a_libra"
        ],
        "creationTime": "2020-07-02T15:31:34Z",
        "creationDate": "2020-07-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/2007.00077"
        ],
        "arxiv_author": [
            "Sean Culatana",
            "I. Zeki Yalniz",
            "Cody Coleman",
            "Roshan Sumbaly",
            "Matei Zaharia",
            "Peter Bailis",
            "Edward Chou",
            "Alexander C. Berg"
        ],
        "arxiv_summary": "Many active learning and search approaches are intractable for industrial\nsettings with billions of unlabeled examples. Existing approaches, such as\nuncertainty sampling or information density, search globally for the optimal\nexamples to label, scaling linearly or even quadratically with the unlabeled\ndata. However, in practice, data is often heavily skewed; only a small fraction\nof collected data will be relevant for a given learning task. For example, when\nidentifying rare classes, detecting malicious content, or debugging model\nperformance, the ratio of positive to negative examples can be 1 to 1,000 or\nmore. In this work, we exploit this skew in large training datasets to reduce\nthe number of unlabeled examples considered in each selection round by only\nlooking at the nearest neighbors to the labeled examples. Empirically, we\nobserve that learned representations effectively cluster unseen concepts,\nmaking active learning very effective and substantially reducing the number of\nviable unlabeled examples. We evaluate several active learning and search\ntechniques in this setting on three large-scale datasets: ImageNet, Goodreads\nspoiler detection, and OpenImages. For rare classes, active learning methods\nneed as little as 0.31% of the labeled data to match the average precision of\nfull supervision. By limiting active learning methods to only consider the\nimmediate neighbors of the labeled data as candidates for labeling, we need\nonly process as little as 1% of the unlabeled data while achieving similar\nreductions in labeling costs as the traditional global approach. This process\nof expanding the candidate pool with the nearest neighbors of the labeled set\ncan be done efficiently and reduces the computational complexity of selection\nby orders of magnitude.",
        "arxiv_firstAuthor": "Cody Coleman",
        "arxiv_title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts",
        "arxiv_num": "2007.00077",
        "arxiv_published": "2020-06-30T19:46:10Z",
        "arxiv_updated": "2020-06-30T19:46:10Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1804.04526",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/rdf"
        ],
        "comment": "690 thousand contemporary and historical events and over 2.3 million temporal relations",
        "title": "[1804.04526] EventKG: A Multilingual Event-Centric Temporal Knowledge Graph",
        "relatedDoc": [],
        "creationTime": "2018-04-15T08:43:10Z",
        "creationDate": "2018-04-15",
        "bookmarkOf": [],
        "arxiv_author": [
            "Elena Demidova",
            "Simon Gottschalk"
        ],
        "arxiv_summary": "One of the key requirements to facilitate semantic analytics of information\nregarding contemporary and historical events on the Web, in the news and in\nsocial media is the availability of reference knowledge repositories containing\ncomprehensive representations of events and temporal relations. Existing\nknowledge graphs, with popular examples including DBpedia, YAGO and Wikidata,\nfocus mostly on entity-centric information and are insufficient in terms of\ntheir coverage and completeness with respect to events and temporal relations.\nEventKG presented in this paper is a multilingual event-centric temporal\nknowledge graph that addresses this gap. EventKG incorporates over 690 thousand\ncontemporary and historical events and over 2.3 million temporal relations\nextracted from several large-scale knowledge graphs and semi-structured sources\nand makes them available through a canonical representation.",
        "arxiv_firstAuthor": "Simon Gottschalk",
        "arxiv_title": "EventKG: A Multilingual Event-Centric Temporal Knowledge Graph",
        "arxiv_num": "1804.04526",
        "arxiv_published": "2018-04-12T14:12:48Z",
        "arxiv_updated": "2018-04-12T14:12:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1706_00384_deep_mutual_learni",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/mutual_learning",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/kd_mkb_biblio"
        ],
        "comment": "> In this paper we explore a different but related idea to model distillation \u2013 that of mutual learning. Distillation starts with a powerful large and pre-trained teacher network and performs one-way knowledge transfer to a small untrained student. In contrast, in mutual learning we start with a pool of untrained students who learn simultaneously to solve the task together.\r\n\r\n[critic here](doc:2020/06/1804_03235_large_scale_distri):\r\n\r\n> Zhang et al. (2017) reported a benefit in quality over\r\nbasic distillation, but they compare distilling model M1 into model M2 with training model M1\r\nand model M2 using codistillation; they do not compare to distilling an ensemble of models M1\r\nand M2 into model M3.\r\n>\r\n> ...\r\n>\r\n> we can achieve the 70.7% they report for online\r\ndistillation using traditional offline distillation.",
        "title": "[1706.00384] Deep Mutual Learning",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/06/1804_03235_large_scale_distri"
        ],
        "creationTime": "2020-05-11T21:21:42Z",
        "creationDate": "2020-05-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1706.00384"
        ],
        "arxiv_author": [
            "Ying Zhang",
            "Tao Xiang",
            "Timothy M. Hospedales",
            "Huchuan Lu"
        ],
        "arxiv_summary": "Model distillation is an effective and widely used technique to transfer\nknowledge from a teacher to a student network. The typical application is to\ntransfer from a powerful large network or ensemble to a small network, that is\nbetter suited to low-memory or fast execution requirements. In this paper, we\npresent a deep mutual learning (DML) strategy where, rather than one way\ntransfer between a static pre-defined teacher and a student, an ensemble of\nstudents learn collaboratively and teach each other throughout the training\nprocess. Our experiments show that a variety of network architectures benefit\nfrom mutual learning and achieve compelling results on CIFAR-100 recognition\nand Market-1501 person re-identification benchmarks. Surprisingly, it is\nrevealed that no prior powerful teacher network is necessary -- mutual learning\nof a collection of simple student networks works, and moreover outperforms\ndistillation from a more powerful yet static teacher.",
        "arxiv_firstAuthor": "Ying Zhang",
        "arxiv_title": "Deep Mutual Learning",
        "arxiv_num": "1706.00384",
        "arxiv_published": "2017-06-01T16:57:15Z",
        "arxiv_updated": "2017-06-01T16:57:15Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "http://www.semanlink.net/doc/2020/05/1511_03643_unifying_distillat",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/2004_14843_knowledge_graph_em",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/explainable_ai"
        ],
        "comment": "survey of \r\n\r\n- the state-of-the-art in the field of knowledge graph embeddings\r\n- methods for explaining predictions obtained via knowledge graph embeddings.",
        "title": "[2004.14843] Knowledge Graph Embeddings and Explainable AI",
        "relatedDoc": [],
        "creationTime": "2020-05-04T13:29:14Z",
        "creationDate": "2020-05-04",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.14843"
        ],
        "arxiv_author": [
            "Federico Bianchi",
            "Luca Costabello",
            "Pasquale Minervini",
            "Gaetano Rossiello",
            "Matteo Palmonari"
        ],
        "arxiv_summary": "Knowledge graph embeddings are now a widely adopted approach to knowledge\nrepresentation in which entities and relationships are embedded in vector\nspaces. In this chapter, we introduce the reader to the concept of knowledge\ngraph embeddings by explaining what they are, how they can be generated and how\nthey can be evaluated. We summarize the state-of-the-art in this field by\ndescribing the approaches that have been introduced to represent knowledge in\nthe vector space. In relation to knowledge representation, we consider the\nproblem of explainability, and discuss models and methods for explaining\npredictions obtained via knowledge graph embeddings.",
        "arxiv_firstAuthor": "Federico Bianchi",
        "arxiv_title": "Knowledge Graph Embeddings and Explainable AI",
        "arxiv_num": "2004.14843",
        "arxiv_published": "2020-04-30T14:55:09Z",
        "arxiv_updated": "2020-04-30T14:55:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/1410_5859_towards_a_model_the",
        "tag": [
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation",
            "http://www.semanlink.net/tag/guha",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/first_order_logic",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings"
        ],
        "comment": "> **We would like to have systems that are largely learnt, which\r\nwe can also teach**\r\n\r\n> We believe that an essential step in bringing logic and\r\ndistributed representations closer is to create a model theory based on embeddings.\r\n\r\n> despite our best attempts,\r\nterms and axioms in knowledge based systems end\r\nup having many of the characteristics of natural language -- LOL",
        "title": "[1410.5859] Towards a Model Theory for Distributed Representations",
        "relatedDoc": [],
        "creationTime": "2021-06-10T16:30:07Z",
        "creationDate": "2021-06-10",
        "bookmarkOf": [
            "https://arxiv.org/abs/1410.5859"
        ],
        "arxiv_author": [
            "Ramanathan Guha"
        ],
        "arxiv_summary": "Distributed representations (such as those based on embeddings) and discrete\nrepresentations (such as those based on logic) have complementary strengths. We\nexplore one possible approach to combining these two kinds of representations.\nWe present a model theory/semantics for first order logic based on vectors of\nreals. We describe the model theory, discuss some interesting properties of\nsuch a system and present a simple approach to query answering.",
        "arxiv_firstAuthor": "Ramanathan Guha",
        "arxiv_title": "Towards a Model Theory for Distributed Representations",
        "arxiv_num": "1410.5859",
        "arxiv_published": "2014-10-21T21:15:45Z",
        "arxiv_updated": "2015-02-05T03:06:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/07/_1602_01137_a_dual_embedding_s",
        "tag": [
            "http://www.semanlink.net/tag/embeddings_in_ir",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ranking_information_retrieval",
            "http://www.semanlink.net/tag/bhaskar_mitra"
        ],
        "comment": "Investigate neural word embeddings as a source of evidence in document ranking.\r\n\r\nPresented in [this Stanford course on IR](/doc/?uri=https%3A%2F%2Fweb.stanford.edu%2Fclass%2Fcs276%2Fhandouts%2Flecture20-distributed-representations.pdf) by Chris Manning (starting slide 44)\r\n\r\nThey train a word2vec model, but retain both the input and the output projections.\r\n\r\n> During ranking we map the query words into the input space and the document words into the output space, and compute a query-document relevance score by aggregating the cosine similarities across all the query-document word pairs. \r\n\r\n> However, when ranking a larger set of candidate documents, we find the embeddings-based approach is prone to false positives",
        "title": "[1602.01137] A Dual Embedding Space Model for Document Ranking",
        "relatedDoc": [
            "https://web.stanford.edu/class/cs276/handouts/lecture20-distributed-representations.pdf"
        ],
        "creationTime": "2019-07-17T12:15:50Z",
        "creationDate": "2019-07-17",
        "bookmarkOf": [
            "https://arxiv.org/abs/1602.01137"
        ],
        "arxiv_author": [
            "Nick Craswell",
            "Bhaskar Mitra",
            "Rich Caruana",
            "Eric Nalisnick"
        ],
        "arxiv_summary": "A fundamental goal of search engines is to identify, given a query, documents\nthat have relevant text. This is intrinsically difficult because the query and\nthe document may use different vocabulary, or the document may contain query\nwords without being relevant. We investigate neural word embeddings as a source\nof evidence in document ranking. We train a word2vec embedding model on a large\nunlabelled query corpus, but in contrast to how the model is commonly used, we\nretain both the input and the output projections, allowing us to leverage both\nthe embedding spaces to derive richer distributional relationships. During\nranking we map the query words into the input space and the document words into\nthe output space, and compute a query-document relevance score by aggregating\nthe cosine similarities across all the query-document word pairs.\nWe postulate that the proposed Dual Embedding Space Model (DESM) captures\nevidence on whether a document is about a query term in addition to what is\nmodelled by traditional term-frequency based approaches. Our experiments show\nthat the DESM can re-rank top documents returned by a commercial Web search\nengine, like Bing, better than a term-matching based signal like TF-IDF.\nHowever, when ranking a larger set of candidate documents, we find the\nembeddings-based approach is prone to false positives, retrieving documents\nthat are only loosely related to the query. We demonstrate that this problem\ncan be solved effectively by ranking based on a linear mixture of the DESM and\nthe word counting features.",
        "arxiv_firstAuthor": "Bhaskar Mitra",
        "arxiv_title": "A Dual Embedding Space Model for Document Ranking",
        "arxiv_num": "1602.01137",
        "arxiv_published": "2016-02-02T22:23:18Z",
        "arxiv_updated": "2016-02-02T22:23:18Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1602.06797",
        "tag": [
            "http://www.semanlink.net/tag/short_text_clustering",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/semi_supervised_learning"
        ],
        "comment": ">semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the k-means clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: \r\n>\r\n>1. assign each short text to its nearest centroid based on its representation from the current neural networks;\r\n>2. re-estimate the cluster centroids based on cluster assignments from step (1);\r\n>3. update neural networks according to the objective by keeping centroids and cluster assignments fixed.\r\n",
        "title": "[1602.06797] Semi-supervised Clustering for Short Text via Deep Representation Learning",
        "relatedDoc": [],
        "creationTime": "2017-11-07T23:07:48Z",
        "creationDate": "2017-11-07",
        "bookmarkOf": [],
        "arxiv_author": [
            "Zhiguo Wang",
            "Haitao Mi",
            "Abraham Ittycheriah"
        ],
        "arxiv_summary": "In this work, we propose a semi-supervised method for short text clustering,\nwhere we represent texts as distributed vectors with neural networks, and use a\nsmall amount of labeled data to specify our intention for clustering. We design\na novel objective to combine the representation learning process and the\nk-means clustering process together, and optimize the objective with both\nlabeled data and unlabeled data iteratively until convergence through three\nsteps: (1) assign each short text to its nearest centroid based on its\nrepresentation from the current neural networks; (2) re-estimate the cluster\ncentroids based on cluster assignments from step (1); (3) update neural\nnetworks according to the objective by keeping centroids and cluster\nassignments fixed. Experimental results on four datasets show that our method\nworks significantly better than several other text clustering methods.",
        "arxiv_firstAuthor": "Zhiguo Wang",
        "arxiv_title": "Semi-supervised Clustering for Short Text via Deep Representation Learning",
        "arxiv_num": "1602.06797",
        "arxiv_published": "2016-02-22T14:55:26Z",
        "arxiv_updated": "2017-07-14T19:52:33Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1507.07998.pdf",
        "tag": [
            "http://www.semanlink.net/tag/doc2vec",
            "http://www.semanlink.net/tag/christopher_olah",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1507.07998] Document Embedding with Paragraph Vectors",
        "relatedDoc": [],
        "creationTime": "2017-08-20T23:29:27Z",
        "creationDate": "2017-08-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Christopher Olah",
            "Quoc V. Le",
            "Andrew M. Dai"
        ],
        "arxiv_summary": "Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.",
        "arxiv_firstAuthor": "Andrew M. Dai",
        "arxiv_title": "Document Embedding with Paragraph Vectors",
        "arxiv_num": "1507.07998",
        "arxiv_published": "2015-07-29T01:04:28Z",
        "arxiv_updated": "2015-07-29T01:04:28Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/2001_11631_enhancement_of_sho",
        "tag": [
            "http://www.semanlink.net/tag/short_text_clustering",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> Given a clustering of short texts obtained using an arbitrary clustering algorithm, iterative classification applies outlier removal to obtain outlier-free clusters. Then it trains a classification algorithm using the non-outliers based on their cluster distributions. Using the trained classification model, iterative classification reclassifies the outliers to obtain a new set of clusters.",
        "title": "[2001.11631] Enhancement of Short Text Clustering by Iterative Classification",
        "relatedDoc": [],
        "creationTime": "2021-05-20T17:59:46Z",
        "creationDate": "2021-05-20",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.11631"
        ],
        "arxiv_author": [
            "Md Rashadul Hasan Rakib",
            "Magdalena Jankowska",
            "Norbert Zeh",
            "Evangelos Milios"
        ],
        "arxiv_summary": "Short text clustering is a challenging task due to the lack of signal\ncontained in such short texts. In this work, we propose iterative\nclassification as a method to b o ost the clustering quality (e.g., accuracy)\nof short texts. Given a clustering of short texts obtained using an arbitrary\nclustering algorithm, iterative classification applies outlier removal to\nobtain outlier-free clusters. Then it trains a classification algorithm using\nthe non-outliers based on their cluster distributions. Using the trained\nclassification model, iterative classification reclassifies the outliers to\nobtain a new set of clusters. By repeating this several times, we obtain a much\nimproved clustering of texts. Our experimental results show that the proposed\nclustering enhancement method not only improves the clustering quality of\ndifferent clustering methods (e.g., k-means, k-means--, and hierarchical\nclustering) but also outperforms the state-of-the-art short text clustering\nmethods on several short text datasets by a statistically significant margin.",
        "arxiv_firstAuthor": "Md Rashadul Hasan Rakib",
        "arxiv_title": "Enhancement of Short Text Clustering by Iterative Classification",
        "arxiv_num": "2001.11631",
        "arxiv_published": "2020-01-31T02:12:05Z",
        "arxiv_updated": "2020-01-31T02:12:05Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1601.01272",
        "tag": [
            "http://www.semanlink.net/tag/lstm_networks",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/rnn_based_language_model",
            "http://www.semanlink.net/tag/recurrent_neural_network"
        ],
        "comment": "> Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge.\r\n>\r\n> In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data.\r\n>\r\n> We demonstrate the power of RMN on language modeling and sentence completion tasks.\r\n>\r\n> On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin.\r\n",
        "title": "[1601.01272] Recurrent Memory Networks for Language Modeling",
        "relatedDoc": [],
        "creationTime": "2016-01-09T00:35:09Z",
        "creationDate": "2016-01-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Christof Monz",
            "Arianna Bisazza",
            "Ke Tran"
        ],
        "arxiv_summary": "Recurrent Neural Networks (RNN) have obtained excellent result in many\nnatural language processing (NLP) tasks. However, understanding and\ninterpreting the source of this success remains a challenge. In this paper, we\npropose Recurrent Memory Network (RMN), a novel RNN architecture, that not only\namplifies the power of RNN but also facilitates our understanding of its\ninternal functioning and allows us to discover underlying patterns in data. We\ndemonstrate the power of RMN on language modeling and sentence completion\ntasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)\nnetwork on three large German, Italian, and English dataset. Additionally we\nperform in-depth analysis of various linguistic dimensions that RMN captures.\nOn Sentence Completion Challenge, for which it is essential to capture sentence\ncoherence, our RMN obtains 69.2% accuracy, surpassing the previous\nstate-of-the-art by a large margin.",
        "arxiv_firstAuthor": "Ke Tran",
        "arxiv_title": "Recurrent Memory Networks for Language Modeling",
        "arxiv_num": "1601.01272",
        "arxiv_published": "2016-01-06T18:44:07Z",
        "arxiv_updated": "2016-04-22T11:13:11Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_2002_02925_bert_of_theseus_c",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/knowledge_distillation"
        ],
        "comment": "approach to compress BERT by progressive module replacing.\r\n\r\n> Compared to the previous knowledge distillation approaches for BERT compression, our approach leverages only one loss function and one hyper-parameter\r\n\r\n[Github](https://github.com/JetRunner/BERT-of-Theseus)",
        "title": "[2002.02925] BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
        "relatedDoc": [],
        "creationTime": "2020-02-10T21:50:03Z",
        "creationDate": "2020-02-10",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.02925"
        ],
        "arxiv_author": [
            "Tao Ge",
            "Ming Zhou",
            "Wangchunshu Zhou",
            "Canwen Xu",
            "Furu Wei"
        ],
        "arxiv_summary": "In this paper, we propose a novel model compression approach to effectively\ncompress BERT by progressive module replacing. Our approach first divides the\noriginal BERT into several modules and builds their compact substitutes. Then,\nwe randomly replace the original modules with their substitutes to train the\ncompact modules to mimic the behavior of the original modules. We progressively\nincrease the probability of replacement through the training. In this way, our\napproach brings a deeper level of interaction between the original and compact\nmodels, and smooths the training process. Compared to the previous knowledge\ndistillation approaches for BERT compression, our approach leverages only one\nloss function and one hyper-parameter, liberating human effort from\nhyper-parameter tuning. Our approach outperforms existing knowledge\ndistillation approaches on GLUE benchmark, showing a new perspective of model\ncompression.",
        "arxiv_firstAuthor": "Canwen Xu",
        "arxiv_title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
        "arxiv_num": "2002.02925",
        "arxiv_published": "2020-02-07T17:52:16Z",
        "arxiv_updated": "2020-03-25T15:20:44Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1806.01261",
        "tag": [
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation",
            "http://www.semanlink.net/tag/combinatorial_generalization",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/frequently_cited_paper",
            "http://www.semanlink.net/tag/google_deepmind",
            "http://www.semanlink.net/tag/graph_neural_networks",
            "http://www.semanlink.net/tag/relational_inductive_biases",
            "http://www.semanlink.net/tag/todo_read",
            "http://www.semanlink.net/tag/artificial_general_intelligence",
            "http://www.semanlink.net/tag/ml_google"
        ],
        "comment": "> generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI\r\n\r\n> A key signature of human intelligence is the ability to make infine use of finite means\" (Humboldt,\r\n1836; Chomsky, 1965) (ex: words / sentences\r\n\r\n> Here we explore how to improve modern AI's capacity for **combinatorial generalization** by\r\nbiasing learning towards structured representations and computations, and in particular, systems\r\nthat operate on graphs.\r\n\r\n(papier recommand\u00e9 par [Peter Bloem](tag:peter_bloem))",
        "title": "[1806.01261] Relational inductive biases, deep learning, and graph networks",
        "relatedDoc": [],
        "creationTime": "2018-06-13T13:34:03Z",
        "creationDate": "2018-06-13",
        "bookmarkOf": [],
        "arxiv_author": [
            "Matt Botvinick",
            "David Raposo",
            "Pushmeet Kohli",
            "Justin Gilmer",
            "Nicolas Heess",
            "Oriol Vinyals",
            "Razvan Pascanu",
            "Jessica B. Hamrick",
            "Andrea Tacchetti",
            "Charles Nash",
            "Andrew Ballard",
            "Daan Wierstra",
            "Kelsey Allen",
            "Yujia Li",
            "Francis Song",
            "Victoria Langston",
            "Ryan Faulkner",
            "Caglar Gulcehre",
            "Peter W. Battaglia",
            "Vinicius Zambaldi",
            "George Dahl",
            "Ashish Vaswani",
            "Mateusz Malinowski",
            "Alvaro Sanchez-Gonzalez",
            "Adam Santoro",
            "Chris Dyer",
            "Victor Bapst"
        ],
        "arxiv_summary": "Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one's experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\nThe following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning. As a\ncompanion to this paper, we have released an open-source software library for\nbuilding graph networks, with demonstrations of how to use them in practice.",
        "arxiv_firstAuthor": "Peter W. Battaglia",
        "arxiv_title": "Relational inductive biases, deep learning, and graph networks",
        "arxiv_num": "1806.01261",
        "arxiv_published": "2018-06-04T17:58:18Z",
        "arxiv_updated": "2018-10-17T17:51:36Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_1802_01528_the_matrix_calculu",
        "tag": [
            "http://www.semanlink.net/tag/jeremy_howard",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/matrix_calculus"
        ],
        "comment": "Related blog post [The Math Behind Neural Networks](https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9)",
        "title": "[1802.01528] The Matrix Calculus You Need For Deep Learning",
        "relatedDoc": [],
        "creationTime": "2020-02-19T21:52:12Z",
        "creationDate": "2020-02-19",
        "bookmarkOf": [
            "https://arxiv.org/abs/1802.01528"
        ],
        "arxiv_author": [
            "Terence Parr",
            "Jeremy Howard"
        ],
        "arxiv_summary": "This paper is an attempt to explain all the matrix calculus you need in order\nto understand the training of deep neural networks. We assume no math knowledge\nbeyond what you learned in calculus 1, and provide links to help you refresh\nthe necessary math where needed. Note that you do not need to understand this\nmaterial before you start learning to train and use deep learning in practice;\nrather, this material is for those who are already familiar with the basics of\nneural networks, and wish to deepen their understanding of the underlying math.\nDon't worry if you get stuck at some point along the way---just go back and\nreread the previous section, and try writing down and working through some\nexamples. And if you're still stuck, we're happy to answer your questions in\nthe Theory category at forums.fast.ai. Note: There is a reference section at\nthe end of the paper summarizing all the key matrix calculus rules and\nterminology discussed here. See related articles at http://explained.ai",
        "arxiv_firstAuthor": "Terence Parr",
        "arxiv_title": "The Matrix Calculus You Need For Deep Learning",
        "arxiv_num": "1802.01528",
        "arxiv_published": "2018-02-05T17:37:59Z",
        "arxiv_updated": "2018-07-02T17:36:34Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/08/2003_11644_multi_label_text_c",
        "tag": [
            "http://www.semanlink.net/tag/graph_attention_networks",
            "http://www.semanlink.net/tag/text_multi_label_classification",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/classification_relations_between_classes"
        ],
        "comment": "> **Existing methods tend to ignore the relationship among labels**. \r\n\r\nThis model employs [Graph Attention Networks](tag:graph_attention_networks) (GAT) to find the correlation between\r\nlabels. The generated classifiers are applied to sentence feature vectors obtained from the text feature extraction network (BiLSTM) to enable end-to-end training.\r\n\r\n\r\n> GAT network takes the node features and adjacency\r\nmatrix that represents the graph data as inputs.\r\nThe adjacency matrix is constructed based on\r\nthe samples. **In our case, we do not have a graph\r\ndataset. Instead, we learn the adjacency matrix**, hoping\r\nthat the model will determine the graph, thereby\r\nlearning the correlation of the labels.\r\n> Our intuition is that by modeling the correlation\r\namong labels as a weighted graph, we force the GAT\r\nnetwork to learn such that the adjacency matrix and\r\nthe attention weights together represent the correlation.\r\n\r\n// TODO compare with [this](doc:2019/06/_1905_10070_label_aware_docume)",
        "title": "[2003.11644] MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/06/_1905_10070_label_aware_docume"
        ],
        "creationTime": "2020-08-14T16:11:43Z",
        "creationDate": "2020-08-14",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.11644"
        ],
        "arxiv_author": [
            "Malaikannan Sankarasubbu",
            "Ankit Pal",
            "Muru Selvakumar"
        ],
        "arxiv_summary": "In Multi-Label Text Classification (MLTC), one sample can belong to more than\none class. It is observed that most MLTC tasks, there are dependencies or\ncorrelations among labels. Existing methods tend to ignore the relationship\namong labels. In this paper, a graph attention network-based model is proposed\nto capture the attentive dependency structure among the labels. The graph\nattention network uses a feature matrix and a correlation matrix to capture and\nexplore the crucial dependencies between the labels and generate classifiers\nfor the task. The generated classifiers are applied to sentence feature vectors\nobtained from the text feature extraction network (BiLSTM) to enable end-to-end\ntraining. Attention allows the system to assign different weights to neighbor\nnodes per label, thus allowing it to learn the dependencies among labels\nimplicitly. The results of the proposed model are validated on five real-world\nMLTC datasets. The proposed model achieves similar or better performance\ncompared to the previous state-of-the-art models.",
        "arxiv_firstAuthor": "Ankit Pal",
        "arxiv_title": "Multi-Label Text Classification using Attention-based Graph Neural Network",
        "arxiv_num": "2003.11644",
        "arxiv_published": "2020-03-22T17:12:43Z",
        "arxiv_updated": "2020-03-22T17:12:43Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/07/2002_10640_differentiable_rea",
        "tag": [
            "http://www.semanlink.net/tag/virtual_knowledge_graph",
            "http://www.semanlink.net/tag/differentiable_reasoning_over_text",
            "http://www.semanlink.net/tag/knowledge_base",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov",
            "http://www.semanlink.net/tag/multi_hop_reasonning",
            "http://www.semanlink.net/tag/end_to_end_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_memory"
        ],
        "comment": "> We consider the task of answering complex multi-hop questions **using a corpus as a virtual knowledge base** (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a **special index of contextual representations of the mentions**. This module is **differentiable**, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing knowledge bases.\r\n\r\n[(Bhuwan Dhingra PhD Thesis)](doc:2020/07/end_to_end_learning_with_text_)",
        "title": "[2002.10640] Differentiable Reasoning over a Virtual Knowledge Base",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/07/end_to_end_learning_with_text_"
        ],
        "creationTime": "2020-07-11T14:03:19Z",
        "creationDate": "2020-07-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.10640"
        ],
        "arxiv_author": [
            "Vidhisha Balachandran",
            "Ruslan Salakhutdinov",
            "Graham Neubig",
            "Bhuwan Dhingra",
            "Manzil Zaheer",
            "William W. Cohen"
        ],
        "arxiv_summary": "We consider the task of answering complex multi-hop questions using a corpus\nas a virtual knowledge base (KB). In particular, we describe a neural module,\nDrKIT, that traverses textual data like a KB, softly following paths of\nrelations between mentions of entities in the corpus. At each step the module\nuses a combination of sparse-matrix TFIDF indices and a maximum inner product\nsearch (MIPS) on a special index of contextual representations of the mentions.\nThis module is differentiable, so the full system can be trained end-to-end\nusing gradient based methods, starting from natural language inputs. We also\ndescribe a pretraining scheme for the contextual representation encoder by\ngenerating hard negative examples using existing knowledge bases. We show that\nDrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset,\ncutting the gap between text-based and KB-based state-of-the-art by 70%. On\nHotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking\napproach to retrieving the relevant passages required to answer a question.\nDrKIT is also very efficient, processing 10-100x more queries per second than\nexisting multi-hop systems.",
        "arxiv_firstAuthor": "Bhuwan Dhingra",
        "arxiv_title": "Differentiable Reasoning over a Virtual Knowledge Base",
        "arxiv_num": "2002.10640",
        "arxiv_published": "2020-02-25T03:13:32Z",
        "arxiv_updated": "2020-02-25T03:13:32Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1907_04829_bam_born_again_mu",
        "tag": [
            "http://www.semanlink.net/tag/quoc_le",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/kd_mkb_biblio",
            "http://www.semanlink.net/tag/multitask_learning_in_nlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/chris_manning"
        ],
        "comment": "> **knowledge distillation where single-task models teach a multi-task model.** We enhance this training with **teacher annealing**, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers",
        "title": "[1907.04829] BAM! Born-Again Multi-Task Networks for Natural Language Understanding",
        "relatedDoc": [],
        "creationTime": "2020-05-12T19:08:45Z",
        "creationDate": "2020-05-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/1907.04829"
        ],
        "arxiv_author": [
            "Urvashi Khandelwal",
            "Minh-Thang Luong",
            "Kevin Clark",
            "Christopher D. Manning",
            "Quoc V. Le"
        ],
        "arxiv_summary": "It can be challenging to train multi-task neural networks that outperform or\neven match their single-task counterparts. To help address this, we propose\nusing knowledge distillation where single-task models teach a multi-task model.\nWe enhance this training with teacher annealing, a novel method that gradually\ntransitions the model from distillation to supervised learning, helping the\nmulti-task model surpass its single-task teachers. We evaluate our approach by\nmulti-task fine-tuning BERT on the GLUE benchmark. Our method consistently\nimproves over standard single-task and multi-task training.",
        "arxiv_firstAuthor": "Kevin Clark",
        "arxiv_title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding",
        "arxiv_num": "1907.04829",
        "arxiv_published": "2019-07-10T17:14:47Z",
        "arxiv_updated": "2019-07-10T17:14:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/04/2011_05864_on_the_sentence_em",
        "tag": [
            "http://www.semanlink.net/tag/bert_and_sentence_embeddings",
            "http://www.semanlink.net/tag/anisotropy_in_lm_space",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/emnlp_2020"
        ],
        "comment": "> **the sentence\r\nembeddings from the pre-trained language\r\nmodels without fine-tuning have been\r\nfound to poorly capture semantic meaning of\r\nsentences.**\r\n>\r\n> We find that **BERT always induces\r\na non-smooth anisotropic semantic space of\r\nsentences**, which harms its performance of\r\nsemantic similarity. To address this issue,\r\nwe propose to transform the anisotropic sentence\r\nembedding distribution to a smooth and\r\nisotropic Gaussian distribution through normalizing\r\nflows that are learned with an unsupervised\r\nobjective\r\n\r\n> normalizing flows (Dinh et al., 2015): invertible function parameterized by neural networks.\r\n> **During\r\ntraining, only the flow network is optimized\r\nwhile the BERT parameters remain unchanged**\r\n\r\n> When combined with external supervision from\r\nnatural language inference tasks (Bowman et al.,\r\n2015; Williams et al., 2018), our method outperforms\r\nthe [Sentence-BERT](tag:sbert) embeddings\r\n\r\n[GitHub](https://github.com/bohanli/BERT-flow)\r\n",
        "title": "[2011.05864] On the Sentence Embeddings from Pre-trained Language Models",
        "relatedDoc": [],
        "creationTime": "2021-04-19T01:13:25Z",
        "creationDate": "2021-04-19",
        "bookmarkOf": [
            "https://arxiv.org/abs/2011.05864"
        ],
        "arxiv_author": [
            "Hao Zhou",
            "Mingxuan Wang",
            "Yiming Yang",
            "Junxian He",
            "Lei Li",
            "Bohan Li"
        ],
        "arxiv_summary": "Pre-trained contextual representations like BERT have achieved great success\nin natural language processing. However, the sentence embeddings from the\npre-trained language models without fine-tuning have been found to poorly\ncapture semantic meaning of sentences. In this paper, we argue that the\nsemantic information in the BERT embeddings is not fully exploited. We first\nreveal the theoretical connection between the masked language model\npre-training objective and the semantic similarity task theoretically, and then\nanalyze the BERT sentence embeddings empirically. We find that BERT always\ninduces a non-smooth anisotropic semantic space of sentences, which harms its\nperformance of semantic similarity. To address this issue, we propose to\ntransform the anisotropic sentence embedding distribution to a smooth and\nisotropic Gaussian distribution through normalizing flows that are learned with\nan unsupervised objective. Experimental results show that our proposed\nBERT-flow method obtains significant performance gains over the\nstate-of-the-art sentence embeddings on a variety of semantic textual\nsimilarity tasks. The code is available at\nhttps://github.com/bohanli/BERT-flow.",
        "arxiv_firstAuthor": "Bohan Li",
        "arxiv_title": "On the Sentence Embeddings from Pre-trained Language Models",
        "arxiv_num": "2011.05864",
        "arxiv_published": "2020-11-02T13:14:57Z",
        "arxiv_updated": "2020-11-02T13:14:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/05/_1905_05950_bert_rediscovers_t",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bertology"
        ],
        "comment": "> We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\r\n",
        "title": "[1905.05950] BERT Rediscovers the Classical NLP Pipeline",
        "relatedDoc": [],
        "creationTime": "2019-05-18T17:50:08Z",
        "creationDate": "2019-05-18",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.05950"
        ],
        "arxiv_author": [
            "Ian Tenney",
            "Dipanjan Das",
            "Ellie Pavlick"
        ],
        "arxiv_summary": "Pre-trained text encoders have rapidly advanced the state of the art on many\nNLP tasks. We focus on one such model, BERT, and aim to quantify where\nlinguistic information is captured within the network. We find that the model\nrepresents the steps of the traditional NLP pipeline in an interpretable and\nlocalizable way, and that the regions responsible for each step appear in the\nexpected sequence: POS tagging, parsing, NER, semantic roles, then coreference.\nQualitative analysis reveals that the model can and often does adjust this\npipeline dynamically, revising lower-level decisions on the basis of\ndisambiguating information from higher-level representations.",
        "arxiv_firstAuthor": "Ian Tenney",
        "arxiv_title": "BERT Rediscovers the Classical NLP Pipeline",
        "arxiv_num": "1905.05950",
        "arxiv_published": "2019-05-15T05:47:23Z",
        "arxiv_updated": "2019-08-09T15:51:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/1911_02116_unsupervised_cross",
        "tag": [
            "http://www.semanlink.net/tag/cross_lingual_nlp",
            "http://www.semanlink.net/tag/multilingual_embeddings",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Data: [CC-100: Monolingual Datasets from Web Crawl Data](doc:2021/07/cc_100_monolingual_datasets_fr)",
        "title": "[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/07/cc_100_monolingual_datasets_fr"
        ],
        "creationTime": "2021-07-29T00:16:13Z",
        "creationDate": "2021-07-29",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.02116"
        ],
        "arxiv_author": [
            "Alexis Conneau",
            "Kartikay Khandelwal",
            "Veselin Stoyanov",
            "Myle Ott",
            "Vishrav Chaudhary",
            "Guillaume Wenzek",
            "Luke Zettlemoyer",
            "Francisco Guzm\u00e1n",
            "Naman Goyal",
            "Edouard Grave"
        ],
        "arxiv_summary": "This paper shows that pretraining multilingual language models at scale leads\nto significant performance gains for a wide range of cross-lingual transfer\ntasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\ndetailed empirical analysis of the key factors that are required to achieve\nthese gains, including the trade-offs between (1) positive transfer and\ncapacity dilution and (2) the performance of high and low resource languages at\nscale. Finally, we show, for the first time, the possibility of multilingual\nmodeling without sacrificing per-language performance; XLM-R is very\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make our code, data and models publicly available.",
        "arxiv_firstAuthor": "Alexis Conneau",
        "arxiv_title": "Unsupervised Cross-lingual Representation Learning at Scale",
        "arxiv_num": "1911.02116",
        "arxiv_published": "2019-11-05T22:42:00Z",
        "arxiv_updated": "2020-04-08T01:02:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "https://aclanthology.org/2020.acl-main.747.pdf",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2106_04098_ultra_fine_entity_",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_type_prediction",
            "http://www.semanlink.net/tag/acl_2021",
            "http://www.semanlink.net/tag/discute_avec_raphael"
        ],
        "comment": "> we propose to obtain\r\ntraining data for ultra-fine entity typing by using\r\na BERT Masked Language Model. Given a mention in a sentence, our approach\r\nconstructs an input for the BERT MLM so that\r\nit predicts context dependent hypernyms of the\r\nmention, which can be used as type labels\r\n\r\nRefers to [[1807.04905] Ultra-Fine Entity Typing](doc:2021/06/1807_04905_ultra_fine_entity_)",
        "title": "[2106.04098] Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/06/1807_04905_ultra_fine_entity_"
        ],
        "creationTime": "2021-06-16T11:26:44Z",
        "creationDate": "2021-06-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/2106.04098"
        ],
        "arxiv_author": [
            "Yangqiu Song",
            "Haixun Wang",
            "Hongliang Dai"
        ],
        "arxiv_summary": "Recently, there is an effort to extend fine-grained entity typing by using a\nricher and ultra-fine set of types, and labeling noun phrases including\npronouns and nominal nouns instead of just named entity mentions. A key\nchallenge for this ultra-fine entity typing task is that human annotated data\nare extremely scarce, and the annotation ability of existing distant or weak\nsupervision approaches is very limited. To remedy this problem, in this paper,\nwe propose to obtain training data for ultra-fine entity typing by using a BERT\nMasked Language Model (MLM). Given a mention in a sentence, our approach\nconstructs an input for the BERT MLM so that it predicts context dependent\nhypernyms of the mention, which can be used as type labels. Experimental\nresults demonstrate that, with the help of these automatically generated\nlabels, the performance of an ultra-fine entity typing model can be improved\nsubstantially. We also show that our approach can be applied to improve\ntraditional fine-grained entity typing after performing simple type mapping.",
        "arxiv_firstAuthor": "Hongliang Dai",
        "arxiv_title": "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
        "arxiv_num": "2106.04098",
        "arxiv_published": "2021-06-08T04:43:28Z",
        "arxiv_updated": "2021-06-08T04:43:28Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1703.00993",
        "tag": [
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/oov",
            "http://www.semanlink.net/tag/nlp_reading_comprehension",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov"
        ],
        "comment": "abstract: \r\nThe focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on\r\n\r\n1.  the use of pre-trained word embeddings, and\r\n2. the representation of out-of-vocabulary tokens at test time, \r\n\r\ncan turn out to have a larger impact than architectural choices on the final performance\r\n\r\n\r\n\r\n",
        "title": "[1703.00993] A Comparative Study of Word Embeddings for Reading Comprehension",
        "relatedDoc": [],
        "creationTime": "2017-08-28T00:22:38Z",
        "creationDate": "2017-08-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "William W. Cohen",
            "Hanxiao Liu",
            "Ruslan Salakhutdinov",
            "Bhuwan Dhingra"
        ],
        "arxiv_summary": "The focus of past machine learning research for Reading Comprehension tasks\nhas been primarily on the design of novel deep learning architectures. Here we\nshow that seemingly minor choices made on (1) the use of pre-trained word\nembeddings, and (2) the representation of out-of-vocabulary tokens at test\ntime, can turn out to have a larger impact than architectural choices on the\nfinal performance. We systematically explore several options for these choices,\nand provide recommendations to researchers working in this area.",
        "arxiv_firstAuthor": "Bhuwan Dhingra",
        "arxiv_title": "A Comparative Study of Word Embeddings for Reading Comprehension",
        "arxiv_num": "1703.00993",
        "arxiv_published": "2017-03-02T23:58:54Z",
        "arxiv_updated": "2017-03-02T23:58:54Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1905_10070_label_aware_docume",
        "tag": [
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/extreme_multi_label_classification",
            "http://www.semanlink.net/tag/text_multi_label_classification",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/classification_relations_between_classes",
            "http://www.semanlink.net/tag/nlp_4_semanlink"
        ],
        "comment": "> This paper is motivated to better explore the semantic **relationship between each document and extreme labels by taking advantage of both document content and label correlation**. Our objective is to establish an explicit **label-aware representation for each document**.\r\n\r\n> LAHA consists of three parts. \r\n> 1. The first part\r\nadopts a multi-label self-attention mechanism **to detect the contribution\r\nof each word to labels**. \r\n> 2. The second part exploits the label structure and\r\ndocument content **to determine the semantic connection between words\r\nand labels in a same latent space**. \r\n> 3. An adaptive fusion strategy is designed\r\nin the third part to obtain the final label-aware document representation\r\n\r\n[Github](https://github.com/HX-idiot/Hybrid_Attention_XML)\r\n\r\n// TODO compare with [this](doc:2020/08/2003_11644_multi_label_text_c)",
        "title": "[1905.10070] Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/08/2003_11644_multi_label_text_c"
        ],
        "creationTime": "2019-06-22T17:15:57Z",
        "creationDate": "2019-06-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.10070"
        ],
        "arxiv_author": [
            "Xin Huang",
            "Lin Xiao",
            "Liping Jing",
            "Boli Chen"
        ],
        "arxiv_summary": "Extreme multi-label text classification (XMTC) aims at tagging a document\nwith most relevant labels from an extremely large-scale label set. It is a\nchallenging problem especially for the tail labels because there are only few\ntraining documents to build classifier. This paper is motivated to better\nexplore the semantic relationship between each document and extreme labels by\ntaking advantage of both document content and label correlation. Our objective\nis to establish an explicit label-aware representation for each document with a\nhybrid attention deep neural network model(LAHA). LAHA consists of three parts.\nThe first part adopts a multi-label self-attention mechanism to detect the\ncontribution of each word to labels. The second part exploits the label\nstructure and document content to determine the semantic connection between\nwords and labels in a same latent space. An adaptive fusion strategy is\ndesigned in the third part to obtain the final label-aware document\nrepresentation so that the essence of previous two parts can be sufficiently\nintegrated. Extensive experiments have been conducted on six benchmark datasets\nby comparing with the state-of-the-art methods. The results show the\nsuperiority of our proposed LAHA method, especially on the tail labels.",
        "arxiv_firstAuthor": "Xin Huang",
        "arxiv_title": "Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification",
        "arxiv_num": "1905.10070",
        "arxiv_published": "2019-05-24T07:30:34Z",
        "arxiv_updated": "2019-07-12T02:45:08Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/12/2004_10964_don_t_stop_pretrai",
        "tag": [
            "http://www.semanlink.net/tag/allennlp",
            "http://www.semanlink.net/tag/nlp_pretraining",
            "http://www.semanlink.net/tag/language_model_fine_tuning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/domain_adaptation"
        ],
        "comment": "> a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining.",
        "title": "[2004.10964] Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
        "relatedDoc": [],
        "creationTime": "2020-12-01T15:43:33Z",
        "creationDate": "2020-12-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.10964"
        ],
        "arxiv_author": [
            "Iz Beltagy",
            "Doug Downey",
            "Ana Marasovi\u0107",
            "Suchin Gururangan",
            "Swabha Swayamdipta",
            "Noah A. Smith",
            "Kyle Lo"
        ],
        "arxiv_summary": "Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance.",
        "arxiv_firstAuthor": "Suchin Gururangan",
        "arxiv_title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
        "arxiv_num": "2004.10964",
        "arxiv_published": "2020-04-23T04:21:19Z",
        "arxiv_updated": "2020-05-05T22:00:44Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1709.02840",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/introduction",
            "http://www.semanlink.net/tag/machine_learning"
        ],
        "comment": "",
        "title": "[1709.02840] A Brief Introduction to Machine Learning for Engineers",
        "relatedDoc": [],
        "creationTime": "2017-09-26T14:08:05Z",
        "creationDate": "2017-09-26",
        "bookmarkOf": [],
        "arxiv_author": [
            "Osvaldo Simeone"
        ],
        "arxiv_summary": "This monograph aims at providing an introduction to key concepts, algorithms,\nand theoretical results in machine learning. The treatment concentrates on\nprobabilistic models for supervised and unsupervised learning problems. It\nintroduces fundamental concepts and algorithms by building on first principles,\nwhile also exposing the reader to more advanced topics with extensive pointers\nto the literature, within a unified notation and mathematical framework. The\nmaterial is organized according to clearly defined categories, such as\ndiscriminative and generative models, frequentist and Bayesian approaches,\nexact and approximate inference, as well as directed and undirected models.\nThis monograph is meant as an entry point for researchers with a background in\nprobability and linear algebra.",
        "arxiv_firstAuthor": "Osvaldo Simeone",
        "arxiv_title": "A Brief Introduction to Machine Learning for Engineers",
        "arxiv_num": "1709.02840",
        "arxiv_published": "2017-09-08T19:21:26Z",
        "arxiv_updated": "2018-05-17T18:28:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1905_11852_educe_explaining_",
        "tag": [
            "http://www.semanlink.net/tag/explainable_nlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ludovic_denoyer"
        ],
        "comment": "> Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model's prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts.\r\n\r\nPresented in these [slides](/doc/2019/12/unsupervised_learning_with_text)",
        "title": "[1905.11852] EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/12/unsupervised_learning_with_text"
        ],
        "creationTime": "2019-12-05T15:03:48Z",
        "creationDate": "2019-12-05",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.11852"
        ],
        "arxiv_author": [
            "Ludovic Denoyer",
            "Diane Bouchacourt"
        ],
        "arxiv_summary": "Providing explanations along with predictions is crucial in some text\nprocessing tasks. Therefore, we propose a new self-interpretable model that\nperforms output prediction and simultaneously provides an explanation in terms\nof the presence of particular concepts in the input. To do so, our model's\nprediction relies solely on a low-dimensional binary representation of the\ninput, where each feature denotes the presence or absence of concepts. The\npresence of a concept is decided from an excerpt i.e. a small sequence of\nconsecutive words in the text. Relevant concepts for the prediction task at\nhand are automatically defined by our model, avoiding the need for\nconcept-level annotations. To ease interpretability, we enforce that for each\nconcept, the corresponding excerpts share similar semantics and are\ndifferentiable from each others. We experimentally demonstrate the relevance of\nour approach on text classification and multi-sentiment analysis tasks.",
        "arxiv_firstAuthor": "Diane Bouchacourt",
        "arxiv_title": "EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction",
        "arxiv_num": "1905.11852",
        "arxiv_published": "2019-05-28T14:33:19Z",
        "arxiv_updated": "2019-09-27T14:16:30Z",
        "source": "",
        "mainDoc": "http://www.semanlink.net/doc/2019/12/journee_commune_afia_aria_2",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/1712_05972_train_once_test_a",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/zero_shot_learning",
            "http://www.semanlink.net/tag/nlp_text_classification",
            "http://www.semanlink.net/tag/zero_shot_text_classifier"
        ],
        "comment": "> The model learns to predict whether a given sentence is related to a tag or not; unlike other classifiers that learn to classify the sentence as one of the possible classes\r\n\r\ninput: concatenation of the embedding of text and embedding of tag ; output : related / not related (binary classifier)\r\n\r\n> We can say that this technique learns the concept of relatedness between\r\na sentence and a word that can be extended beyond datasets. That said, the levels of accuracy leave\r\na lot of scope for future work.",
        "title": "[1712.05972] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification",
        "relatedDoc": [],
        "creationTime": "2021-10-16T13:59:40Z",
        "creationDate": "2021-10-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/1712.05972"
        ],
        "arxiv_author": [
            "Muktabh Mayank Srivastava",
            "Pushpankar Kumar Pushp"
        ],
        "arxiv_summary": "Zero-shot Learners are models capable of predicting unseen classes. In this\nwork, we propose a Zero-shot Learning approach for text categorization. Our\nmethod involves training model on a large corpus of sentences to learn the\nrelationship between a sentence and embedding of sentence's tags. Learning such\nrelationship makes the model generalize to unseen sentences, tags, and even new\ndatasets provided they can be put into same embedding space. The model learns\nto predict whether a given sentence is related to a tag or not; unlike other\nclassifiers that learn to classify the sentence as one of the possible classes.\nWe propose three different neural networks for the task and report their\naccuracy on the test set of the dataset used for training them as well as two\nother standard datasets for which no retraining was done. We show that our\nmodels generalize well across new unseen classes in both cases. Although the\nmodels do not achieve the accuracy level of the state of the art supervised\nmodels, yet it evidently is a step forward towards general intelligence in\nnatural language processing.",
        "arxiv_firstAuthor": "Pushpankar Kumar Pushp",
        "arxiv_title": "Train Once, Test Anywhere: Zero-Shot Learning for Text Classification",
        "arxiv_num": "1712.05972",
        "arxiv_published": "2017-12-16T15:17:07Z",
        "arxiv_updated": "2017-12-23T20:05:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1607.00570",
        "tag": [
            "http://www.semanlink.net/tag/nlp_short_texts",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/tf_idf"
        ],
        "comment": "A method based on word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. <a href=\"https://github.com/cedricdeboom/RepresentationLearning\">Github</a> (hmm...) (python code)\r\n\r\n",
        "title": "[1607.00570] Representation learning for very short texts using weighted word embedding aggregation",
        "relatedDoc": [],
        "creationTime": "2017-06-09T15:01:36Z",
        "creationDate": "2017-06-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Bart Dhoedt",
            "Thomas Demeester",
            "Steven Van Canneyt",
            "Cedric De Boom"
        ],
        "arxiv_summary": "Short text messages such as tweets are very noisy and sparse in their use of\nvocabulary. Traditional textual representations, such as tf-idf, have\ndifficulty grasping the semantic meaning of such texts, which is important in\napplications such as event detection, opinion mining, news recommendation, etc.\nWe constructed a method based on semantic word embeddings and frequency\ninformation to arrive at low-dimensional representations for short texts\ndesigned to capture semantic similarity. For this purpose we designed a\nweight-based model and a learning procedure based on a novel median-based loss\nfunction. This paper discusses the details of our model and the optimization\nmethods, together with the experimental results on both Wikipedia and Twitter\ndata. We find that our method outperforms the baseline approaches in the\nexperiments, and that it generalizes well on different word embeddings without\nretraining. Our method is therefore capable of retaining most of the semantic\ninformation in the text, and is applicable out-of-the-box.",
        "arxiv_firstAuthor": "Cedric De Boom",
        "arxiv_title": "Representation learning for very short texts using weighted word embedding aggregation",
        "arxiv_num": "1607.00570",
        "arxiv_published": "2016-07-02T23:10:09Z",
        "arxiv_updated": "2016-07-02T23:10:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/pdf/1608.04062v1.pdf",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/backpropagation",
            "http://www.semanlink.net/tag/deep_learning"
        ],
        "comment": "This paper seems too good to be true! They can train a VGG-like net VERY quickly to good accuracy, without backprop.",
        "title": "[1608.04062] Stacked Approximated Regression Machine: A Simple Deep Learning Approach",
        "relatedDoc": [],
        "creationTime": "2016-09-03T12:32:25Z",
        "creationDate": "2016-09-03",
        "bookmarkOf": [],
        "arxiv_author": [
            "Shiyu Chang",
            "Qing Ling",
            "Xia Hu",
            "Thomas S. Huang",
            "Honghui Shi",
            "Zhangyang Wang",
            "Shuai Huang"
        ],
        "arxiv_summary": "With the agreement of my coauthors, I Zhangyang Wang would like to withdraw\nthe manuscript \"Stacked Approximated Regression Machine: A Simple Deep Learning\nApproach\". Some experimental procedures were not included in the manuscript,\nwhich makes a part of important claims not meaningful. In the relevant\nresearch, I was solely responsible for carrying out the experiments; the other\ncoauthors joined in the discussions leading to the main algorithm.\nPlease see the updated text for more details.",
        "arxiv_firstAuthor": "Zhangyang Wang",
        "arxiv_title": "Stacked Approximated Regression Machine: A Simple Deep Learning Approach",
        "arxiv_num": "1608.04062",
        "arxiv_published": "2016-08-14T05:35:11Z",
        "arxiv_updated": "2016-09-08T17:46:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1411.4166",
        "tag": [
            "http://www.semanlink.net/tag/manaal_faruqui",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/these_irit_renault_biblio_initiale",
            "http://www.semanlink.net/tag/word_embeddings_with_lexical_resources"
        ],
        "comment": "Method for refining vector space representations using relational information from semantic lexicons **by encouraging linked words to have similar vector representations**, and it makes no assumptions about how the input vectors were constructed. \r\n\r\nGraph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call \u201cretrofitting.\u201d Retrofitting is applied as a **post-processing step** by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors. This allows retrofitting to be used on pre-trained word vectors obtained using any vector training model.\r\n\r\n[github](https://github.com/mfaruqui/retrofitting)\r\n\r\n",
        "title": "[1411.4166] Retrofitting Word Vectors to Semantic Lexicons",
        "relatedDoc": [],
        "creationTime": "2018-02-25T18:06:07Z",
        "creationDate": "2018-02-25",
        "bookmarkOf": [],
        "arxiv_author": [
            "Manaal Faruqui",
            "Noah A. Smith",
            "Eduard Hovy",
            "Chris Dyer",
            "Jesse Dodge",
            "Sujay K. Jauhar"
        ],
        "arxiv_summary": "Vector space word representations are learned from distributional information\nof words in large corpora. Although such statistics are semantically\ninformative, they disregard the valuable information that is contained in\nsemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This\npaper proposes a method for refining vector space representations using\nrelational information from semantic lexicons by encouraging linked words to\nhave similar vector representations, and it makes no assumptions about how the\ninput vectors were constructed. Evaluated on a battery of standard lexical\nsemantic evaluation tasks in several languages, we obtain substantial\nimprovements starting with a variety of word vector models. Our refinement\nmethod outperforms prior techniques for incorporating semantic lexicons into\nthe word vector training algorithms.",
        "arxiv_firstAuthor": "Manaal Faruqui",
        "arxiv_title": "Retrofitting Word Vectors to Semantic Lexicons",
        "arxiv_num": "1411.4166",
        "arxiv_published": "2014-11-15T17:34:20Z",
        "arxiv_updated": "2015-03-22T17:55:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/2010_07245_text_classificatio",
        "tag": [
            "http://www.semanlink.net/tag/zero_shot_text_classifier",
            "http://www.semanlink.net/tag/self_training",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/text_classification_using_label_names_only"
        ],
        "comment": "> In this paper, we explore the potential of only **using the label name of each class** to train classification models on unlabeled data, **without using any labeled documents**. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method\r\n> 1. associates semantically related words with the label names,\r\n> 2. finds category-indicative words and trains the model to predict their implied categories, and\r\n> 3. generalizes the model via self-training.",
        "title": "[2010.07245] Text Classification Using Label Names Only: A Language Model Self-Training Approach",
        "relatedDoc": [],
        "creationTime": "2021-10-16T13:48:25Z",
        "creationDate": "2021-10-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.07245"
        ],
        "arxiv_author": [
            "Chao Zhang",
            "Jiaxin Huang",
            "Chenyan Xiong",
            "Yu Meng",
            "Heng Ji",
            "Yunyi Zhang",
            "Jiawei Han"
        ],
        "arxiv_summary": "Current text classification methods typically require a good number of\nhuman-labeled documents as training data, which can be costly and difficult to\nobtain in real applications. Humans can perform classification without seeing\nany labeled examples but only based on a small set of words describing the\ncategories to be classified. In this paper, we explore the potential of only\nusing the label name of each class to train classification models on unlabeled\ndata, without using any labeled documents. We use pre-trained neural language\nmodels both as general linguistic knowledge sources for category understanding\nand as representation learning models for document classification. Our method\n(1) associates semantically related words with the label names, (2) finds\ncategory-indicative words and trains the model to predict their implied\ncategories, and (3) generalizes the model via self-training. We show that our\nmodel achieves around 90% accuracy on four benchmark datasets including topic\nand sentiment classification without using any labeled documents but learning\nfrom unlabeled data supervised by at most 3 words (1 in most cases) per class\nas the label name.",
        "arxiv_firstAuthor": "Yu Meng",
        "arxiv_title": "Text Classification Using Label Names Only: A Language Model Self-Training Approach",
        "arxiv_num": "2010.07245",
        "arxiv_published": "2020-10-14T17:06:41Z",
        "arxiv_updated": "2020-10-14T17:06:41Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/2004_14958_a_call_for_more_ri",
        "tag": [
            "http://www.semanlink.net/tag/sebastian_ruder",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/cross_lingual_nlp",
            "http://www.semanlink.net/tag/unsupervised_machine_translation",
            "http://www.semanlink.net/tag/ml_evaluation"
        ],
        "comment": "> a scenario without any parallel data and abundant monolingual data is unrealistic in practice",
        "title": "[2004.14958] A Call for More Rigor in Unsupervised Cross-lingual Learning",
        "relatedDoc": [],
        "creationTime": "2020-05-02T12:35:54Z",
        "creationDate": "2020-05-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.14958"
        ],
        "arxiv_author": [
            "Dani Yogatama",
            "Mikel Artetxe",
            "Gorka Labaka",
            "Sebastian Ruder",
            "Eneko Agirre"
        ],
        "arxiv_summary": "We review motivations, definition, approaches, and methodology for\nunsupervised cross-lingual learning and call for a more rigorous position in\neach of them. An existing rationale for such research is based on the lack of\nparallel data for many of the world's languages. However, we argue that a\nscenario without any parallel data and abundant monolingual data is unrealistic\nin practice. We also discuss different training signals that have been used in\nprevious work, which depart from the pure unsupervised setting. We then\ndescribe common methodological issues in tuning and evaluation of unsupervised\ncross-lingual models and present best practices. Finally, we provide a unified\noutlook for different types of research in this area (i.e., cross-lingual word\nembeddings, deep multilingual pretraining, and unsupervised machine\ntranslation) and argue for comparable evaluation of these models.",
        "arxiv_firstAuthor": "Mikel Artetxe",
        "arxiv_title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
        "arxiv_num": "2004.14958",
        "arxiv_published": "2020-04-30T17:06:23Z",
        "arxiv_updated": "2020-04-30T17:06:23Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/2001_09522_taxoexpan_self_su",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_graph_completion",
            "http://www.semanlink.net/tag/taxonomy_expansion_task",
            "http://www.semanlink.net/tag/text_aware_kg_embedding",
            "http://www.semanlink.net/tag/thewebconf_2020",
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/microsoft_research",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/taxonomies",
            "http://www.semanlink.net/tag/graph_neural_networks"
        ],
        "comment": "how to add a set of new concepts to an existing taxonomy. \r\n\r\n[Tweet](https://twitter.com/mickeyjs6/status/1253772146142216194?s=20) [GitHub](https://github.com/mickeystroller/TaxoExpan)\r\n\r\n> we study the taxonomy expansion task: given an\r\nexisting taxonomy and a set of new emerging concepts, we aim\r\nto automatically expand the taxonomy to incorporate these new\r\nconcepts (without changing the existing relations in the given taxonomy).\r\n\r\n> To the best of our knowledge, this is the first study on **how to\r\nexpand an existing directed acyclic graph (as we model a taxonomy\r\nas a DAG) using self-supervised learning**.\r\n\r\nSelf-supervised framework, the existing taxonomy being used as training data: it learns a model to predict whether a query concept is the direct hyponym of an anchor concept. \r\n\r\n> 2 techniques:\r\n>\r\n> 1. a **position-enhanced graph neural network that encodes the local structure of an anchor concept** in the existing taxonomy,\r\n> 2. a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. \r\n\r\nRegarding 1: uses [GNN](/tag/graph_neural_networks.html) to model the \"ego network\" of concepts (potential \u201csiblings\u201d\r\nand \u201cgrand parents\u201d of the query concept).\r\n\r\n> Regular\r\nGNNs fail to distinguish nodes with different relative positions to\r\nthe query (i.e., some nodes are grand parents of the query while\r\nthe others are siblings of the query). To address this limitation, we\r\npresent a simple but effective enhancement to inject such position\r\ninformation into GNNs using position embedding. We show that\r\nsuch embedding can be easily integrated with existing GNN architectures\r\n(e.g., [GCN](/tag/graph_convolutional_networks) and GAT) and significantly boosts the\r\nprediction performance\r\n\r\nRegarding point 2: uses InfoNCE loss, cf. [Contrastive Predictive Coding](/doc/?uri=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748)\r\n\r\n> Instead of predicting\r\nwhether each individual \u27e8query concept, anchor concept\u27e9 pair\r\nis positive or not, we first group all pairs sharing the same query\r\nconcept into a single training instance and learn a model to select\r\nthe positive pair among other negative ones from the group. \r\n\r\n(Hum, \u00e7a me rappelle quelque chose)\r\n\r\n> assume each concept (in existing taxonomy + set of new concepts) has an initial embedding\r\nvector learned from some text associated with this concept.\r\n\r\nTo keep things tractable, only attempts to find a single parent node of each new concept.",
        "title": "[2001.09522] TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network",
        "relatedDoc": [
            "https://arxiv.org/abs/1807.03748"
        ],
        "creationTime": "2020-04-25T10:03:35Z",
        "creationDate": "2020-04-25",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.09522"
        ],
        "arxiv_author": [
            "Kuansan Wang",
            "Chenyan Xiong",
            "Jiaming Shen",
            "Jiawei Han",
            "Chi Wang",
            "Zhihong Shen"
        ],
        "arxiv_summary": "Taxonomies consist of machine-interpretable semantics and provide valuable\nknowledge for many web applications. For example, online retailers (e.g.,\nAmazon and eBay) use taxonomies for product recommendation, and web search\nengines (e.g., Google and Bing) leverage taxonomies to enhance query\nunderstanding. Enormous efforts have been made on constructing taxonomies\neither manually or semi-automatically. However, with the fast-growing volume of\nweb content, existing taxonomies will become outdated and fail to capture\nemerging knowledge. Therefore, in many applications, dynamic expansions of an\nexisting taxonomy are in great demand. In this paper, we study how to expand an\nexisting taxonomy by adding a set of new concepts. We propose a novel\nself-supervised framework, named TaxoExpan, which automatically generates a set\nof <query concept, anchor concept> pairs from the existing taxonomy as training\ndata. Using such self-supervision data, TaxoExpan learns a model to predict\nwhether a query concept is the direct hyponym of an anchor concept. We develop\ntwo innovative techniques in TaxoExpan: (1) a position-enhanced graph neural\nnetwork that encodes the local structure of an anchor concept in the existing\ntaxonomy, and (2) a noise-robust training objective that enables the learned\nmodel to be insensitive to the label noise in the self-supervision data.\nExtensive experiments on three large-scale datasets from different domains\ndemonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy\nexpansion.",
        "arxiv_firstAuthor": "Jiaming Shen",
        "arxiv_title": "TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network",
        "arxiv_num": "2001.09522",
        "arxiv_published": "2020-01-26T21:30:21Z",
        "arxiv_updated": "2020-01-26T21:30:21Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/2110_08207_multitask_prompted",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/zero_shot",
            "http://www.semanlink.net/tag/huggingface_bigscience"
        ],
        "comment": "[Tweet](https://twitter.com/BigscienceW/status/1450084548872744961?s=20)",
        "title": "[2110.08207] Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "relatedDoc": [],
        "creationTime": "2021-10-18T23:12:20Z",
        "creationDate": "2021-10-18",
        "bookmarkOf": [
            "https://arxiv.org/abs/2110.08207"
        ],
        "arxiv_author": [
            "Albert Webson",
            "Alexander M. Rush",
            "Tali Bers",
            "Ryan Teehan",
            "M Saiful Bari",
            "Teven Le Scao",
            "Jonathan Chang",
            "Gunjan Chhablani",
            "Colin Raffel",
            "Jason Alan Fries",
            "Abheesht Sharma",
            "Stephen H. Bach",
            "Eliza Szczechla",
            "Urmish Thakker",
            "Lintang Sutawika",
            "Manan Dey",
            "Zheng Xin Yong",
            "Antoine Chaffin",
            "Rachel Bawden",
            "Mike Tian-Jian Jiang",
            "Andrea Santilli",
            "Matteo Manica",
            "Sheng Shen",
            "Jos Rozen",
            "Thomas Wolf",
            "Nihal Nayak",
            "Thomas Wang",
            "Leo Gao",
            "Canwen Xu",
            "Arun Raja",
            "Harshit Pandey",
            "Debajyoti Datta",
            "Stella Biderman",
            "Shanya Sharma Sharma",
            "Zaid Alyafeai",
            "Victor Sanh",
            "Trishala Neeraj",
            "Arnaud Stiegler",
            "Thibault Fevry",
            "Taewoon Kim",
            "Han Wang"
        ],
        "arxiv_summary": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks. It has been hypothesized that this is\na consequence of implicit multitask learning in language model training. Can\nzero-shot generalization instead be directly induced by explicit multitask\nlearning? To test this question at scale, we develop a system for easily\nmapping general natural language tasks into a human-readable prompted form. We\nconvert a large set of supervised datasets, each with multiple prompts using\nvarying natural language. These prompted datasets allow for benchmarking the\nability of a model to perform completely unseen tasks specified in natural\nlanguage. We fine-tune a pretrained encoder-decoder model on this multitask\nmixture covering a wide variety of tasks. The model attains strong zero-shot\nperformance on several standard datasets, often outperforming models 16x its\nsize. Further, our approach attains strong performance on a subset of tasks\nfrom the BIG-Bench benchmark, outperforming models 6x its size. All prompts and\ntrained models are available at github.com/bigscience-workshop/promptsource/.",
        "arxiv_firstAuthor": "Victor Sanh",
        "arxiv_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "arxiv_num": "2110.08207",
        "arxiv_published": "2021-10-15T17:08:57Z",
        "arxiv_updated": "2021-10-15T17:08:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1812.09449",
        "tag": [
            "http://www.semanlink.net/tag/named_entity_recognition",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "mainly focus on generic NEs in English language",
        "title": "[1812.09449] A Survey on Deep Learning for Named Entity Recognition",
        "relatedDoc": [],
        "creationTime": "2019-04-24T00:28:42Z",
        "creationDate": "2019-04-24",
        "bookmarkOf": [],
        "arxiv_author": [
            "Chenliang Li",
            "Jianglei Han",
            "Jing Li",
            "Aixin Sun"
        ],
        "arxiv_summary": "Named entity recognition (NER) is the task to identify mentions of rigid\ndesignators from text belonging to predefined semantic types such as person,\nlocation, organization etc. NER always serves as the foundation for many\nnatural language applications such as question answering, text summarization,\nand machine translation. Early NER systems got a huge success in achieving good\nperformance with the cost of human engineering in designing domain-specific\nfeatures and rules. In recent years, deep learning, empowered by continuous\nreal-valued vector representations and semantic composition through nonlinear\nprocessing, has been employed in NER systems, yielding stat-of-the-art\nperformance. In this paper, we provide a comprehensive review on existing deep\nlearning techniques for NER. We first introduce NER resources, including tagged\nNER corpora and off-the-shelf NER tools. Then, we systematically categorize\nexisting works based on a taxonomy along three axes: distributed\nrepresentations for input, context encoder, and tag decoder. Next, we survey\nthe most representative methods for recent applied techniques of deep learning\nin new NER problem settings and applications. Finally, we present readers with\nthe challenges faced by NER systems and outline future directions in this area.",
        "arxiv_firstAuthor": "Jing Li",
        "arxiv_title": "A Survey on Deep Learning for Named Entity Recognition",
        "arxiv_num": "1812.09449",
        "arxiv_published": "2018-12-22T04:54:13Z",
        "arxiv_updated": "2020-03-18T15:57:10Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/03/2103_12876_complex_factoid_qu",
        "tag": [
            "http://www.semanlink.net/tag/question_answering",
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> delft builds a free-text knowledge graph from Wikipedia, with entities as nodes and sentences in which entities co-occur as edges",
        "title": "[2103.12876] Complex Factoid Question Answering with a Free-Text Knowledge Graph",
        "relatedDoc": [],
        "creationTime": "2021-03-30T00:35:13Z",
        "creationDate": "2021-03-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/2103.12876"
        ],
        "arxiv_author": [
            "Jordan Boyd-Graber",
            "Xin Qian",
            "Chenyan Xiong",
            "Chen Zhao"
        ],
        "arxiv_summary": "We introduce DELFT, a factoid question answering system which combines the\nnuance and depth of knowledge graph question answering approaches with the\nbroader coverage of free-text. DELFT builds a free-text knowledge graph from\nWikipedia, with entities as nodes and sentences in which entities co-occur as\nedges. For each question, DELFT finds the subgraph linking question entity\nnodes to candidates using text sentences as edges, creating a dense and high\ncoverage semantic graph. A novel graph neural network reasons over the\nfree-text graph-combining evidence on the nodes via information along edge\nsentences-to select a final answer. Experiments on three question answering\ndatasets show DELFT can answer entity-rich questions better than machine\nreading based models, bert-based answer ranking and memory networks. DELFT's\nadvantage comes from both the high coverage of its free-text knowledge\ngraph-more than double that of dbpedia relations-and the novel graph neural\nnetwork which reasons on the rich but noisy free-text evidence.",
        "arxiv_firstAuthor": "Chen Zhao",
        "arxiv_title": "Complex Factoid Question Answering with a Free-Text Knowledge Graph",
        "arxiv_num": "2103.12876",
        "arxiv_published": "2021-03-23T22:53:09Z",
        "arxiv_updated": "2021-03-23T22:53:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/04/2007_12603_ir_bert_leveragin",
        "tag": [
            "http://www.semanlink.net/tag/semantic_search",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/okapi_bm25",
            "http://www.semanlink.net/tag/embeddings_in_ir"
        ],
        "comment": "",
        "title": "[2007.12603] IR-BERT: Leveraging BERT for Semantic Search in Background Linking for News Articles",
        "relatedDoc": [],
        "creationTime": "2021-04-12T18:27:34Z",
        "creationDate": "2021-04-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/2007.12603"
        ],
        "arxiv_author": [
            "Udhav Sethi",
            "Anup Anand Deshmukh"
        ],
        "arxiv_summary": "This work describes our two approaches for the background linking task of\nTREC 2020 News Track. The main objective of this task is to recommend a list of\nrelevant articles that the reader should refer to in order to understand the\ncontext and gain background information of the query article. Our first\napproach focuses on building an effective search query by combining weighted\nkeywords extracted from the query document and uses BM25 for retrieval. The\nsecond approach leverages the capability of SBERT (Nils Reimers et al.) to\nlearn contextual representations of the query in order to perform semantic\nsearch over the corpus. We empirically show that employing a language model\nbenefits our approach in understanding the context as well as the background of\nthe query article. The proposed approaches are evaluated on the TREC 2018\nWashington Post dataset and our best model outperforms the TREC median as well\nas the highest scoring model of 2018 in terms of the nDCG@5 metric. We further\npropose a diversity measure to evaluate the effectiveness of the various\napproaches in retrieving a diverse set of documents. This would potentially\nmotivate researchers to work on introducing diversity in their recommended\nlist. We have open sourced our implementation on Github and plan to submit our\nruns for the background linking task in TREC 2020.",
        "arxiv_firstAuthor": "Anup Anand Deshmukh",
        "arxiv_title": "IR-BERT: Leveraging BERT for Semantic Search in Background Linking for News Articles",
        "arxiv_num": "2007.12603",
        "arxiv_published": "2020-07-24T16:02:14Z",
        "arxiv_updated": "2020-07-24T16:02:14Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/2004_06842_layered_graph_embe",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/yahoo",
            "http://www.semanlink.net/tag/brad_pitt",
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/recommender_systems",
            "http://www.semanlink.net/tag/entity_recommendation"
        ],
        "comment": "an embedding-based entity recommendation framework for Wikipedia that organizes Wikipedia into a collection of graphs layered on top of each other, **learns complementary entity representations from their topology and content**, and combines them with a lightweight **learning-to-rank** approach to recommend related entities on Wikipedia",
        "title": "[2004.06842] Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph",
        "relatedDoc": [],
        "creationTime": "2020-04-17T19:14:01Z",
        "creationDate": "2020-04-17",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.06842"
        ],
        "arxiv_author": [
            "Chien-Chun Ni",
            "Kin Sum Liu",
            "Nicolas Torzec"
        ],
        "arxiv_summary": "In this paper, we describe an embedding-based entity recommendation framework\nfor Wikipedia that organizes Wikipedia into a collection of graphs layered on\ntop of each other, learns complementary entity representations from their\ntopology and content, and combines them with a lightweight learning-to-rank\napproach to recommend related entities on Wikipedia. Through offline and online\nevaluations, we show that the resulting embeddings and recommendations perform\nwell in terms of quality and user engagement. Balancing simplicity and quality,\nthis framework provides default entity recommendations for English and other\nlanguages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.",
        "arxiv_firstAuthor": "Chien-Chun Ni",
        "arxiv_title": "Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph",
        "arxiv_num": "2004.06842",
        "arxiv_published": "2020-04-15T00:49:27Z",
        "arxiv_updated": "2020-04-15T00:49:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1904_02342_text_generation_fr",
        "tag": [
            "http://www.semanlink.net/tag/kg_and_nlp",
            "http://www.semanlink.net/tag/natural_language_generation",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1904.02342] Text Generation from Knowledge Graphs with Graph Transformers",
        "relatedDoc": [],
        "creationTime": "2019-08-23T00:39:46Z",
        "creationDate": "2019-08-23",
        "bookmarkOf": [
            "https://arxiv.org/abs/1904.02342"
        ],
        "arxiv_author": [
            "Yi Luan",
            "Rik Koncel-Kedziorski",
            "Hannaneh Hajishirzi",
            "Mirella Lapata",
            "Dhanush Bekal"
        ],
        "arxiv_summary": "Generating texts which express complex ideas spanning multiple sentences\nrequires a structured representation of their content (document plan), but\nthese representations are prohibitively expensive to manually produce. In this\nwork, we address the problem of generating coherent multi-sentence texts from\nthe output of an information extraction system, and in particular a knowledge\ngraph. Graphical knowledge representations are ubiquitous in computing, but\npose a significant challenge for text generation techniques due to their\nnon-hierarchical nature, collapsing of long-distance dependencies, and\nstructural variety. We introduce a novel graph transforming encoder which can\nleverage the relational structure of such knowledge graphs without imposing\nlinearization or hierarchical constraints. Incorporated into an encoder-decoder\nsetup, we provide an end-to-end trainable system for graph-to-text generation\nthat we apply to the domain of scientific text. Automatic and human evaluations\nshow that our technique produces more informative texts which exhibit better\ndocument structure than competitive encoder-decoder methods.",
        "arxiv_firstAuthor": "Rik Koncel-Kedziorski",
        "arxiv_title": "Text Generation from Knowledge Graphs with Graph Transformers",
        "arxiv_num": "1904.02342",
        "arxiv_published": "2019-04-04T04:33:15Z",
        "arxiv_updated": "2019-05-18T01:07:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/2106_13474_adapt_and_distill_",
        "tag": [
            "http://www.semanlink.net/tag/domain_adaptation",
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/domain_specific_nlp",
            "http://www.semanlink.net/tag/nlp_microsoft"
        ],
        "comment": "> adapting the off-the-\r\nshelf general pretrained models and performing\r\ntask-agnostic knowledge distillation\r\nin target domains\r\n\r\n> Our findings suggest that\r\ndomain-specific vocabulary and general-domain\r\nlanguage model play vital roles in domain adaptation\r\nof a pretrained model",
        "title": "[2106.13474] Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains",
        "relatedDoc": [],
        "creationTime": "2021-10-21T18:24:46Z",
        "creationDate": "2021-10-21",
        "bookmarkOf": [
            "https://arxiv.org/abs/2106.13474"
        ],
        "arxiv_author": [
            "Yunzhi Yao",
            "Li Dong",
            "Shaohan Huang",
            "Furu Wei",
            "Wenhui Wang"
        ],
        "arxiv_summary": "Large pre-trained models have achieved great success in many natural language\nprocessing tasks. However, when they are applied in specific domains, these\nmodels suffer from domain shift and bring challenges in fine-tuning and online\nserving for latency and capacity constraints. In this paper, we present a\ngeneral approach to developing small, fast and effective pre-trained models for\nspecific domains. This is achieved by adapting the off-the-shelf general\npre-trained models and performing task-agnostic knowledge distillation in\ntarget domains. Specifically, we propose domain-specific vocabulary expansion\nin the adaptation stage and employ corpus level occurrence probability to\nchoose the size of incremental vocabulary automatically. Then we systematically\nexplore different strategies to compress the large pre-trained models for\nspecific domains. We conduct our experiments in the biomedical and computer\nscience domain. The experimental results demonstrate that our approach achieves\nbetter performance over the BERT BASE model in domain-specific tasks while 3.3x\nsmaller and 5.1x faster than BERT BASE. The code and pre-trained models are\navailable at https://aka.ms/adalm.",
        "arxiv_firstAuthor": "Yunzhi Yao",
        "arxiv_title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains",
        "arxiv_num": "2106.13474",
        "arxiv_published": "2021-06-25T07:37:05Z",
        "arxiv_updated": "2021-06-29T05:42:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/09/2010_12566_dict_mlm_improved",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/dictionnaire",
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/multilingual_language_models"
        ],
        "comment": "> Despite the strong representation learning capability enabled by MLM, we demonstrate an inherent limitation of MLM for multilingual representation learning. In particular, by requiring the model to predict the language-specific token, the MLM objective disincentivizes learning a language-agnostic representation -- which is a key goal of multilingual pre-training\r\n>\r\n> DICT-MLM works by incentivizing the model\r\nto be able to predict not just the original\r\nmasked word, but potentially any of its crosslingual\r\nsynonyms as well.",
        "title": "[2010.12566] DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries",
        "relatedDoc": [],
        "creationTime": "2021-09-06T18:27:44Z",
        "creationDate": "2021-09-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.12566"
        ],
        "arxiv_author": [
            "Krishna Srinivasan",
            "Karthik Raman",
            "Jiecao Chen",
            "Aditi Chaudhary"
        ],
        "arxiv_summary": "Pre-trained multilingual language models such as mBERT have shown immense\ngains for several natural language processing (NLP) tasks, especially in the\nzero-shot cross-lingual setting. Most, if not all, of these pre-trained models\nrely on the masked-language modeling (MLM) objective as the key language\nlearning objective. The principle behind these approaches is that predicting\nthe masked words with the help of the surrounding text helps learn potent\ncontextualized representations. Despite the strong representation learning\ncapability enabled by MLM, we demonstrate an inherent limitation of MLM for\nmultilingual representation learning. In particular, by requiring the model to\npredict the language-specific token, the MLM objective disincentivizes learning\na language-agnostic representation -- which is a key goal of multilingual\npre-training. Therefore to encourage better cross-lingual representation\nlearning we propose the DICT-MLM method. DICT-MLM works by incentivizing the\nmodel to be able to predict not just the original masked word, but potentially\nany of its cross-lingual synonyms as well. Our empirical analysis on multiple\ndownstream tasks spanning 30+ languages, demonstrates the efficacy of the\nproposed approach and its ability to learn better multilingual representations.",
        "arxiv_firstAuthor": "Aditi Chaudhary",
        "arxiv_title": "DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries",
        "arxiv_num": "2010.12566",
        "arxiv_published": "2020-10-23T17:53:11Z",
        "arxiv_updated": "2020-10-23T17:53:11Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_1909_03193_kg_bert_bert_for_",
        "tag": [
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge_graph_completion",
            "http://www.semanlink.net/tag/attention_is_all_you_need"
        ],
        "comment": "Pre-trained language models for knowledge graph completion. **Triples are treated as textual sequences**. (Hum, j'ai d\u00e9j\u00e0 vu \u00e7a quelque part. Ah, peut-\u00eatre [RDF2VEC](tag:rdf2vec)? // TODO \u00e0 voir)\r\n\r\nTakes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model\r\n\r\n> we first treat entities, relations and triples as\r\ntextual sequences and turn knowledge graph completion into\r\na sequence classification problem. We then fine-tune BERT\r\nmodel on these sequences for predicting the plausibility of\r\na triple or a relation. The method\r\n\r\n[GitHub](https://github.com/yao8839836/kg-bert)",
        "title": "[1909.03193] KG-BERT: BERT for Knowledge Graph Completion",
        "relatedDoc": [],
        "creationTime": "2020-03-22T18:56:43Z",
        "creationDate": "2020-03-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.03193"
        ],
        "arxiv_author": [
            "Liang Yao",
            "Yuan Luo",
            "Chengsheng Mao"
        ],
        "arxiv_summary": "Knowledge graphs are important resources for many artificial intelligence\ntasks but often suffer from incompleteness. In this work, we propose to use\npre-trained language models for knowledge graph completion. We treat triples in\nknowledge graphs as textual sequences and propose a novel framework named\nKnowledge Graph Bidirectional Encoder Representations from Transformer\n(KG-BERT) to model these triples. Our method takes entity and relation\ndescriptions of a triple as input and computes scoring function of the triple\nwith the KG-BERT language model. Experimental results on multiple benchmark\nknowledge graphs show that our method can achieve state-of-the-art performance\nin triple classification, link prediction and relation prediction tasks.",
        "arxiv_firstAuthor": "Liang Yao",
        "arxiv_title": "KG-BERT: BERT for Knowledge Graph Completion",
        "arxiv_num": "1909.03193",
        "arxiv_published": "2019-09-07T06:09:25Z",
        "arxiv_updated": "2019-09-11T06:03:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_1911_02168_coke_contextualiz",
        "tag": [
            "http://www.semanlink.net/tag/baidu",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_knowledge_graphs",
            "http://www.semanlink.net/tag/link_prediction",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings"
        ],
        "comment": "A method to build contextualized entity and relation embeddings. Entities and relations may appear in different graph contexts. **Edges and paths, both formulated as sequences of entities and relations, are passed as input to a Transformer encoder to learn the contextualized representations..**\r\n\r\n[Github](https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE)",
        "title": "[1911.02168] CoKE: Contextualized Knowledge Graph Embedding",
        "relatedDoc": [],
        "creationTime": "2020-03-22T17:34:10Z",
        "creationDate": "2020-03-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.02168"
        ],
        "arxiv_author": [
            "Pingping Huang",
            "Quan Wang",
            "Wenbin Jiang",
            "Haifeng Wang",
            "Jing Liu",
            "Hua Wu",
            "Yajuan Lyu",
            "Songtai Dai",
            "Yong Zhu"
        ],
        "arxiv_summary": "Knowledge graph embedding, which projects symbolic entities and relations\ninto continuous vector spaces, is gaining increasing attention. Previous\nmethods allow a single static embedding for each entity or relation, ignoring\ntheir intrinsic contextual nature, i.e., entities and relations may appear in\ndifferent graph contexts, and accordingly, exhibit different properties. This\nwork presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm\nthat takes into account such contextual nature, and learns dynamic, flexible,\nand fully contextualized entity and relation embeddings. Two types of graph\ncontexts are studied: edges and paths, both formulated as sequences of entities\nand relations. CoKE takes a sequence as input and uses a Transformer encoder to\nobtain contextualized representations. These representations are hence\nnaturally adaptive to the input, capturing contextual meanings of entities and\nrelations therein. Evaluation on a wide variety of public benchmarks verifies\nthe superiority of CoKE in link prediction and path query answering. It\nperforms consistently better than, or at least equally well as current\nstate-of-the-art in almost every case, in particular offering an absolute\nimprovement of 21.0% in H@10 on path query answering. Our code is available at\n\\url{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE}.",
        "arxiv_firstAuthor": "Quan Wang",
        "arxiv_title": "CoKE: Contextualized Knowledge Graph Embedding",
        "arxiv_num": "1911.02168",
        "arxiv_published": "2019-11-06T02:27:39Z",
        "arxiv_updated": "2020-04-04T07:22:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/08/2008_08995_constructing_a_kno",
        "tag": [
            "http://www.semanlink.net/tag/kg_and_nlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/virtual_knowledge_graph",
            "http://www.semanlink.net/tag/knowledge_graph_construction"
        ],
        "comment": "Building a virtual KG from unstructured documents\r\n\r\n> we first extract knowledge tuples in their surface form from unstructured documents, encode them using a pre-trained language model, and link the surface-entities via the encoding to form the graph structure.",
        "title": "[2008.08995] Constructing a Knowledge Graph from Unstructured Documents without External Alignment",
        "relatedDoc": [],
        "creationTime": "2020-08-21T18:38:32Z",
        "creationDate": "2020-08-21",
        "bookmarkOf": [
            "https://arxiv.org/abs/2008.08995"
        ],
        "arxiv_author": [
            "Tianxing He",
            "Seunghak Yu",
            "James Glass"
        ],
        "arxiv_summary": "Knowledge graphs (KGs) are relevant to many NLP tasks, but building a\nreliable domain-specific KG is time-consuming and expensive. A number of\nmethods for constructing KGs with minimized human intervention have been\nproposed, but still require a process to align into the human-annotated\nknowledge base. To overcome this issue, we propose a novel method to\nautomatically construct a KG from unstructured documents that does not require\nexternal alignment and explore its use to extract desired information. To\nsummarize our approach, we first extract knowledge tuples in their surface form\nfrom unstructured documents, encode them using a pre-trained language model,\nand link the surface-entities via the encoding to form the graph structure. We\nperform experiments with benchmark datasets such as WikiMovies and MetaQA. The\nexperimental results show that our method can successfully create and search a\nKG with 18K documents and achieve 69.7% hits@10 (close to an oracle model) on a\nquery retrieval task.",
        "arxiv_firstAuthor": "Seunghak Yu",
        "arxiv_title": "Constructing a Knowledge Graph from Unstructured Documents without External Alignment",
        "arxiv_num": "2008.08995",
        "arxiv_published": "2020-08-20T14:30:33Z",
        "arxiv_updated": "2020-08-20T14:30:33Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1312.6184v5",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/machine_learning"
        ],
        "comment": "",
        "title": "[1312.6184] Do Deep Nets Really Need to be Deep?",
        "relatedDoc": [],
        "creationTime": "2014-10-06T00:29:41Z",
        "creationDate": "2014-10-06",
        "bookmarkOf": [],
        "arxiv_author": [
            "Lei Jimmy Ba",
            "Rich Caruana"
        ],
        "arxiv_summary": "Currently, deep neural networks are the state of the art on problems such as\nspeech recognition and computer vision. In this extended abstract, we show that\nshallow feed-forward networks can learn the complex functions previously\nlearned by deep nets and achieve accuracies previously only achievable with\ndeep models. Moreover, in some cases the shallow neural nets can learn these\ndeep functions using a total number of parameters similar to the original deep\nmodel. We evaluate our method on the TIMIT phoneme recognition task and are\nable to train shallow fully-connected nets that perform similarly to complex,\nwell-engineered, deep convolutional architectures. Our success in training\nshallow neural nets to mimic deeper models suggests that there probably exist\nbetter algorithms for training shallow feed-forward nets than those currently\navailable.",
        "arxiv_firstAuthor": "Lei Jimmy Ba",
        "arxiv_title": "Do Deep Nets Really Need to be Deep?",
        "arxiv_num": "1312.6184",
        "arxiv_published": "2013-12-21T00:47:43Z",
        "arxiv_updated": "2014-10-11T00:19:10Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1909_02164_tabfact_a_large_s",
        "tag": [
            "http://www.semanlink.net/tag/table_based_fact_verification",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "fact verification given semi-structured data as evidence",
        "title": "[1909.02164] TabFact: A Large-scale Dataset for Table-based Fact Verification",
        "relatedDoc": [],
        "creationTime": "2019-12-01T13:20:21Z",
        "creationDate": "2019-12-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.02164"
        ],
        "arxiv_author": [
            "Shiyang Li",
            "Hongmin Wang",
            "Yunkai Zhang",
            "Jianshu Chen",
            "Hong Wang",
            "Wenhu Chen",
            "Xiyou Zhou",
            "William Yang Wang"
        ],
        "arxiv_summary": "The problem of verifying whether a textual hypothesis holds based on the\ngiven evidence, also known as fact verification, plays an important role in the\nstudy of natural language understanding and semantic representation. However,\nexisting studies are mainly restricted to dealing with unstructured evidence\n(e.g., natural language sentences and documents, news, etc), while verification\nunder structured evidence, such as tables, graphs, and databases, remains\nunder-explored. This paper specifically aims to study the fact verification\ngiven semi-structured data as evidence. To this end, we construct a large-scale\ndataset called TabFact with 16k Wikipedia tables as the evidence for 118k\nhuman-annotated natural language statements, which are labeled as either\nENTAILED or REFUTED. TabFact is challenging since it involves both soft\nlinguistic reasoning and hard symbolic reasoning. To address these reasoning\nchallenges, we design two different models: Table-BERT and Latent Program\nAlgorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language\nmodel to encode the linearized tables and statements into continuous vectors\nfor verification. LPA parses statements into programs and executes them against\nthe tables to obtain the returned binary value for verification. Both methods\nachieve similar accuracy but still lag far behind human performance. We also\nperform a comprehensive analysis to demonstrate great future opportunities. The\ndata and code of the dataset are provided in\n\\url{https://github.com/wenhuchen/Table-Fact-Checking}.",
        "arxiv_firstAuthor": "Wenhu Chen",
        "arxiv_title": "TabFact: A Large-scale Dataset for Table-based Fact Verification",
        "arxiv_num": "1909.02164",
        "arxiv_published": "2019-09-05T00:25:17Z",
        "arxiv_updated": "2019-12-31T17:16:32Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/12/2002_08909_realm_retrieval_a",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_augmented_language_models",
            "http://www.semanlink.net/tag/not_encoding_knowledge_in_language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/retrieval_augmented_lm",
            "http://www.semanlink.net/tag/neural_models_for_information_retrieval",
            "http://www.semanlink.net/tag/nlp_google"
        ],
        "comment": "**Augment language model pre-training with a retriever module**, which\r\nis trained using the masked language modeling objective.\r\n\r\n> To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. **For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner**, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents\r\n\r\nHum, #TODO: parallel to be drawn with techniques in [KG-augmented Language Models](tag:knowledge_graph_augmented_language_models) which focus \"on the problem of capturing declarative knowledge in the learned parameters of a language model.\"\r\n\r\n[Google AI Blog Post](doc:2020/08/google_ai_blog_realm_integrat)\r\n\r\n[Summary](https://joeddav.github.io/blog/2020/03/03/REALM.html) for the [Hugging Face awesome-papers reading group](doc:2021/03/huggingface_awesome_papers_pap)",
        "title": "[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/08/google_ai_blog_realm_integrat",
            "http://www.semanlink.net/doc/2021/03/huggingface_awesome_papers_pap"
        ],
        "creationTime": "2020-12-12T02:30:25Z",
        "creationDate": "2020-12-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.08909"
        ],
        "arxiv_author": [
            "Ming-Wei Chang",
            "Kenton Lee",
            "Panupong Pasupat",
            "Kelvin Guu",
            "Zora Tung"
        ],
        "arxiv_summary": "Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.",
        "arxiv_firstAuthor": "Kelvin Guu",
        "arxiv_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
        "arxiv_num": "2002.08909",
        "arxiv_published": "2020-02-10T18:40:59Z",
        "arxiv_updated": "2020-02-10T18:40:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_1503_08677_label_embedding_fo",
        "tag": [
            "http://www.semanlink.net/tag/label_embedding",
            "http://www.semanlink.net/tag/image_classification",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1503.08677] Label-Embedding for Image Classification",
        "relatedDoc": [],
        "creationTime": "2020-02-18T15:00:20Z",
        "creationDate": "2020-02-18",
        "bookmarkOf": [
            "https://arxiv.org/abs/1503.08677"
        ],
        "arxiv_author": [
            "Zaid Harchaoui",
            "Zeynep Akata",
            "Florent Perronnin",
            "Cordelia Schmid"
        ],
        "arxiv_summary": "Attributes act as intermediate representations that enable parameter sharing\nbetween classes, a must when training data is scarce. We propose to view\nattribute-based image classification as a label-embedding problem: each class\nis embedded in the space of attribute vectors. We introduce a function that\nmeasures the compatibility between an image and a label embedding. The\nparameters of this function are learned on a training set of labeled samples to\nensure that, given an image, the correct classes rank higher than the incorrect\nones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets\nshow that the proposed framework outperforms the standard Direct Attribute\nPrediction baseline in a zero-shot learning scenario. Label embedding enjoys a\nbuilt-in ability to leverage alternative sources of information instead of or\nin addition to attributes, such as e.g. class hierarchies or textual\ndescriptions. Moreover, label embedding encompasses the whole range of learning\nsettings from zero-shot learning to regular learning with a large number of\nlabeled examples.",
        "arxiv_firstAuthor": "Zeynep Akata",
        "arxiv_title": "Label-Embedding for Image Classification",
        "arxiv_num": "1503.08677",
        "arxiv_published": "2015-03-30T14:04:34Z",
        "arxiv_updated": "2015-10-01T10:48:38Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1607.01759",
        "tag": [
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_text_classification",
            "http://www.semanlink.net/tag/fasttext",
            "http://www.semanlink.net/tag/tomas_mikolov"
        ],
        "comment": "A simple and efficient baseline for text classification. \r\n\r\n**Our word features can\r\nbe averaged** together to form good sentence representations.\r\n\r\nOur experiments show that fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.\r\n",
        "title": "[1607.01759] Bag of Tricks for Efficient Text Classification",
        "relatedDoc": [],
        "creationTime": "2017-09-10T12:07:48Z",
        "creationDate": "2017-09-10",
        "bookmarkOf": [],
        "arxiv_author": [
            "Edouard Grave",
            "Piotr Bojanowski",
            "Tomas Mikolov",
            "Armand Joulin"
        ],
        "arxiv_summary": "This paper explores a simple and efficient baseline for text classification.\nOur experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation. We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute.",
        "arxiv_firstAuthor": "Armand Joulin",
        "arxiv_title": "Bag of Tricks for Efficient Text Classification",
        "arxiv_num": "1607.01759",
        "arxiv_published": "2016-07-06T19:40:15Z",
        "arxiv_updated": "2016-08-09T17:38:43Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/10/_1910_03524_beyond_vector_spac",
        "tag": [
            "http://www.semanlink.net/tag/poincare_embeddings",
            "http://www.semanlink.net/tag/geometry_of_language_embeddings",
            "http://www.semanlink.net/tag/vector_space_model",
            "http://www.semanlink.net/tag/graphs_machine_learning",
            "http://www.semanlink.net/tag/yandex",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> In this paper, we aim to eliminate the inductive bias imposed by the embedding space geometry. Namely, we propose to map data into more general non-vector metric spaces: a weighted graph with a shortest path distance. By design, such graphs can model arbitrary geometry with a proper configuration of edges and weights. Our main contribution is PRODIGE (Probabilistic Differentiable Graph Embeddings): a method that learns a weighted graph representation of data end-to-end by gradient descent.\r\n\r\n[Github](https://github.com/stanis-morozov/prodige)\r\n\r\n",
        "title": "[1910.03524] Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs",
        "relatedDoc": [],
        "creationTime": "2019-10-09T23:21:08Z",
        "creationDate": "2019-10-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/1910.03524"
        ],
        "arxiv_author": [
            "Vage Egiazarian",
            "Stanislav Morozov",
            "Artem Babenko",
            "Denis Mazur"
        ],
        "arxiv_summary": "Learning useful representations is a key ingredient to the success of modern\nmachine learning. Currently, representation learning mostly relies on embedding\ndata into Euclidean space. However, recent work has shown that data in some\ndomains is better modeled by non-euclidean metric spaces, and inappropriate\ngeometry can result in inferior performance. In this paper, we aim to eliminate\nthe inductive bias imposed by the embedding space geometry. Namely, we propose\nto map data into more general non-vector metric spaces: a weighted graph with a\nshortest path distance. By design, such graphs can model arbitrary geometry\nwith a proper configuration of edges and weights. Our main contribution is\nPRODIGE: a method that learns a weighted graph representation of data\nend-to-end by gradient descent. Greater generality and fewer model assumptions\nmake PRODIGE more powerful than existing embedding-based approaches. We confirm\nthe superiority of our method via extensive experiments on a wide range of\ntasks, including classification, compression, and collaborative filtering.",
        "arxiv_firstAuthor": "Denis Mazur",
        "arxiv_title": "Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs",
        "arxiv_num": "1910.03524",
        "arxiv_published": "2019-10-08T16:31:11Z",
        "arxiv_updated": "2019-10-16T16:43:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_1910_04126_scalable_nearest_n",
        "tag": [
            "http://www.semanlink.net/tag/word_mover_s_distance",
            "http://www.semanlink.net/tag/nearest_neighbor_search",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1910.04126] Scalable Nearest Neighbor Search for Optimal Transport",
        "relatedDoc": [],
        "creationTime": "2020-02-20T09:11:40Z",
        "creationDate": "2020-02-20",
        "bookmarkOf": [
            "https://arxiv.org/abs/1910.04126"
        ],
        "arxiv_author": [
            "Arturs Backurs",
            "Piotr Indyk",
            "Yihe Dong",
            "Ilya Razenshteyn",
            "Tal Wagner"
        ],
        "arxiv_summary": "The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly\npopular similarity measure for rich data domains, such as images or text\ndocuments. This raises the necessity for fast nearest neighbor search with\nrespect to this distance, a problem that poses a substantial computational\nbottleneck for various tasks on massive datasets.\nIn this work, we study fast tree-based approximation algorithms for searching\nnearest neighbors w.r.t. the Wasserstein-1 distance. A standard tree-based\ntechnique, known as Quadtree, has been previously shown to obtain good results.\nWe introduce a variant of this algorithm, called Flowtree, and formally prove\nit achieves asymptotically better accuracy. Our extensive experiments, on\nreal-world text and image datasets, show that Flowtree improves over various\nbaselines and existing methods in either running time or accuracy. In\nparticular, its quality of approximation is in line with previous high-accuracy\nmethods, while its running time is much faster.",
        "arxiv_firstAuthor": "Arturs Backurs",
        "arxiv_title": "Scalable Nearest Neighbor Search for Optimal Transport",
        "arxiv_num": "1910.04126",
        "arxiv_published": "2019-10-09T17:12:41Z",
        "arxiv_updated": "2020-02-14T14:54:37Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1906_04341_what_does_bert_loo",
        "tag": [
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/what_s_encoded_by_a_nn",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bertology",
            "http://www.semanlink.net/tag/deep_learning_attention",
            "http://www.semanlink.net/tag/chris_manning"
        ],
        "comment": "",
        "title": "[1906.04341] What Does BERT Look At? An Analysis of BERT's Attention",
        "relatedDoc": [],
        "creationTime": "2019-06-21T21:49:32Z",
        "creationDate": "2019-06-21",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.04341"
        ],
        "arxiv_author": [
            "Christopher D. Manning",
            "Omer Levy",
            "Urvashi Khandelwal",
            "Kevin Clark"
        ],
        "arxiv_summary": "Large pre-trained neural networks such as BERT have had great recent success\nin NLP, motivating a growing body of research investigating what aspects of\nlanguage they are able to learn from unlabeled data. Most recent analysis has\nfocused on model outputs (e.g., language model surprisal) or internal vector\nrepresentations (e.g., probing classifiers). Complementary to these works, we\npropose methods for analyzing the attention mechanisms of pre-trained models\nand apply them to BERT. BERT's attention heads exhibit patterns such as\nattending to delimiter tokens, specific positional offsets, or broadly\nattending over the whole sentence, with heads in the same layer often\nexhibiting similar behaviors. We further show that certain attention heads\ncorrespond well to linguistic notions of syntax and coreference. For example,\nwe find heads that attend to the direct objects of verbs, determiners of nouns,\nobjects of prepositions, and coreferent mentions with remarkably high accuracy.\nLastly, we propose an attention-based probing classifier and use it to further\ndemonstrate that substantial syntactic information is captured in BERT's\nattention.",
        "arxiv_firstAuthor": "Kevin Clark",
        "arxiv_title": "What Does BERT Look At? An Analysis of BERT's Attention",
        "arxiv_num": "1906.04341",
        "arxiv_published": "2019-06-11T01:31:41Z",
        "arxiv_updated": "2019-06-11T01:31:41Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1503.08895",
        "tag": [
            "http://www.semanlink.net/tag/memory_networks",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/deep_learning_attention",
            "http://www.semanlink.net/tag/france_is_ai_2018"
        ],
        "comment": "Neural network with a recurrent attention model over a possibly large external memory.\r\n\r\ncit\u00e9 par [#A. Bordes](/tag/antoine_bordes) \u00e0 [#ParisIsAI conf 2018](/tag/france_is_ai_2018.html)",
        "title": "[1503.08895] End-To-End Memory Networks",
        "relatedDoc": [],
        "creationTime": "2018-10-23T20:17:35Z",
        "creationDate": "2018-10-23",
        "bookmarkOf": [],
        "arxiv_author": [
            "Rob Fergus",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar",
            "Jason Weston"
        ],
        "arxiv_summary": "We introduce a neural network with a recurrent attention model over a\npossibly large external memory. The architecture is a form of Memory Network\n(Weston et al., 2015) but unlike the model in that work, it is trained\nend-to-end, and hence requires significantly less supervision during training,\nmaking it more generally applicable in realistic settings. It can also be seen\nas an extension of RNNsearch to the case where multiple computational steps\n(hops) are performed per output symbol. The flexibility of the model allows us\nto apply it to tasks as diverse as (synthetic) question answering and to\nlanguage modeling. For the former our approach is competitive with Memory\nNetworks, but with less supervision. For the latter, on the Penn TreeBank and\nText8 datasets our approach demonstrates comparable performance to RNNs and\nLSTMs. In both cases we show that the key concept of multiple computational\nhops yields improved results.",
        "arxiv_firstAuthor": "Sainbayar Sukhbaatar",
        "arxiv_title": "End-To-End Memory Networks",
        "arxiv_num": "1503.08895",
        "arxiv_published": "2015-03-31T03:05:37Z",
        "arxiv_updated": "2015-11-24T19:41:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_1503_03832_facenet_a_unified",
        "tag": [
            "http://www.semanlink.net/tag/face_recognition",
            "http://www.semanlink.net/tag/ml_google",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/siamese_network"
        ],
        "comment": "Learns a Euclidean embedding per image\r\n\r\n> Uses a deep CNN trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method.\r\n\r\n> state-of-the-art face recognition performance using only **128-bytes per face**. \r\n\r\n",
        "title": "[1503.03832] FaceNet: A Unified Embedding for Face Recognition and Clustering",
        "relatedDoc": [],
        "creationTime": "2020-01-25T01:03:31Z",
        "creationDate": "2020-01-25",
        "bookmarkOf": [
            "https://arxiv.org/abs/1503.03832"
        ],
        "arxiv_author": [
            "Dmitry Kalenichenko",
            "James Philbin",
            "Florian Schroff"
        ],
        "arxiv_summary": "Despite significant recent advances in the field of face recognition,\nimplementing face verification and recognition efficiently at scale presents\nserious challenges to current approaches. In this paper we present a system,\ncalled FaceNet, that directly learns a mapping from face images to a compact\nEuclidean space where distances directly correspond to a measure of face\nsimilarity. Once this space has been produced, tasks such as face recognition,\nverification and clustering can be easily implemented using standard techniques\nwith FaceNet embeddings as feature vectors.\nOur method uses a deep convolutional network trained to directly optimize the\nembedding itself, rather than an intermediate bottleneck layer as in previous\ndeep learning approaches. To train, we use triplets of roughly aligned matching\n/ non-matching face patches generated using a novel online triplet mining\nmethod. The benefit of our approach is much greater representational\nefficiency: we achieve state-of-the-art face recognition performance using only\n128-bytes per face.\nOn the widely used Labeled Faces in the Wild (LFW) dataset, our system\nachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves\n95.12%. Our system cuts the error rate in comparison to the best published\nresult by 30% on both datasets.\nWe also introduce the concept of harmonic embeddings, and a harmonic triplet\nloss, which describe different versions of face embeddings (produced by\ndifferent networks) that are compatible to each other and allow for direct\ncomparison between each other.",
        "arxiv_firstAuthor": "Florian Schroff",
        "arxiv_title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
        "arxiv_num": "1503.03832",
        "arxiv_published": "2015-03-12T18:10:53Z",
        "arxiv_updated": "2015-06-17T23:35:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2010_00402_from_trees_to_cont",
        "tag": [
            "http://www.semanlink.net/tag/ml_google",
            "http://www.semanlink.net/tag/ai_stanford",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/poincare_embeddings",
            "http://www.semanlink.net/tag/hierarchical_clustering"
        ],
        "comment": "> The key idea of our method, HypHC, is showing a direct correspondence from discrete trees to continuous representations (via the hyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm that maps leaf embeddings to a dendrogram), **allowing us to search the space of discrete binary trees with continuous optimization**.\r\n\r\nCites [Dasgupta: A cost function for similarity-based hierarchical clustering](https://arxiv.org/abs/1510.05043)",
        "title": "[2010.00402] From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering",
        "relatedDoc": [],
        "creationTime": "2020-10-03T14:46:20Z",
        "creationDate": "2020-10-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.00402"
        ],
        "arxiv_author": [
            "Christopher R\u00e9",
            "Ines Chami",
            "Albert Gu",
            "Vaggos Chatziafratis"
        ],
        "arxiv_summary": "Similarity-based Hierarchical Clustering (HC) is a classical unsupervised\nmachine learning algorithm that has traditionally been solved with heuristic\nalgorithms like Average-Linkage. Recently, Dasgupta reframed HC as a discrete\noptimization problem by introducing a global cost function measuring the\nquality of a given tree. In this work, we provide the first continuous\nrelaxation of Dasgupta's discrete optimization problem with provable quality\nguarantees. The key idea of our method, HypHC, is showing a direct\ncorrespondence from discrete trees to continuous representations (via the\nhyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm\nthat maps leaf embeddings to a dendrogram), allowing us to search the space of\ndiscrete binary trees with continuous optimization. Building on analogies\nbetween trees and hyperbolic space, we derive a continuous analogue for the\nnotion of lowest common ancestor, which leads to a continuous relaxation of\nDasgupta's discrete objective. We can show that after decoding, the global\nminimizer of our continuous relaxation yields a discrete tree with a (1 +\nepsilon)-factor approximation for Dasgupta's optimal tree, where epsilon can be\nmade arbitrarily small and controls optimization challenges. We experimentally\nevaluate HypHC on a variety of HC benchmarks and find that even approximate\nsolutions found with gradient descent have superior clustering quality than\nagglomerative heuristics or other gradient based algorithms. Finally, we\nhighlight the flexibility of HypHC using end-to-end training in a downstream\nclassification task.",
        "arxiv_firstAuthor": "Ines Chami",
        "arxiv_title": "From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering",
        "arxiv_num": "2010.00402",
        "arxiv_published": "2020-10-01T13:43:19Z",
        "arxiv_updated": "2020-10-01T13:43:19Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/07/2007_00849_facts_as_experts_",
        "tag": [
            "http://www.semanlink.net/tag/not_encoding_knowledge_in_language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_memory",
            "http://www.semanlink.net/tag/ai_knowledge_bases",
            "http://www.semanlink.net/tag/knowledge_graph_deep_learning",
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation",
            "http://www.semanlink.net/tag/google_research"
        ],
        "comment": "> a neural language model that includes **an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge.**... **The model can be updated without re-training by manipulating its symbolic representations**. In particular this model allows us to add new facts and overwrite existing ones.\r\n\r\n> a **neural language model which learns to access information\r\nin a symbolic knowledge graph.**\r\n\r\n> This\r\nmodel builds on the recently-proposed [Entities as\r\nExperts](doc:2020/07/2004_07202_entities_as_expert) (EaE) language model (F\u00e9vry et al., 2020),\r\nwhich extends the same transformer (Vaswani\r\net al., 2017) architecture of BERT (Devlin et al., 2019) with an additional external memory for entities.\r\n>\r\n> After training EaE, the embedding associated\r\nwith an entity will (ideally) capture information\r\nabout the textual context in which that\r\nentity appears, and by inference, the entity\u2019s semantic\r\nproperties\r\n>\r\n> we include an additional\r\nmemory called a fact memory, which encodes\r\ntriples from a symbolic KB.\r\n>\r\n> This combination results in a\r\nneural language model which learns to access information\r\nin a the symbolic knowledge graph.\r\n\r\n\r\n\r\nTODO: \r\n\r\n- read again IBM's [Span Selection Pre-training for Question Answering](doc:2019/09/_1909_04120_span_selection_pre) (\"an effort to avoid encoding general knowledge in the transformer network itself\")\r\n- compare with [[1907.05242] Large Memory Layers with Product Keys](doc:2019/07/_1907_05242_large_memory_layer)\r\n- how does it relate with [[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training](doc:2020/12/2002_08909_realm_retrieval_a)?",
        "title": "[2007.00849] Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/09/_1909_04120_span_selection_pre",
            "http://www.semanlink.net/doc/2020/07/2004_07202_entities_as_expert",
            "http://www.semanlink.net/doc/2020/12/2002_08909_realm_retrieval_a",
            "http://www.semanlink.net/doc/2019/07/_1907_05242_large_memory_layer"
        ],
        "creationTime": "2020-07-09T23:54:59Z",
        "creationDate": "2020-07-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/2007.00849"
        ],
        "arxiv_author": [
            "Pat Verga",
            "William W. Cohen",
            "Haitian Sun",
            "Livio Baldini Soares"
        ],
        "arxiv_summary": "Massive language models are the core of modern NLP modeling and have been\nshown to encode impressive amounts of commonsense and factual information.\nHowever, that knowledge exists only within the latent parameters of the model,\ninaccessible to inspection and interpretation, and even worse, factual\ninformation memorized from the training corpora is likely to become stale as\nthe world changes. Knowledge stored as parameters will also inevitably exhibit\nall of the biases inherent in the source materials. To address these problems,\nwe develop a neural language model that includes an explicit interface between\nsymbolically interpretable factual information and subsymbolic neural\nknowledge. We show that this model dramatically improves performance on two\nknowledge-intensive question-answering tasks. More interestingly, the model can\nbe updated without re-training by manipulating its symbolic representations. In\nparticular this model allows us to add new facts and overwrite existing ones in\nways that are not possible for earlier models.",
        "arxiv_firstAuthor": "Pat Verga",
        "arxiv_title": "Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge",
        "arxiv_num": "2007.00849",
        "arxiv_published": "2020-07-02T03:05:41Z",
        "arxiv_updated": "2020-07-02T03:05:41Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1810.07150",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/semantic_hashing"
        ],
        "comment": "",
        "title": "[1810.07150] Subword Semantic Hashing for Intent Classification on Small Datasets",
        "relatedDoc": [],
        "creationTime": "2018-10-22T14:23:00Z",
        "creationDate": "2018-10-22",
        "bookmarkOf": [],
        "arxiv_author": [
            "Kumar Shridhar",
            "Foteini Simistira",
            "Pedro Alonso",
            "Ayushman Dash",
            "Amit Sahu",
            "Marcus Liwicki",
            "Vinaychandran Pondenkandath",
            "Gustav Grund Pihlgren",
            "Gyorgy Kovacs"
        ],
        "arxiv_summary": "In this paper, we introduce the use of Semantic Hashing as embedding for the\ntask of Intent Classification and achieve state-of-the-art performance on three\nfrequently used benchmarks. Intent Classification on a small dataset is a\nchallenging task for data-hungry state-of-the-art Deep Learning based systems.\nSemantic Hashing is an attempt to overcome such a challenge and learn robust\ntext classification. Current word embedding based are dependent on\nvocabularies. One of the major drawbacks of such methods is out-of-vocabulary\nterms, especially when having small training datasets and using a wider\nvocabulary. This is the case in Intent Classification for chatbots, where\ntypically small datasets are extracted from internet communication. Two\nproblems arise by the use of internet communication. First, such datasets miss\na lot of terms in the vocabulary to use word embeddings efficiently. Second,\nusers frequently make spelling errors. Typically, the models for intent\nclassification are not trained with spelling errors and it is difficult to\nthink about ways in which users will make mistakes. Models depending on a word\nvocabulary will always face such issues. An ideal classifier should handle\nspelling errors inherently. With Semantic Hashing, we overcome these challenges\nand achieve state-of-the-art results on three datasets: AskUbuntu, Chatbot, and\nWeb Application. Our benchmarks are available online:\nhttps://github.com/kumar-shridhar/Know-Your-Intent",
        "arxiv_firstAuthor": "Kumar Shridhar",
        "arxiv_title": "Subword Semantic Hashing for Intent Classification on Small Datasets",
        "arxiv_num": "1810.07150",
        "arxiv_published": "2018-10-16T17:25:22Z",
        "arxiv_updated": "2019-09-14T15:42:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1806.05662",
        "tag": [
            "http://www.semanlink.net/tag/yann_lecun",
            "http://www.semanlink.net/tag/transfer_learning",
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov",
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks.",
        "title": "[1806.05662] GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations",
        "relatedDoc": [],
        "creationTime": "2018-06-23T00:58:21Z",
        "creationDate": "2018-06-23",
        "bookmarkOf": [],
        "arxiv_author": [
            "Zhilin Yang",
            "Bhuwan Dhingra",
            "Jake Zhao",
            "Yann LeCun",
            "William W. Cohen",
            "Kaiming He",
            "Ruslan Salakhutdinov"
        ],
        "arxiv_summary": "Modern deep transfer learning approaches have mainly focused on learning\ngeneric feature vectors from one task that are transferable to other tasks,\nsuch as word embeddings in language and pretrained convolutional features in\nvision. However, these approaches usually transfer unary features and largely\nignore more structured graphical representations. This work explores the\npossibility of learning generic latent relational graphs that capture\ndependencies between pairs of data units (e.g., words or pixels) from\nlarge-scale unlabeled data and transferring the graphs to downstream tasks. Our\nproposed transfer learning framework improves performance on various tasks\nincluding question answering, natural language inference, sentiment analysis,\nand image classification. We also show that the learned graphs are generic\nenough to be transferred to different embeddings on which the graphs have not\nbeen trained (including GloVe embeddings, ELMo embeddings, and task-specific\nRNN hidden unit), or embedding-free units such as image pixels.",
        "arxiv_firstAuthor": "Zhilin Yang",
        "arxiv_title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations",
        "arxiv_num": "1806.05662",
        "arxiv_published": "2018-06-14T17:41:19Z",
        "arxiv_updated": "2018-07-02T20:24:33Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1810.04805",
        "tag": [
            "http://www.semanlink.net/tag/language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_machine_translation",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/sequence_to_sequence_learning",
            "http://www.semanlink.net/tag/bert"
        ],
        "comment": "",
        "title": "[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "relatedDoc": [],
        "creationTime": "2018-10-12T14:36:01Z",
        "creationDate": "2018-10-12",
        "bookmarkOf": [],
        "arxiv_author": [
            "Kenton Lee",
            "Kristina Toutanova",
            "Ming-Wei Chang",
            "Jacob Devlin"
        ],
        "arxiv_summary": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\nBERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
        "arxiv_firstAuthor": "Jacob Devlin",
        "arxiv_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "arxiv_num": "1810.04805",
        "arxiv_published": "2018-10-11T00:50:01Z",
        "arxiv_updated": "2019-05-24T20:37:26Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1802.04865",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/out_of_distribution_detection"
        ],
        "comment": "",
        "title": "[1802.04865] Learning Confidence for Out-of-Distribution Detection in Neural Networks",
        "relatedDoc": [],
        "creationTime": "2018-08-27T00:13:24Z",
        "creationDate": "2018-08-27",
        "bookmarkOf": [],
        "arxiv_author": [
            "Graham W. Taylor",
            "Terrance DeVries"
        ],
        "arxiv_summary": "Modern neural networks are very powerful predictive models, but they are\noften incapable of recognizing when their predictions may be wrong. Closely\nrelated to this is the task of out-of-distribution detection, where a network\nmust determine whether or not an input is outside of the set on which it is\nexpected to safely perform. To jointly address these issues, we propose a\nmethod of learning confidence estimates for neural networks that is simple to\nimplement and produces intuitively interpretable outputs. We demonstrate that\non the task of out-of-distribution detection, our technique surpasses recently\nproposed techniques which construct confidence based on the network's output\ndistribution, without requiring any additional labels or access to\nout-of-distribution examples. Additionally, we address the problem of\ncalibrating out-of-distribution detectors, where we demonstrate that\nmisclassified in-distribution examples can be used as a proxy for\nout-of-distribution examples.",
        "arxiv_firstAuthor": "Terrance DeVries",
        "arxiv_title": "Learning Confidence for Out-of-Distribution Detection in Neural Networks",
        "arxiv_num": "1802.04865",
        "arxiv_published": "2018-02-13T21:31:36Z",
        "arxiv_updated": "2018-02-13T21:31:36Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/10/_1011_4088_an_introduction_to_",
        "tag": [
            "http://www.semanlink.net/tag/conditional_random_field",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/andrew_mccallum"
        ],
        "comment": "",
        "title": "[1011.4088] An Introduction to Conditional Random Fields",
        "relatedDoc": [],
        "creationTime": "2019-10-13T23:51:20Z",
        "creationDate": "2019-10-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/1011.4088"
        ],
        "arxiv_author": [
            "Charles Sutton",
            "Andrew McCallum"
        ],
        "arxiv_summary": "Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields.",
        "arxiv_firstAuthor": "Charles Sutton",
        "arxiv_title": "An Introduction to Conditional Random Fields",
        "arxiv_num": "1011.4088",
        "arxiv_published": "2010-11-17T22:14:50Z",
        "arxiv_updated": "2010-11-17T22:14:50Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/1911_09419_learning_hierarchy",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/hierarchy_aware_knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/discute_avec_raphael"
        ],
        "comment": "Models semantic hierarchies by mapping entities into the polar coordinate system\r\n> Specifically,\r\nthe radial coordinate aims to model entities at different levels\r\nof the hierarchy... the angular coordinate aims to distinguish\r\nentities at the same level of the hierarchy, and these entities\r\nare expected to have roughly the same radii but different\r\nangles.",
        "title": "[1911.09419] Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction",
        "relatedDoc": [],
        "creationTime": "2021-05-17T15:11:47Z",
        "creationDate": "2021-05-17",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.09419"
        ],
        "arxiv_author": [
            "Yongdong Zhang",
            "Zhanqiu Zhang",
            "Jianyu Cai",
            "Jie Wang"
        ],
        "arxiv_summary": "Knowledge graph embedding, which aims to represent entities and relations as\nlow dimensional vectors (or matrices, tensors, etc.), has been shown to be a\npowerful technique for predicting missing links in knowledge graphs. Existing\nknowledge graph embedding models mainly focus on modeling relation patterns\nsuch as symmetry/antisymmetry, inversion, and composition. However, many\nexisting approaches fail to model semantic hierarchies, which are common in\nreal-world applications. To address this challenge, we propose a novel\nknowledge graph embedding model---namely, Hierarchy-Aware Knowledge Graph\nEmbedding (HAKE)---which maps entities into the polar coordinate system. HAKE\nis inspired by the fact that concentric circles in the polar coordinate system\ncan naturally reflect the hierarchy. Specifically, the radial coordinate aims\nto model entities at different levels of the hierarchy, and entities with\nsmaller radii are expected to be at higher levels; the angular coordinate aims\nto distinguish entities at the same level of the hierarchy, and these entities\nare expected to have roughly the same radii but different angles. Experiments\ndemonstrate that HAKE can effectively model the semantic hierarchies in\nknowledge graphs, and significantly outperforms existing state-of-the-art\nmethods on benchmark datasets for the link prediction task.",
        "arxiv_firstAuthor": "Zhanqiu Zhang",
        "arxiv_title": "Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction",
        "arxiv_num": "1911.09419",
        "arxiv_published": "2019-11-21T11:37:18Z",
        "arxiv_updated": "2019-12-25T12:31:40Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_2003_08271_pre_trained_models",
        "tag": [
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2003.08271] Pre-trained Models for Natural Language Processing: A Survey",
        "relatedDoc": [],
        "creationTime": "2020-03-19T13:34:50Z",
        "creationDate": "2020-03-19",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.08271"
        ],
        "arxiv_author": [
            "Xipeng Qiu",
            "Ning Dai",
            "Yige Xu",
            "Xuanjing Huang",
            "Tianxiang Sun",
            "Yunfan Shao"
        ],
        "arxiv_summary": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.",
        "arxiv_firstAuthor": "Xipeng Qiu",
        "arxiv_title": "Pre-trained Models for Natural Language Processing: A Survey",
        "arxiv_num": "2003.08271",
        "arxiv_published": "2020-03-18T15:22:51Z",
        "arxiv_updated": "2020-03-24T10:32:40Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1908_01580_the_hsic_bottlenec",
        "tag": [
            "http://www.semanlink.net/tag/information_bottleneck_method",
            "http://www.semanlink.net/tag/information_theory_and_deep_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neuroscience_and_ai",
            "http://www.semanlink.net/tag/backpropagation",
            "http://www.semanlink.net/tag/backpropagation_vs_biology"
        ],
        "comment": "> we show that it is possible to learn classification tasks at near competitive accuracy **without\r\nbackpropagation**, by **maximizing a surrogate of the mutual information between hidden representations and labels** and\r\nsimultaneously **minimizing the mutual dependency between hidden representations and the inputs**...\r\nthe hidden units of a network trained in this way form useful representations. Specifically, fully competitive accuracy\r\ncan be obtained by freezing the network trained without backpropagation and appending and training a one-layer\r\nnetwork using conventional SGD to convert convert the representation to the desired format.\r\n\r\nThe training method uses an approximation of the [#information bottleneck](/tag/information_bottleneck_method).\r\n\r\nAdvantages:\r\n\r\n> - The method facilitates parallel processing and requires significantly less operations. \r\n> - It does not suffer from exploding or vanishing gradients.\r\n> - It is biologically more plausible than Backpropagation\r\n\r\n",
        "title": "[1908.01580] The HSIC Bottleneck: Deep Learning without Back-Propagation",
        "relatedDoc": [],
        "creationTime": "2019-08-15T17:13:21Z",
        "creationDate": "2019-08-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/1908.01580"
        ],
        "arxiv_author": [
            "W. Bastiaan Kleijn",
            "Wan-Duo Kurt Ma",
            "J. P. Lewis"
        ],
        "arxiv_summary": "We introduce the HSIC (Hilbert-Schmidt independence criterion) bottleneck for\ntraining deep neural networks. The HSIC bottleneck is an alternative to the\nconventional cross-entropy loss and backpropagation that has a number of\ndistinct advantages. It mitigates exploding and vanishing gradients, resulting\nin the ability to learn very deep networks without skip connections. There is\nno requirement for symmetric feedback or update locking. We find that the HSIC\nbottleneck provides performance on MNIST/FashionMNIST/CIFAR10 classification\ncomparable to backpropagation with a cross-entropy target, even when the system\nis not encouraged to make the output resemble the classification labels.\nAppending a single layer trained with SGD (without backpropagation) to reformat\nthe information further improves performance.",
        "arxiv_firstAuthor": "Wan-Duo Kurt Ma",
        "arxiv_title": "The HSIC Bottleneck: Deep Learning without Back-Propagation",
        "arxiv_num": "1908.01580",
        "arxiv_published": "2019-08-05T12:23:24Z",
        "arxiv_updated": "2019-12-05T09:24:24Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1810_10531_a_mathematical_the",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge"
        ],
        "comment": "> a fundamental conceptual question: what are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences?\r\n",
        "title": "[1810.10531] A mathematical theory of semantic development in deep neural networks",
        "relatedDoc": [],
        "creationTime": "2019-06-29T15:22:55Z",
        "creationDate": "2019-06-29",
        "bookmarkOf": [
            "https://arxiv.org/abs/1810.10531"
        ],
        "arxiv_author": [
            "Andrew M. Saxe",
            "James L. McClelland",
            "Surya Ganguli"
        ],
        "arxiv_summary": "An extensive body of empirical research has revealed remarkable regularities\nin the acquisition, organization, deployment, and neural representation of\nhuman semantic knowledge, thereby raising a fundamental conceptual question:\nwhat are the theoretical principles governing the ability of neural networks to\nacquire, organize, and deploy abstract knowledge by integrating across many\nindividual experiences? We address this question by mathematically analyzing\nthe nonlinear dynamics of learning in deep linear networks. We find exact\nsolutions to this learning dynamics that yield a conceptual explanation for the\nprevalence of many disparate phenomena in semantic cognition, including the\nhierarchical differentiation of concepts through rapid developmental\ntransitions, the ubiquity of semantic illusions between such transitions, the\nemergence of item typicality and category coherence as factors controlling the\nspeed of semantic processing, changing patterns of inductive projection over\ndevelopment, and the conservation of semantic similarity in neural\nrepresentations across species. Thus, surprisingly, our simple neural model\nqualitatively recapitulates many diverse regularities underlying semantic\ndevelopment, while providing analytic insight into how the statistical\nstructure of an environment can interact with nonlinear deep learning dynamics\nto give rise to these regularities.",
        "arxiv_firstAuthor": "Andrew M. Saxe",
        "arxiv_title": "A mathematical theory of semantic development in deep neural networks",
        "arxiv_num": "1810.10531",
        "arxiv_published": "2018-10-23T22:20:27Z",
        "arxiv_updated": "2018-10-23T22:20:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_2003_00330_graph_neural_netwo",
        "tag": [
            "http://www.semanlink.net/tag/neural_symbolic_computing",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/graph_neural_networks",
            "http://www.semanlink.net/tag/survey"
        ],
        "comment": "reviews the state-of-the-art on the use of GNNs as a model of neural-symbolic computing.",
        "title": "[2003.00330] Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective",
        "relatedDoc": [],
        "creationTime": "2020-03-15T10:39:59Z",
        "creationDate": "2020-03-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.00330"
        ],
        "arxiv_author": [
            "Artur Garcez",
            "Marco Gori",
            "Luis Lamb",
            "Marcelo Prates",
            "Pedro Avelar",
            "Moshe Vardi"
        ],
        "arxiv_summary": "Neural-symbolic computing has now become the subject of interest of both\nacademic and industry research laboratories. Graph Neural Networks (GNN) have\nbeen widely used in relational and symbolic domains, with widespread\napplication of GNNs in combinatorial optimization, constraint satisfaction,\nrelational reasoning and other scientific domains. The need for improved\nexplainability, interpretability and trust of AI systems in general demands\nprincipled methodologies, as suggested by neural-symbolic computing. In this\npaper, we review the state-of-the-art on the use of GNNs as a model of\nneural-symbolic computing. This includes the application of GNNs in several\ndomains as well as its relationship to current developments in neural-symbolic\ncomputing.",
        "arxiv_firstAuthor": "Luis Lamb",
        "arxiv_title": "Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective",
        "arxiv_num": "2003.00330",
        "arxiv_published": "2020-02-29T18:55:13Z",
        "arxiv_updated": "2020-03-11T20:33:01Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1801.06146",
        "tag": [
            "http://www.semanlink.net/tag/jeremy_howard",
            "http://www.semanlink.net/tag/catastrophic_forgetting",
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/sebastian_ruder",
            "http://www.semanlink.net/tag/transfer_learning_in_nlp",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/transfer_learning",
            "http://www.semanlink.net/tag/ulmfit",
            "http://www.semanlink.net/tag/nlp_text_classification"
        ],
        "comment": "code is available in the fastai lib\r\n\r\n[blog post](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)\r\n\r\n[see also](/doc/?uri=https%3A%2F%2Fyashuseth.blog%2F2018%2F06%2F17%2Funderstanding-universal-language-model-fine-tuning-ulmfit%2F)\r\n\r\n\r\n\r\n\r\n\r\n",
        "title": "[1801.06146] Universal Language Model Fine-tuning for Text Classification",
        "relatedDoc": [
            "https://yashuseth.blog/2018/06/17/understanding-universal-language-model-fine-tuning-ulmfit/"
        ],
        "creationTime": "2018-01-19T11:31:32Z",
        "creationDate": "2018-01-19",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jeremy Howard",
            "Sebastian Ruder"
        ],
        "arxiv_summary": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.",
        "arxiv_firstAuthor": "Jeremy Howard",
        "arxiv_title": "Universal Language Model Fine-tuning for Text Classification",
        "arxiv_num": "1801.06146",
        "arxiv_published": "2018-01-18T17:54:52Z",
        "arxiv_updated": "2018-05-23T09:23:47Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/2006_15020_pre_training_via_p",
        "tag": [
            "http://www.semanlink.net/tag/nlp_pretraining",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/zero_shot_learning",
            "http://www.semanlink.net/tag/sequence_to_sequence_learning"
        ],
        "comment": "",
        "title": "[2006.15020] Pre-training via Paraphrasing",
        "relatedDoc": [],
        "creationTime": "2020-06-30T11:32:08Z",
        "creationDate": "2020-06-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/2006.15020"
        ],
        "arxiv_author": [
            "Marjan Ghazvininejad",
            "Mike Lewis",
            "Sida Wang",
            "Gargi Ghosh",
            "Armen Aghajanyan",
            "Luke Zettlemoyer"
        ],
        "arxiv_summary": "We introduce MARGE, a pre-trained sequence-to-sequence model learned with an\nunsupervised multi-lingual multi-document paraphrasing objective. MARGE\nprovides an alternative to the dominant masked language modeling paradigm,\nwhere we self-supervise the reconstruction of target text by retrieving a set\nof related texts (in many languages) and conditioning on them to maximize the\nlikelihood of generating the original. We show it is possible to jointly learn\nto do retrieval and reconstruction, given only a random initialization. The\nobjective noisily captures aspects of paraphrase, translation, multi-document\nsummarization, and information retrieval, allowing for strong zero-shot\nperformance on several tasks. For example, with no additional task-specific\ntraining we achieve BLEU scores of up to 35.8 for document translation. We\nfurther show that fine-tuning gives strong performance on a range of\ndiscriminative and generative tasks in many languages, making MARGE the most\ngenerally applicable pre-training method to date.",
        "arxiv_firstAuthor": "Mike Lewis",
        "arxiv_title": "Pre-training via Paraphrasing",
        "arxiv_num": "2006.15020",
        "arxiv_published": "2020-06-26T14:43:43Z",
        "arxiv_updated": "2020-06-26T14:43:43Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1905_07129_ernie_enhanced_la",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/acl_2019",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/ernie",
            "http://www.semanlink.net/tag/nlp_using_knowledge_graphs",
            "http://www.semanlink.net/tag/discute_avec_raphael"
        ],
        "comment": "> We argue that informative entities in **KGs can enhance language representation with external knowledge**. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously.\r\n\r\n> ERNIE achieves significant improvements on\r\nvarious knowledge-driven tasks, and meanwhile\r\nis comparable with the state-of-the-art\r\nmodel BERT on other common NLP tasks\r\n\r\n[GitHub](https://github.com/thunlp/ERNIE)\r\n\r\nWARNING, there is another ERNIE (by [NLP@Baidu](tag:nlp_baidu)): Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\r\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and\r\nHua Wu. 2019. Ernie: Enhanced representation through\r\nknowledge integration. This doesn't happen when you choose Fran\u00e7ois-Paul as the name for your child.",
        "title": "[1905.07129] ERNIE: Enhanced Language Representation with Informative Entities",
        "relatedDoc": [],
        "creationTime": "2019-08-05T15:40:17Z",
        "creationDate": "2019-08-05",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.07129"
        ],
        "arxiv_author": [
            "Zhengyan Zhang",
            "Qun Liu",
            "Maosong Sun",
            "Xin Jiang",
            "Xu Han",
            "Zhiyuan Liu"
        ],
        "arxiv_summary": "Neural language representation models such as BERT pre-trained on large-scale\ncorpora can well capture rich semantic patterns from plain text, and be\nfine-tuned to consistently improve the performance of various NLP tasks.\nHowever, the existing pre-trained language models rarely consider incorporating\nknowledge graphs (KGs), which can provide rich structured knowledge facts for\nbetter language understanding. We argue that informative entities in KGs can\nenhance language representation with external knowledge. In this paper, we\nutilize both large-scale textual corpora and KGs to train an enhanced language\nrepresentation model (ERNIE), which can take full advantage of lexical,\nsyntactic, and knowledge information simultaneously. The experimental results\nhave demonstrated that ERNIE achieves significant improvements on various\nknowledge-driven tasks, and meanwhile is comparable with the state-of-the-art\nmodel BERT on other common NLP tasks. The source code of this paper can be\nobtained from https://github.com/thunlp/ERNIE.",
        "arxiv_firstAuthor": "Zhengyan Zhang",
        "arxiv_title": "ERNIE: Enhanced Language Representation with Informative Entities",
        "arxiv_num": "1905.07129",
        "arxiv_published": "2019-05-17T06:24:16Z",
        "arxiv_updated": "2019-06-04T11:35:58Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1805.04032",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/sense_embeddings"
        ],
        "comment": "Survey focused on semantic representation of meaning (methods that try to directly model individual meanings of words).\r\n\r\nPb with word embeddings: the meaning conflation deficiency (representing a word with all its possible meanings as a single vector). Can be addressed by a method for modelling unambiguous lexical meaning.\r\n\r\ntwo main branches of sense representation :\r\n\r\n- unsupervised \r\n- knowledge-based",
        "title": "[1805.04032] From Word to Sense Embeddings: A Survey on Vector Representations of Meaning",
        "relatedDoc": [],
        "creationTime": "2018-05-30T23:44:56Z",
        "creationDate": "2018-05-30",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jose Camacho-Collados",
            "Mohammad Taher Pilehvar"
        ],
        "arxiv_summary": "Over the past years, distributed semantic representations have proved to be\neffective and flexible keepers of prior knowledge to be integrated into\ndownstream applications. This survey focuses on the representation of meaning.\nWe start from the theoretical background behind word vector space models and\nhighlight one of their major limitations: the meaning conflation deficiency,\nwhich arises from representing a word with all its possible meanings as a\nsingle vector. Then, we explain how this deficiency can be addressed through a\ntransition from the word level to the more fine-grained level of word senses\n(in its broader acceptation) as a method for modelling unambiguous lexical\nmeaning. We present a comprehensive overview of the wide range of techniques in\nthe two main branches of sense representation, i.e., unsupervised and\nknowledge-based. Finally, this survey covers the main evaluation procedures and\napplications for this type of representation, and provides an analysis of four\nof its important aspects: interpretability, sense granularity, adaptability to\ndifferent domains and compositionality.",
        "arxiv_firstAuthor": "Jose Camacho-Collados",
        "arxiv_title": "From Word to Sense Embeddings: A Survey on Vector Representations of Meaning",
        "arxiv_num": "1805.04032",
        "arxiv_published": "2018-05-10T15:56:48Z",
        "arxiv_updated": "2018-10-26T09:34:36Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/1909_10506_learning_dense_rep",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/entity_discovery_and_linking"
        ],
        "comment": "> We show that it is feasible to perform **entity\r\nlinking by training a dual encoder (two-tower)\r\nmodel that encodes mentions and entities in\r\nthe same dense vector space**, where candidate\r\nentities are retrieved by approximate nearest\r\nneighbor search. Unlike prior work, **this setup\r\ndoes not rely on an alias table followed by a\r\nre-ranker, and is thus the first fully learned entity\r\nretrieval model**.\r\n\r\nContributions:\r\n\r\n> -  a dual encoder architecture for\r\nlearning entity and mention encodings suitable for\r\nretrieval. A key feature of the architecture is that it\r\nemploys a modular **hierarchy of sub-encoders that\r\ncapture different aspects of mentions and entities**\r\n> - a simple, fully unsupervised **hard negative\r\nmining** strategy that produces massive gains\r\nin retrieval performance, compared to using only\r\nrandom negatives\r\n> - high\r\nquality candidate entities very efficiently using approximate nearest neighbor search\r\n> - outperforms discrete retrieval\r\nbaselines like an alias table or BM25\r\n\r\n> strong retrieval\r\nperformance across all 5.7 million Wikipedia entities in\r\naround 3ms per mention\r\n\r\n> since we are using a two-tower or dual\r\nencoder architecture, **our model cannot use any kind of attention over\r\nboth mentions and entities at once**, nor feature-wise\r\ncomparisons as done by Francis-Landau et al. (2016).\r\nThis is a fairly severe constraint \u2013 for example, **we cannot\r\ndirectly compare the mention span to the entity title**\r\n\u2013 but it permits retrieval with nearest neighbor search\r\nfor the entire context against a single, all encompassing\r\nrepresentation for each entity",
        "title": "[1909.10506] Learning Dense Representations for Entity Retrieval",
        "relatedDoc": [],
        "creationTime": "2021-05-01T09:11:15Z",
        "creationDate": "2021-05-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.10506"
        ],
        "arxiv_author": [
            "Eugene Ie",
            "Diego Garcia-Olano",
            "Larry Lansing",
            "Alessandro Presta",
            "Sayali Kulkarni",
            "Jason Baldridge",
            "Daniel Gillick"
        ],
        "arxiv_summary": "We show that it is feasible to perform entity linking by training a dual\nencoder (two-tower) model that encodes mentions and entities in the same dense\nvector space, where candidate entities are retrieved by approximate nearest\nneighbor search. Unlike prior work, this setup does not rely on an alias table\nfollowed by a re-ranker, and is thus the first fully learned entity retrieval\nmodel. We show that our dual encoder, trained using only anchor-text links in\nWikipedia, outperforms discrete alias table and BM25 baselines, and is\ncompetitive with the best comparable results on the standard TACKBP-2010\ndataset. In addition, it can retrieve candidates extremely fast, and\ngeneralizes well to a new dataset derived from Wikinews. On the modeling side,\nwe demonstrate the dramatic value of an unsupervised negative mining algorithm\nfor this task.",
        "arxiv_firstAuthor": "Daniel Gillick",
        "arxiv_title": "Learning Dense Representations for Entity Retrieval",
        "arxiv_num": "1909.10506",
        "arxiv_published": "2019-09-23T17:52:34Z",
        "arxiv_updated": "2019-09-23T17:52:34Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/09/_1909_03186_on_extractive_and_",
        "tag": [
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/automatic_summarization",
            "http://www.semanlink.net/tag/rigolo",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.",
        "title": "[1909.03186] On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
        "relatedDoc": [],
        "creationTime": "2019-09-11T18:15:42Z",
        "creationDate": "2019-09-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.03186"
        ],
        "arxiv_author": [
            "Christopher Pal",
            "Jonathan Pilault",
            "Raymond Li",
            "Sandeep Subramanian"
        ],
        "arxiv_summary": "We present a method to produce abstractive summaries of long documents that\nexceed several thousand words via neural abstractive summarization. We perform\na simple extractive step before generating a summary, which is then used to\ncondition the transformer language model on relevant information before being\ntasked with generating a summary. We show that this extractive step\nsignificantly improves summarization results. We also show that this approach\nproduces more abstractive summaries compared to prior work that employs a copy\nmechanism while still achieving higher rouge scores. Note: The abstract above\nwas not written by the authors, it was generated by one of the models presented\nin this paper.",
        "arxiv_firstAuthor": "Sandeep Subramanian",
        "arxiv_title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
        "arxiv_num": "1909.03186",
        "arxiv_published": "2019-09-07T04:33:26Z",
        "arxiv_updated": "2019-09-07T04:33:26Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1804.01486",
        "tag": [
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/combining_word_and_entity_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/medical_data"
        ],
        "comment": "",
        "title": "[1804.01486] Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data",
        "relatedDoc": [],
        "creationTime": "2018-04-14T11:10:40Z",
        "creationDate": "2018-04-14",
        "bookmarkOf": [],
        "arxiv_author": [
            "Benjamin Kompa",
            "Griffin Weber",
            "Nathan P. Palmer",
            "Isaac S. Kohane",
            "Allen Schmaltz",
            "Inbar Fried",
            "Andrew L. Beam",
            "Tianxi Cai",
            "Xu Shi"
        ],
        "arxiv_summary": "Word embeddings are a popular approach to unsupervised learning of word\nrelationships that are widely used in natural language processing. In this\narticle, we present a new set of embeddings for medical concepts learned using\nan extremely large collection of multimodal medical data. Leaning on recent\ntheoretical insights, we demonstrate how an insurance claims database of 60\nmillion members, a collection of 20 million clinical notes, and 1.7 million\nfull text biomedical journal articles can be combined to embed concepts into a\ncommon space, resulting in the largest ever set of embeddings for 108,477\nmedical concepts. To evaluate our approach, we present a new benchmark\nmethodology based on statistical power specifically designed to test embeddings\nof medical concepts. Our approach, called cui2vec, attains state-of-the-art\nperformance relative to previous methods in most instances. Finally, we provide\na downloadable set of pre-trained embeddings for other researchers to use, as\nwell as an online tool for interactive exploration of the cui2vec embeddings",
        "arxiv_firstAuthor": "Andrew L. Beam",
        "arxiv_title": "Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data",
        "arxiv_num": "1804.01486",
        "arxiv_published": "2018-04-04T16:02:54Z",
        "arxiv_updated": "2019-08-20T00:32:33Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1703.02507",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/sif_embeddings",
            "http://www.semanlink.net/tag/sent2vec"
        ],
        "comment": "",
        "title": "[1703.02507] Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features",
        "relatedDoc": [],
        "creationTime": "2019-03-25T15:36:27Z",
        "creationDate": "2019-03-25",
        "bookmarkOf": [],
        "arxiv_author": [
            "Prakhar Gupta",
            "Martin Jaggi",
            "Matteo Pagliardini"
        ],
        "arxiv_summary": "The recent tremendous success of unsupervised word embeddings in a multitude\nof applications raises the obvious question if similar methods could be derived\nto improve embeddings (i.e. semantic representations) of word sequences as\nwell. We present a simple but efficient unsupervised objective to train\ndistributed representations of sentences. Our method outperforms the\nstate-of-the-art unsupervised models on most benchmark tasks, highlighting the\nrobustness of the produced general-purpose sentence embeddings.",
        "arxiv_firstAuthor": "Matteo Pagliardini",
        "arxiv_title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features",
        "arxiv_num": "1703.02507",
        "arxiv_published": "2017-03-07T18:19:11Z",
        "arxiv_updated": "2018-12-28T15:12:58Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1809.01797",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_base",
            "http://www.semanlink.net/tag/natural_language_generation",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1809.01797] Describing a Knowledge Base",
        "relatedDoc": [],
        "creationTime": "2018-09-07T12:57:23Z",
        "creationDate": "2018-09-07",
        "bookmarkOf": [],
        "arxiv_author": [
            "Heng Ji",
            "Kevin Knight",
            "Xiaoman Pan",
            "Boliang Zhang",
            "Zhiying Jiang",
            "Qingyun Wang",
            "Lifu Huang"
        ],
        "arxiv_summary": "We aim to automatically generate natural language descriptions about an input\nstructured knowledge base (KB). We build our generation framework based on a\npointer network which can copy facts from the input KB, and add two attention\nmechanisms: (i) slot-aware attention to capture the association between a slot\ntype and its corresponding slot value; and (ii) a new \\emph{table position\nself-attention} to capture the inter-dependencies among related slots. For\nevaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we\npropose a KB reconstruction based metric by extracting a KB from the generation\noutput and comparing it with the input KB. We also create a new data set which\nincludes 106,216 pairs of structured KBs and their corresponding natural\nlanguage descriptions for two distinct entity types. Experiments show that our\napproach significantly outperforms state-of-the-art methods. The reconstructed\nKB achieves 68.8% - 72.6% F-score.",
        "arxiv_firstAuthor": "Qingyun Wang",
        "arxiv_title": "Describing a Knowledge Base",
        "arxiv_num": "1809.01797",
        "arxiv_published": "2018-09-06T02:56:58Z",
        "arxiv_updated": "2018-09-30T04:36:18Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1603.08861",
        "tag": [
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/semi_supervised_learning"
        ],
        "comment": "",
        "title": "[1603.08861] Revisiting Semi-Supervised Learning with Graph Embeddings",
        "relatedDoc": [],
        "creationTime": "2018-02-13T15:38:38Z",
        "creationDate": "2018-02-13",
        "bookmarkOf": [],
        "arxiv_author": [
            "Zhilin Yang",
            "William W. Cohen",
            "Ruslan Salakhutdinov"
        ],
        "arxiv_summary": "We present a semi-supervised learning framework based on graph embeddings.\nGiven a graph between instances, we train an embedding for each instance to\njointly predict the class label and the neighborhood context in the graph. We\ndevelop both transductive and inductive variants of our method. In the\ntransductive variant of our method, the class labels are determined by both the\nlearned embeddings and input feature vectors, while in the inductive variant,\nthe embeddings are defined as a parametric function of the feature vectors, so\npredictions can be made on instances not seen during training. On a large and\ndiverse set of benchmark tasks, including text classification, distantly\nsupervised entity extraction, and entity classification, we show improved\nperformance over many of the existing models.",
        "arxiv_firstAuthor": "Zhilin Yang",
        "arxiv_title": "Revisiting Semi-Supervised Learning with Graph Embeddings",
        "arxiv_num": "1603.08861",
        "arxiv_published": "2016-03-29T17:46:16Z",
        "arxiv_updated": "2016-05-26T23:57:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_1912_08904_macaw_an_extensib",
        "tag": [
            "http://www.semanlink.net/tag/nlp_tools",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/information_retrieval",
            "http://www.semanlink.net/tag/question_answering",
            "http://www.semanlink.net/tag/chatbot"
        ],
        "comment": "",
        "title": "[1912.08904] Macaw: An Extensible Conversational Information Seeking Platform",
        "relatedDoc": [],
        "creationTime": "2020-01-01T10:55:09Z",
        "creationDate": "2020-01-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/1912.08904"
        ],
        "arxiv_author": [
            "Hamed Zamani",
            "Nick Craswell"
        ],
        "arxiv_summary": "Conversational information seeking (CIS) has been recognized as a major\nemerging research area in information retrieval. Such research will require\ndata and tools, to allow the implementation and study of conversational\nsystems. This paper introduces Macaw, an open-source framework with a modular\narchitecture for CIS research. Macaw supports multi-turn, multi-modal, and\nmixed-initiative interactions, and enables research for tasks such as document\nretrieval, question answering, recommendation, and structured data exploration.\nIt has a modular design to encourage the study of new CIS algorithms, which can\nbe evaluated in batch mode. It can also integrate with a user interface, which\nallows user studies and data collection in an interactive mode, where the back\nend can be fully algorithmic or a wizard of oz setup. Macaw is distributed\nunder the MIT License.",
        "arxiv_firstAuthor": "Hamed Zamani",
        "arxiv_title": "Macaw: An Extensible Conversational Information Seeking Platform",
        "arxiv_num": "1912.08904",
        "arxiv_published": "2019-12-18T21:51:22Z",
        "arxiv_updated": "2019-12-18T21:51:22Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1601.07752",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/algorithmes",
            "http://www.semanlink.net/tag/polynomial",
            "http://www.semanlink.net/tag/jean_paul"
        ],
        "comment": "",
        "title": "[1601.07752] Enhancing the Power of Cardinal's Algorithm",
        "relatedDoc": [],
        "creationTime": "2016-05-28T09:14:36Z",
        "creationDate": "2016-05-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Victor Y. Pan"
        ],
        "arxiv_summary": "Cardinal's factorization algorithm of 1996 splits a univariate polynomial\ninto two factors with root sets separated by the imaginary axis, which is an\nimportant goal itself and a basic step toward root-finding. The novelty of the\nalgorithm and its potential power have been well recognized by experts\nimmediately, but by 2016, that is, two decades later, its practical value still\nremains nil, particularly because of the high computational cost of performing\nits final stage by means of computing approximate greatest common divisor of\ntwo polynomials. We briefly recall Cardinal's algorithm and its difficulties,\namend it based on some works performed since 1996, extend its power to\nsplitting out factors of a more general class, and reduce the final stage of\nthe algorithm to quite manageable computations with structured matrices. Some\nof our techniques can be of independent interest for matrix computations.",
        "arxiv_firstAuthor": "Victor Y. Pan",
        "arxiv_title": "Enhancing the Power of Cardinal's Algorithm",
        "arxiv_num": "1601.07752",
        "arxiv_published": "2016-01-28T13:30:37Z",
        "arxiv_updated": "2017-04-13T15:53:50Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/11/_1911_01464_emerging_cross_lin",
        "tag": [
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/bertology",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/cross_lingual_nlp"
        ],
        "comment": "",
        "title": "[1911.01464] Emerging Cross-lingual Structure in Pretrained Language Models",
        "relatedDoc": [],
        "creationTime": "2019-11-06T13:09:03Z",
        "creationDate": "2019-11-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.01464"
        ],
        "arxiv_author": [
            "Alexis Conneau",
            "Veselin Stoyanov",
            "Shijie Wu",
            "Haoran Li",
            "Luke Zettlemoyer"
        ],
        "arxiv_summary": "We study the problem of multilingual masked language modeling, i.e. the\ntraining of a single model on concatenated text from multiple languages, and\npresent a detailed study of several factors that influence why these models are\nso effective for cross-lingual transfer. We show, contrary to what was\npreviously hypothesized, that transfer is possible even when there is no shared\nvocabulary across the monolingual corpora and also when the text comes from\nvery different domains. The only requirement is that there are some shared\nparameters in the top layers of the multi-lingual encoder. To better understand\nthis result, we also show that representations from independently trained\nmodels in different languages can be aligned post-hoc quite effectively,\nstrongly suggesting that, much like for non-contextual word embeddings, there\nare universal latent symmetries in the learned embedding spaces. For\nmultilingual masked language modeling, these symmetries seem to be\nautomatically discovered and aligned during the joint training process.",
        "arxiv_firstAuthor": "Shijie Wu",
        "arxiv_title": "Emerging Cross-lingual Structure in Pretrained Language Models",
        "arxiv_num": "1911.01464",
        "arxiv_published": "2019-11-04T19:41:13Z",
        "arxiv_updated": "2019-11-10T06:55:02Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1912_01412_deep_learning_for_",
        "tag": [
            "http://www.semanlink.net/tag/facebook_fair",
            "http://www.semanlink.net/tag/guillaume_lample",
            "http://www.semanlink.net/tag/deep_learning",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/mathematiques",
            "http://www.semanlink.net/tag/connectionist_vs_symbolic_debate",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1912.01412] Deep Learning for Symbolic Mathematics",
        "relatedDoc": [],
        "creationTime": "2019-12-09T17:11:42Z",
        "creationDate": "2019-12-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/1912.01412"
        ],
        "arxiv_author": [
            "Guillaume Lample",
            "Fran\u00e7ois Charton"
        ],
        "arxiv_summary": "Neural networks have a reputation for being better at solving statistical or\napproximate problems than at performing calculations or working with symbolic\ndata. In this paper, we show that they can be surprisingly good at more\nelaborated tasks in mathematics, such as symbolic integration and solving\ndifferential equations. We propose a syntax for representing mathematical\nproblems, and methods for generating large datasets that can be used to train\nsequence-to-sequence models. We achieve results that outperform commercial\nComputer Algebra Systems such as Matlab or Mathematica.",
        "arxiv_firstAuthor": "Guillaume Lample",
        "arxiv_title": "Deep Learning for Symbolic Mathematics",
        "arxiv_num": "1912.01412",
        "arxiv_published": "2019-12-02T15:05:24Z",
        "arxiv_updated": "2019-12-02T15:05:24Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/08/1905_06316_what_do_you_learn_",
        "tag": [
            "http://www.semanlink.net/tag/probing_ml",
            "http://www.semanlink.net/tag/contextualised_word_representations",
            "http://www.semanlink.net/tag/language_model",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
        "title": "[1905.06316] What do you learn from context? Probing for sentence structure in contextualized word representations",
        "relatedDoc": [],
        "creationTime": "2020-08-02T11:25:38Z",
        "creationDate": "2020-08-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.06316"
        ],
        "arxiv_author": [
            "R Thomas McCoy",
            "Najoung Kim",
            "Alex Wang",
            "Ian Tenney",
            "Berlin Chen",
            "Dipanjan Das",
            "Benjamin Van Durme",
            "Ellie Pavlick",
            "Patrick Xia",
            "Samuel R. Bowman",
            "Adam Poliak"
        ],
        "arxiv_summary": "Contextualized representation models such as ELMo (Peters et al., 2018a) and\nBERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a\ndiverse array of downstream NLP tasks. Building on recent token-level probing\nwork, we introduce a novel edge probing task design and construct a broad suite\nof sub-sentence tasks derived from the traditional structured NLP pipeline. We\nprobe word-level contextual representations from four recent models and\ninvestigate how they encode sentence structure across a range of syntactic,\nsemantic, local, and long-range phenomena. We find that existing models trained\non language modeling and translation produce strong representations for\nsyntactic phenomena, but only offer comparably small improvements on semantic\ntasks over a non-contextual baseline.",
        "arxiv_firstAuthor": "Ian Tenney",
        "arxiv_title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
        "arxiv_num": "1905.06316",
        "arxiv_published": "2019-05-15T17:48:56Z",
        "arxiv_updated": "2019-05-15T17:48:56Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/07/_1907_03950_learning_by_abstra",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/chris_manning",
            "http://www.semanlink.net/tag/consciousness_prior",
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation"
        ],
        "comment": "> Given an image, we first predict a probabilistic graph\r\nthat represents its underlying semantics and serves as a structured world model.\r\nThen, we perform sequential reasoning over the graph, iteratively traversing its\r\nnodes to answer a given question or draw a new inference. In contrast to most\r\nneural architectures that are designed to closely interact with the raw sensory\r\ndata, our model operates instead in an abstract latent space, by transforming both\r\nthe visual and linguistic modalities into semantic concept-based representations,\r\nthereby achieving enhanced transparency and modularity.\r\n\r\n> Drawing inspiration from [Bengio\u2019s consciousness prior](/doc/?uri=https%3A%2F%2Farxiv.org%2Fabs%2F1709.08568)...",
        "title": "[1907.03950] Learning by Abstraction: The Neural State Machine",
        "relatedDoc": [
            "https://arxiv.org/abs/1709.08568"
        ],
        "creationTime": "2019-07-10T22:05:52Z",
        "creationDate": "2019-07-10",
        "bookmarkOf": [
            "https://arxiv.org/abs/1907.03950"
        ],
        "arxiv_author": [
            "Christopher D. Manning",
            "Drew A. Hudson"
        ],
        "arxiv_summary": "We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model's strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.",
        "arxiv_firstAuthor": "Drew A. Hudson",
        "arxiv_title": "Learning by Abstraction: The Neural State Machine",
        "arxiv_num": "1907.03950",
        "arxiv_published": "2019-07-09T03:08:41Z",
        "arxiv_updated": "2019-11-25T10:02:05Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1803.01271",
        "tag": [
            "http://www.semanlink.net/tag/sequence_modeling_cnn_vs_rnn",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks",
        "title": "[1803.01271] An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
        "relatedDoc": [],
        "creationTime": "2018-08-05T10:43:56Z",
        "creationDate": "2018-08-05",
        "bookmarkOf": [],
        "arxiv_author": [
            "Vladlen Koltun",
            "Shaojie Bai",
            "J. Zico Kolter"
        ],
        "arxiv_summary": "For most deep learning practitioners, sequence modeling is synonymous with\nrecurrent networks. Yet recent results indicate that convolutional\narchitectures can outperform recurrent networks on tasks such as audio\nsynthesis and machine translation. Given a new sequence modeling task or\ndataset, which architecture should one use? We conduct a systematic evaluation\nof generic convolutional and recurrent architectures for sequence modeling. The\nmodels are evaluated across a broad range of standard tasks that are commonly\nused to benchmark recurrent networks. Our results indicate that a simple\nconvolutional architecture outperforms canonical recurrent networks such as\nLSTMs across a diverse range of tasks and datasets, while demonstrating longer\neffective memory. We conclude that the common association between sequence\nmodeling and recurrent networks should be reconsidered, and convolutional\nnetworks should be regarded as a natural starting point for sequence modeling\ntasks. To assist related work, we have made code available at\nhttp://github.com/locuslab/TCN .",
        "arxiv_firstAuthor": "Shaojie Bai",
        "arxiv_title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
        "arxiv_num": "1803.01271",
        "arxiv_published": "2018-03-04T00:20:29Z",
        "arxiv_updated": "2018-04-19T14:32:38Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/12/2012_04584_distilling_knowled",
        "tag": [
            "http://www.semanlink.net/tag/question_answering",
            "http://www.semanlink.net/tag/nlp_ens",
            "http://www.semanlink.net/tag/open_domain_question_answering",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/knowledge_augmented_language_models",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_models_for_information_retrieval"
        ],
        "comment": "> a method to train an information retrieval module for downstream tasks, **without using pairs of queries and documents as annotations**.\r\n\r\nUses two models (standard pipeline for open-domain QA):\r\n\r\n- the first one retrieves documents from a large source of knowledge (the retriever)\r\n- the second one processes the support documents to solve the task (the reader).\r\n\r\n> First the retriever selects support passages in a large knowledge\r\nsource. Then these passages are processed by the reader, along with the question, to generate an\r\nanswer\r\n\r\nInspired by knowledge distillation: the reader model is the teacher and the retriever is the student.\r\n\r\n> More precisely, we use a sequence-to-sequence model as the reader, and use\r\nthe attention activations over the input documents as synthetic labels to train the retriever. \r\n> (**train the retriever by learning to approximate the attention score of the reader**)\r\n\r\nRefers to:\r\n\r\n- [REALM: Retrieval-Augmented Language Model Pre-Training](doc:2020/12/2002_08909_realm_retrieval_a)\r\n- [Dehghani: Neural Ranking Models with Weak Supervision](doc:?uri=https%3A%2F%2Farxiv.org%2Fabs%2F1704.08803)",
        "title": "[2012.04584] Distilling Knowledge from Reader to Retriever for Question Answering",
        "relatedDoc": [
            "https://arxiv.org/abs/1704.08803",
            "http://www.semanlink.net/doc/2020/12/2002_08909_realm_retrieval_a"
        ],
        "creationTime": "2020-12-11T16:48:13Z",
        "creationDate": "2020-12-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2012.04584"
        ],
        "arxiv_author": [
            "Edouard Grave",
            "Gautier Izacard"
        ],
        "arxiv_summary": "The task of information retrieval is an important component of many natural\nlanguage processing systems, such as open domain question answering. While\ntraditional methods were based on hand-crafted features, continuous\nrepresentations based on neural networks recently obtained competitive results.\nA challenge of using such methods is to obtain supervised data to train the\nretriever model, corresponding to pairs of query and support documents. In this\npaper, we propose a technique to learn retriever models for downstream tasks,\ninspired by knowledge distillation, and which does not require annotated pairs\nof query and documents. Our approach leverages attention scores of a reader\nmodel, used to solve the task based on retrieved documents, to obtain synthetic\nlabels for the retriever. We evaluate our method on question answering,\nobtaining state-of-the-art results.",
        "arxiv_firstAuthor": "Gautier Izacard",
        "arxiv_title": "Distilling Knowledge from Reader to Retriever for Question Answering",
        "arxiv_num": "2012.04584",
        "arxiv_published": "2020-12-08T17:36:34Z",
        "arxiv_updated": "2020-12-08T17:36:34Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1807_08447_linknbed_multi_gr",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/ai_amazon",
            "http://www.semanlink.net/tag/combining_knowledge_graphs",
            "http://www.semanlink.net/tag/kd_mkb_biblio"
        ],
        "comment": " > a deep relational learning framework that **learns entity and relationship representations across multiple graphs**. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure\r\n>\r\n> We posit that **combining\r\ngraph alignment task with deep representation\r\nlearning across multi-relational graphs** has potential\r\nto induce a synergistic effect on both tasks",
        "title": "[1807.08447] LinkNBed: Multi-Graph Representation Learning with Entity Linkage",
        "relatedDoc": [],
        "creationTime": "2020-05-11T22:30:47Z",
        "creationDate": "2020-05-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1807.08447"
        ],
        "arxiv_author": [
            "Jun Ma",
            "Bunyamin Sisman",
            "Christos Faloutsos",
            "Rakshit Trivedi",
            "Xin Luna Dong",
            "Hongyuan Zha"
        ],
        "arxiv_summary": "Knowledge graphs have emerged as an important model for studying complex\nmulti-relational data. This has given rise to the construction of numerous\nlarge scale but incomplete knowledge graphs encoding information extracted from\nvarious resources. An effective and scalable approach to jointly learn over\nmultiple graphs and eventually construct a unified graph is a crucial next step\nfor the success of knowledge-based inference for many downstream applications.\nTo this end, we propose LinkNBed, a deep relational learning framework that\nlearns entity and relationship representations across multiple graphs. We\nidentify entity linkage across graphs as a vital component to achieve our goal.\nWe design a novel objective that leverage entity linkage and build an efficient\nmulti-task training procedure. Experiments on link prediction and entity\nlinkage demonstrate substantial improvements over the state-of-the-art\nrelational learning approaches.",
        "arxiv_firstAuthor": "Rakshit Trivedi",
        "arxiv_title": "LinkNBed: Multi-Graph Representation Learning with Entity Linkage",
        "arxiv_num": "1807.08447",
        "arxiv_published": "2018-07-23T06:47:57Z",
        "arxiv_updated": "2018-07-23T06:47:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1506.02142",
        "tag": [
            "http://www.semanlink.net/tag/uncertainty_in_deep_learning",
            "http://www.semanlink.net/tag/bayesian_deep_learning",
            "http://www.semanlink.net/tag/dropout",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1506.02142] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
        "relatedDoc": [],
        "creationTime": "2019-05-13T09:11:32Z",
        "creationDate": "2019-05-13",
        "bookmarkOf": [],
        "arxiv_author": [
            "Zoubin Ghahramani",
            "Yarin Gal"
        ],
        "arxiv_summary": "Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning.",
        "arxiv_firstAuthor": "Yarin Gal",
        "arxiv_title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
        "arxiv_num": "1506.02142",
        "arxiv_published": "2015-06-06T12:30:43Z",
        "arxiv_updated": "2016-10-04T16:50:26Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1511.06335",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/cluster_analysis"
        ],
        "comment": "Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective",
        "title": "[1511.06335] Unsupervised Deep Embedding for Clustering Analysis",
        "relatedDoc": [],
        "creationTime": "2019-02-19T19:06:06Z",
        "creationDate": "2019-02-19",
        "bookmarkOf": [],
        "arxiv_author": [
            "Ross Girshick",
            "Junyuan Xie",
            "Ali Farhadi"
        ],
        "arxiv_summary": "Clustering is central to many data-driven application domains and has been\nstudied extensively in terms of distance functions and grouping algorithms.\nRelatively little work has focused on learning representations for clustering.\nIn this paper, we propose Deep Embedded Clustering (DEC), a method that\nsimultaneously learns feature representations and cluster assignments using\ndeep neural networks. DEC learns a mapping from the data space to a\nlower-dimensional feature space in which it iteratively optimizes a clustering\nobjective. Our experimental evaluations on image and text corpora show\nsignificant improvement over state-of-the-art methods.",
        "arxiv_firstAuthor": "Junyuan Xie",
        "arxiv_title": "Unsupervised Deep Embedding for Clustering Analysis",
        "arxiv_num": "1511.06335",
        "arxiv_published": "2015-11-19T20:06:14Z",
        "arxiv_updated": "2016-05-24T22:27:35Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2102_07043_reasoning_over_vir",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/virtual_knowledge_graph",
            "http://www.semanlink.net/tag/open_domain_question_answering",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov",
            "http://www.semanlink.net/tag/nlp_google"
        ],
        "comment": "> a method for constructing **a virtual KB (VKB) trained entirely from text**\r\n\r\nOpen Predicate Query Language (OPQL): constructing a virtual knowledge base (VKB) that supports KB reasoning & open-domain QA, tackling the incompleteness of knowledge bases by constructing a virtual KB only from text\r\n\r\n> OPQL constructs\r\na VKB by **encoding and indexing a set of\r\nrelation mentions** in a way that naturally enables\r\nreasoning and can be trained without any structured\r\nsupervision.\r\n\r\n> can be used\r\nas an **external memory integrated into a language\r\nmodel**\r\n\r\ncf. this earlier paper [[2002.10640] Differentiable Reasoning over a Virtual Knowledge Base](doc:2020/07/2002_10640_differentiable_rea). But does not require an initial structured KB for distant\r\nsupervision.\r\n\r\n> The key idea in constructing the OPQL VKB is to use a\r\ndual-encoder pre-training process, similar to \r\n[[1906.03158] Matching the Blanks: Distributional Similarity for Relation Learning](doc:2021/05/1906_03158_matching_the_blank)\r\n\r\nRelated work section refers to [[1909.04164] Knowledge Enhanced Contextual Word Representations](doc:2020/05/1909_04164_knowledge_enhanced). Also refers to [[2007.00849] Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge](doc:2020/07/2007_00849_facts_as_experts_) (some authors in common)",
        "title": "[2102.07043] Reasoning Over Virtual Knowledge Bases With Open Predicate Relations",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/05/1906_03158_matching_the_blank",
            "http://www.semanlink.net/doc/2020/07/2002_10640_differentiable_rea",
            "http://www.semanlink.net/doc/2020/05/1909_04164_knowledge_enhanced",
            "http://www.semanlink.net/doc/2020/07/2007_00849_facts_as_experts_"
        ],
        "creationTime": "2021-06-20T08:30:31Z",
        "creationDate": "2021-06-20",
        "bookmarkOf": [
            "https://arxiv.org/abs/2102.07043"
        ],
        "arxiv_author": [
            "William W. Cohen",
            "Ruslan Salakhutdinov",
            "Haitian Sun",
            "Bhuwan Dhingra",
            "Pat Verga"
        ],
        "arxiv_summary": "We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.",
        "arxiv_firstAuthor": "Haitian Sun",
        "arxiv_title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations",
        "arxiv_num": "2102.07043",
        "arxiv_published": "2021-02-14T01:29:54Z",
        "arxiv_updated": "2021-06-14T19:34:42Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2004_03705_deep_learning_base",
        "tag": [
            "http://www.semanlink.net/tag/nlp_text_classification",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2004.03705] Deep Learning Based Text Classification: A Comprehensive Review",
        "relatedDoc": [],
        "creationTime": "2020-10-11T01:16:13Z",
        "creationDate": "2020-10-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.03705"
        ],
        "arxiv_author": [
            "Jianfeng Gao",
            "Narjes Nikzad",
            "Meysam Chenaghlu",
            "Nal Kalchbrenner",
            "Shervin Minaee",
            "Erik Cambria"
        ],
        "arxiv_summary": "Deep learning based models have surpassed classical machine learning based\napproaches in various text classification tasks, including sentiment analysis,\nnews categorization, question answering, and natural language inference. In\nthis work, we provide a detailed review of more than 150 deep learning based\nmodels for text classification developed in recent years, and discuss their\ntechnical contributions, similarities, and strengths. We also provide a summary\nof more than 40 popular datasets widely used for text classification. Finally,\nwe provide a quantitative analysis of the performance of different deep\nlearning models on popular benchmarks, and discuss future research directions.",
        "arxiv_firstAuthor": "Shervin Minaee",
        "arxiv_title": "Deep Learning Based Text Classification: A Comprehensive Review",
        "arxiv_num": "2004.03705",
        "arxiv_published": "2020-04-06T02:00:30Z",
        "arxiv_updated": "2020-04-06T02:00:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1711.07128.pdf",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/keyword_spotting"
        ],
        "comment": "",
        "title": "[1711.07128] Hello Edge: Keyword Spotting on Microcontrollers",
        "relatedDoc": [],
        "creationTime": "2017-12-15T09:04:47Z",
        "creationDate": "2017-12-15",
        "bookmarkOf": [],
        "arxiv_author": [
            "Liangzhen Lai",
            "Vikas Chandra",
            "Naveen Suda",
            "Yundong Zhang"
        ],
        "arxiv_summary": "Keyword spotting (KWS) is a critical component for enabling speech based user\ninteractions on smart devices. It requires real-time response and high accuracy\nfor good user experience. Recently, neural networks have become an attractive\nchoice for KWS architecture because of their superior accuracy compared to\ntraditional speech processing algorithms. Due to its always-on nature, KWS\napplication has highly constrained power budget and typically runs on tiny\nmicrocontrollers with limited memory and compute capability. The design of\nneural network architecture for KWS must consider these constraints. In this\nwork, we perform neural network architecture evaluation and exploration for\nrunning KWS on resource-constrained microcontrollers. We train various neural\nnetwork architectures for keyword spotting published in literature to compare\ntheir accuracy and memory/compute requirements. We show that it is possible to\noptimize these neural network architectures to fit within the memory and\ncompute constraints of microcontrollers without sacrificing accuracy. We\nfurther explore the depthwise separable convolutional neural network (DS-CNN)\nand compare it against other neural network architectures. DS-CNN achieves an\naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number\nof parameters.",
        "arxiv_firstAuthor": "Yundong Zhang",
        "arxiv_title": "Hello Edge: Keyword Spotting on Microcontrollers",
        "arxiv_num": "1711.07128",
        "arxiv_published": "2017-11-20T03:19:03Z",
        "arxiv_updated": "2018-02-14T19:24:55Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/1910_01348_on_the_efficacy_of",
        "tag": [
            "http://www.semanlink.net/tag/critical_evaluation",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Evaluation of the efficacy\r\nof knowledge distillation and its dependence on student\r\nand teacher architectures. IEEE International Conference on Computer Vision (ICCV), 2019\r\n\r\n> Despite\r\nwidespread use, an understanding of when the student can\r\nlearn from the teacher is missing.\r\n>\r\n> Our **key finding**\r\nis that knowledge distillation is not a panacea and cannot\r\nsucceed when student capacity is too low to successfully\r\nmimic the teacher. We have presented an approach\r\nto mitigate this issue by **stopping teacher training** early",
        "title": "[1910.01348] On the Efficacy of Knowledge Distillation",
        "relatedDoc": [],
        "creationTime": "2020-06-06T17:20:52Z",
        "creationDate": "2020-06-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/1910.01348",
            "http://openaccess.thecvf.com/content_ICCV_2019/html/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.html"
        ],
        "arxiv_author": [
            "Jang Hyun Cho",
            "Bharath Hariharan"
        ],
        "arxiv_summary": "In this paper, we present a thorough evaluation of the efficacy of knowledge\ndistillation and its dependence on student and teacher architectures. Starting\nwith the observation that more accurate teachers often don't make good\nteachers, we attempt to tease apart the factors that affect knowledge\ndistillation performance. We find crucially that larger models do not often\nmake better teachers. We show that this is a consequence of mismatched\ncapacity, and that small students are unable to mimic large teachers. We find\ntypical ways of circumventing this (such as performing a sequence of knowledge\ndistillation steps) to be ineffective. Finally, we show that this effect can be\nmitigated by stopping the teacher's training early. Our results generalize\nacross datasets and models.",
        "arxiv_firstAuthor": "Jang Hyun Cho",
        "arxiv_title": "On the Efficacy of Knowledge Distillation",
        "arxiv_num": "1910.01348",
        "arxiv_published": "2019-10-03T08:14:13Z",
        "arxiv_updated": "2019-10-03T08:14:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/investigating_entity_knowledge_",
        "tag": [
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/end_to_end_entity_linking",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bertology",
            "http://www.semanlink.net/tag/bert"
        ],
        "comment": "Training BERT-base-uncased on English Wikipedia and then fine-tuned and evaluating it\r\non an entity linking (EL) benchmark (EL implemented as a token classification over the entity vocabulary)\r\n\r\n> BERT+Entity is a straightforward extension on top\r\nof BERT, i.e. we initialize BERT with the publicly\r\navailable weights from the BERT-base-uncased\r\nmodel and add an output classification layer on\r\ntop of the architecture. Given a contextualized token,\r\nthe classifier computes the probability of an\r\nentity link for each entry in the entity vocabulary.\r\n\r\nCan BERT\u2019s architecture learn all entity\r\nlinking steps jointly? To answer:\r\n\r\n> an extreme\r\nsimplification of the **entity linking setup that\r\nworks surprisingly well**: simply cast it as **a\r\nper token classification over the entire entity\r\nvocabulary** (over 700K classes in our case).\r\n\r\n> the model\r\nis the first that performs entity linking without any\r\npipeline or any heuristics, compared to all prior\r\napproaches. We found that with our approach we\r\ncan learn additional entity knowledge in BERT that\r\nhelps in entity linking. **However, we also found\r\nthat almost none of the downstream tasks really\r\nrequired entity knowledge**.\r\n\r\n### Related work \r\n\r\n- > [Durrett and Klein (2014)](/doc/2020/01/a_joint_model_for_entity_analys) were the first to propose\r\njointly modelling Mention detection, Candidate generation and Entity disambiguation in a graphical\r\nmodel and could show that each of those steps are\r\ninterdependent and benefit from a joint objective\r\n\r\nThis paper uses neural techniques instead of CRF.\r\n\r\n- > [Yamada](/showprop.do?pptyuri=http%3A%2F%2Fwww.semanlink.net%2F2001%2F00%2Fsemanlink-schema%23arxiv_author&pptyval=Ikuya%2BYamada) (2016, 2017) was the first to\r\ninvestigate neural text representations and entity\r\nlinking, but their approach is limited to ED.\r\n\r\ncf. [#Wikipedia2Vec](tag:wikipedia2vec). Compare with [newer work by Yamada](doc:2020/09/1909_01259_neural_attentive_b)",
        "title": "[2003.05473] Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking (CoNNL 2019)",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/01/a_joint_model_for_entity_analys",
            "http://www.semanlink.net/doc/2020/09/1909_01259_neural_attentive_b"
        ],
        "creationTime": "2020-01-09T10:36:17Z",
        "creationDate": "2020-01-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.05473"
        ],
        "arxiv_author": [
            "Samuel Broscheit"
        ],
        "arxiv_summary": "A typical architecture for end-to-end entity linking systems consists of\nthree steps: mention detection, candidate generation and entity disambiguation.\nIn this study we investigate the following questions: (a) Can all those steps\nbe learned jointly with a model for contextualized text-representations, i.e.\nBERT (Devlin et al., 2019)? (b) How much entity knowledge is already contained\nin pretrained BERT? (c) Does additional entity knowledge improve BERT's\nperformance in downstream tasks? To this end, we propose an extreme\nsimplification of the entity linking setup that works surprisingly well: simply\ncast it as a per token classification over the entire entity vocabulary (over\n700K classes in our case). We show on an entity linking benchmark that (i) this\nmodel improves the entity representations over plain BERT, (ii) that it\noutperforms entity linking architectures that optimize the tasks separately and\n(iii) that it only comes second to the current state-of-the-art that does\nmention detection and entity disambiguation jointly. Additionally, we\ninvestigate the usefulness of entity-aware token-representations in the\ntext-understanding benchmark GLUE, as well as the question answering benchmarks\nSQUAD V2 and SWAG and also the EN-DE WMT14 machine translation benchmark. To\nour surprise, we find that most of those benchmarks do not benefit from\nadditional entity knowledge, except for a task with very small training data,\nthe RTE task in GLUE, which improves by 2%.",
        "arxiv_firstAuthor": "Samuel Broscheit",
        "arxiv_title": "Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking",
        "arxiv_num": "2003.05473",
        "arxiv_published": "2020-03-11T18:23:00Z",
        "arxiv_updated": "2020-03-11T18:23:00Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1609.08496",
        "tag": [
            "http://www.semanlink.net/tag/topic_models_word_embedding",
            "http://www.semanlink.net/tag/topic_modeling_over_short_texts",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "New method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo-texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic",
        "title": "[1609.08496] Topic Modeling over Short Texts by Incorporating Word Embeddings",
        "relatedDoc": [],
        "creationTime": "2017-06-07T18:13:32Z",
        "creationDate": "2017-06-07",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jipeng Qiang",
            "Tong Wang",
            "Xindong Wu",
            "Ping Chen"
        ],
        "arxiv_summary": "Inferring topics from the overwhelming amount of short texts becomes a\ncritical but challenging task for many content analysis tasks, such as content\ncharactering, user interest profiling, and emerging topic detecting. Existing\nmethods such as probabilistic latent semantic analysis (PLSA) and latent\nDirichlet allocation (LDA) cannot solve this prob- lem very well since only\nvery limited word co-occurrence information is available in short texts. This\npaper studies how to incorporate the external word correlation knowledge into\nshort texts to improve the coherence of topic modeling. Based on recent results\nin word embeddings that learn se- mantically representations for words from a\nlarge corpus, we introduce a novel method, Embedding-based Topic Model (ETM),\nto learn latent topics from short texts. ETM not only solves the problem of\nvery limited word co-occurrence information by aggregating short texts into\nlong pseudo- texts, but also utilizes a Markov Random Field regularized model\nthat gives correlated words a better chance to be put into the same topic. The\nexperiments on real-world datasets validate the effectiveness of our model\ncomparing with the state-of-the-art models.",
        "arxiv_firstAuthor": "Jipeng Qiang",
        "arxiv_title": "Topic Modeling over Short Texts by Incorporating Word Embeddings",
        "arxiv_num": "1609.08496",
        "arxiv_published": "2016-09-27T15:26:07Z",
        "arxiv_updated": "2016-09-27T15:26:07Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1506.01094",
        "tag": [
            "http://www.semanlink.net/tag/path_queries",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/knowledge_graph_completion"
        ],
        "comment": "Knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy.",
        "title": "[1506.01094] Traversing Knowledge Graphs in Vector Space",
        "relatedDoc": [],
        "creationTime": "2015-10-31T00:11:12Z",
        "creationDate": "2015-10-31",
        "bookmarkOf": [],
        "arxiv_author": [
            "Percy Liang",
            "John Miller",
            "Kelvin Guu"
        ],
        "arxiv_summary": "Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.",
        "arxiv_firstAuthor": "Kelvin Guu",
        "arxiv_title": "Traversing Knowledge Graphs in Vector Space",
        "arxiv_num": "1506.01094",
        "arxiv_published": "2015-06-03T00:38:25Z",
        "arxiv_updated": "2015-08-19T05:16:24Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_physics_0004057_the_informati",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/naftali_tishby",
            "http://www.semanlink.net/tag/information_bottleneck_method"
        ],
        "comment": "> We define the relevant information in a signal x \u2208 X as being the information that this signal provides about another signal y \u2208 Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. **Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y.** That is, we squeeze the information that X provides about Y through a \u2018bottleneck\u2019 formed by a limited set of codewords X \u0303... This approach yields an exact set of self consistent equations for the coding rules X \u2192 X \u0303 and X \u0303 \u2192 Y .\r\n\r\n(from the intro) : how to define \"meaningful / relevant\" information? An issue left out of information theory by Shannon (focus on the problem of transmitting information rather than judging its value to the recipient) ->leads to\r\nconsider statistical and information theoretic principles as almost irrelevant\r\nfor the question of meaning. \r\n\r\n> In contrast, **we argue here that information theory,\r\nin particular lossy source compression, provides a natural quantitative\r\napproach to the question of \u201crelevant information.\u201d** Specifically, we formulate\r\na **variational principle** for the extraction or efficient representation of\r\nrelevant information.\r\n\r\n",
        "title": "[physics/0004057] The information bottleneck method",
        "relatedDoc": [],
        "creationTime": "2019-08-15T11:31:33Z",
        "creationDate": "2019-08-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/physics/0004057"
        ],
        "arxiv_author": [
            "Naftali Tishby Hebrew University and NEC Research Institute",
            "Fernando C. Pereira ATT Shannon Laboratory",
            "William Bialek NEC Research Institute"
        ],
        "arxiv_summary": "We define the relevant information in a signal $x\\in X$ as being the\ninformation that this signal provides about another signal $y\\in \\Y$. Examples\ninclude the information that face images provide about the names of the people\nportrayed, or the information that speech sounds provide about the words\nspoken. Understanding the signal $x$ requires more than just predicting $y$, it\nalso requires specifying which features of $\\X$ play a role in the prediction.\nWe formalize this problem as that of finding a short code for $\\X$ that\npreserves the maximum information about $\\Y$. That is, we squeeze the\ninformation that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a\nlimited set of codewords $\\tX$. This constrained optimization problem can be\nseen as a generalization of rate distortion theory in which the distortion\nmeasure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This\napproach yields an exact set of self consistent equations for the coding rules\n$X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a\nconvergent re-estimation method that generalizes the Blahut-Arimoto algorithm.\nOur variational principle provides a surprisingly rich framework for discussing\na variety of problems in signal processing and learning, as will be described\nin detail elsewhere.",
        "arxiv_firstAuthor": "Naftali Tishby Hebrew University and NEC Research Institute",
        "arxiv_title": "The information bottleneck method",
        "arxiv_num": "physics/0004057",
        "arxiv_published": "2000-04-24T15:22:30Z",
        "arxiv_updated": "2000-04-24T15:22:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/08/1607_00653_node2vec_scalable",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/node2vec",
            "http://www.semanlink.net/tag/jure_leskovec"
        ],
        "comment": "> algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.",
        "title": "[1607.00653] node2vec: Scalable Feature Learning for Networks",
        "relatedDoc": [],
        "creationTime": "2020-08-08T15:57:03Z",
        "creationDate": "2020-08-08",
        "bookmarkOf": [
            "https://arxiv.org/abs/1607.00653"
        ],
        "arxiv_author": [
            "Aditya Grover",
            "Jure Leskovec"
        ],
        "arxiv_summary": "Prediction tasks over nodes and edges in networks require careful effort in\nengineering features used by learning algorithms. Recent research in the\nbroader field of representation learning has led to significant progress in\nautomating prediction by learning the features themselves. However, present\nfeature learning approaches are not expressive enough to capture the diversity\nof connectivity patterns observed in networks. Here we propose node2vec, an\nalgorithmic framework for learning continuous feature representations for nodes\nin networks. In node2vec, we learn a mapping of nodes to a low-dimensional\nspace of features that maximizes the likelihood of preserving network\nneighborhoods of nodes. We define a flexible notion of a node's network\nneighborhood and design a biased random walk procedure, which efficiently\nexplores diverse neighborhoods. Our algorithm generalizes prior work which is\nbased on rigid notions of network neighborhoods, and we argue that the added\nflexibility in exploring neighborhoods is the key to learning richer\nrepresentations. We demonstrate the efficacy of node2vec over existing\nstate-of-the-art techniques on multi-label classification and link prediction\nin several real-world networks from diverse domains. Taken together, our work\nrepresents a new way for efficiently learning state-of-the-art task-independent\nrepresentations in complex networks.",
        "arxiv_firstAuthor": "Aditya Grover",
        "arxiv_title": "node2vec: Scalable Feature Learning for Networks",
        "arxiv_num": "1607.00653",
        "arxiv_published": "2016-07-03T16:09:30Z",
        "arxiv_updated": "2016-07-03T16:09:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1605.07427",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/yoshua_bengio",
            "http://www.semanlink.net/tag/hierarchical_memory_networks"
        ],
        "comment": "> hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory",
        "title": "[1605.07427] Hierarchical Memory Networks",
        "relatedDoc": [],
        "creationTime": "2018-11-14T01:42:02Z",
        "creationDate": "2018-11-14",
        "bookmarkOf": [],
        "arxiv_author": [
            "Sarath Chandar",
            "Hugo Larochelle",
            "Pascal Vincent",
            "Sungjin Ahn",
            "Gerald Tesauro",
            "Yoshua Bengio"
        ],
        "arxiv_summary": "Memory networks are neural networks with an explicit memory component that\ncan be both read and written to by the network. The memory is often addressed\nin a soft way using a softmax function, making end-to-end training with\nbackpropagation possible. However, this is not computationally scalable for\napplications which require the network to read from extremely large memories.\nOn the other hand, it is well known that hard attention mechanisms based on\nreinforcement learning are challenging to train successfully. In this paper, we\nexplore a form of hierarchical memory network, which can be considered as a\nhybrid between hard and soft attention memory networks. The memory is organized\nin a hierarchical structure such that reading from it is done with less\ncomputation than soft attention over a flat memory, while also being easier to\ntrain than hard attention over a flat memory. Specifically, we propose to\nincorporate Maximum Inner Product Search (MIPS) in the training and inference\nprocedures for our hierarchical memory network. We explore the use of various\nstate-of-the art approximate MIPS techniques and report results on\nSimpleQuestions, a challenging large scale factoid question answering task.",
        "arxiv_firstAuthor": "Sarath Chandar",
        "arxiv_title": "Hierarchical Memory Networks",
        "arxiv_num": "1605.07427",
        "arxiv_published": "2016-05-24T12:48:19Z",
        "arxiv_updated": "2016-05-24T12:48:19Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1901.02860",
        "tag": [
            "http://www.semanlink.net/tag/acl_2019",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1901.02860] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
        "relatedDoc": [],
        "creationTime": "2019-01-11T17:32:14Z",
        "creationDate": "2019-01-11",
        "bookmarkOf": [],
        "arxiv_author": [
            "Yiming Yang",
            "Zihang Dai",
            "Jaime Carbonell",
            "Quoc V. Le",
            "Ruslan Salakhutdinov",
            "Zhilin Yang"
        ],
        "arxiv_summary": "Transformers have a potential of learning longer-term dependency, but are\nlimited by a fixed-length context in the setting of language modeling. We\npropose a novel neural architecture Transformer-XL that enables learning\ndependency beyond a fixed length without disrupting temporal coherence. It\nconsists of a segment-level recurrence mechanism and a novel positional\nencoding scheme. Our method not only enables capturing longer-term dependency,\nbut also resolves the context fragmentation problem. As a result,\nTransformer-XL learns dependency that is 80% longer than RNNs and 450% longer\nthan vanilla Transformers, achieves better performance on both short and long\nsequences, and is up to 1,800+ times faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-the-art results of bpc/perplexity\nto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion\nWord, and 54.5 on Penn Treebank (without finetuning). When trained only on\nWikiText-103, Transformer-XL manages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our code, pretrained models, and\nhyperparameters are available in both Tensorflow and PyTorch.",
        "arxiv_firstAuthor": "Zihang Dai",
        "arxiv_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
        "arxiv_num": "1901.02860",
        "arxiv_published": "2019-01-09T18:28:19Z",
        "arxiv_updated": "2019-06-02T21:21:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/07/_1904_13001_encoding_categoric",
        "tag": [
            "http://www.semanlink.net/tag/categorical_variables",
            "http://www.semanlink.net/tag/stacking_ensemble_learning",
            "http://www.semanlink.net/tag/reseaux_bayesiens",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> To encode high-cardinality categorical variables, we introduce a technique based on traditional Bayesian statistics. This technique is a paradigm for ensemble modeling, specifically stacking, where the base learner consists of a problem- specific conjugate Bayesian model (CBM)\r\n",
        "title": "[1904.13001] Encoding Categorical Variables with Conjugate Bayesian Models for WeWork Lead Scoring Engine",
        "relatedDoc": [],
        "creationTime": "2019-07-04T01:43:34Z",
        "creationDate": "2019-07-04",
        "bookmarkOf": [
            "https://arxiv.org/abs/1904.13001"
        ],
        "arxiv_author": [
            "Austin Slakey",
            "Daniel Salas",
            "Yoni Schamroth"
        ],
        "arxiv_summary": "Applied Data Scientists throughout various industries are commonly faced with\nthe challenging task of encoding high-cardinality categorical features into\ndigestible inputs for machine learning algorithms. This paper describes a\nBayesian encoding technique developed for WeWork's lead scoring engine which\noutputs the probability of a person touring one of our office spaces based on\ninteraction, enrichment, and geospatial data. We present a paradigm for\nensemble modeling which mitigates the need to build complicated preprocessing\nand encoding schemes for categorical variables. In particular, domain-specific\nconjugate Bayesian models are employed as base learners for features in a\nstacked ensemble model. For each column of a categorical feature matrix we fit\na problem-specific prior distribution, for example, the Beta distribution for a\nbinary classification problem. In order to analytically derive the moments of\nthe posterior distribution, we update the prior with the conjugate likelihood\nof the corresponding target variable for each unique value of the given\ncategorical feature. This function of column and value encodes the categorical\nfeature matrix so that the final learner in the ensemble model ingests\nlow-dimensional numerical input. Experimental results on both curated and real\nworld datasets demonstrate impressive accuracy and computational efficiency on\na variety of problem archetypes. Particularly, for the lead scoring engine at\nWeWork -- where some categorical features have as many as 300,000 levels -- we\nhave seen an AUC improvement from 0.87 to 0.97 through implementing conjugate\nBayesian model encoding.",
        "arxiv_firstAuthor": "Austin Slakey",
        "arxiv_title": "Encoding Categorical Variables with Conjugate Bayesian Models for WeWork Lead Scoring Engine",
        "arxiv_num": "1904.13001",
        "arxiv_published": "2019-04-30T00:24:06Z",
        "arxiv_updated": "2019-04-30T00:24:06Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1711.09677",
        "tag": [
            "http://www.semanlink.net/tag/three_way_decisions",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1711.09677] Binary classification models with \"Uncertain\" predictions",
        "relatedDoc": [],
        "creationTime": "2019-02-02T15:22:02Z",
        "creationDate": "2019-02-02",
        "bookmarkOf": [],
        "arxiv_author": [
            "David E Leahy",
            "Simon Thomas",
            "Ljubomir Buturovic",
            "Damjan Krstajic"
        ],
        "arxiv_summary": "Binary classification models which can assign probabilities to categories\nsuch as \"the tissue is 75% likely to be tumorous\" or \"the chemical is 25%\nlikely to be toxic\" are well understood statistically, but their utility as an\ninput to decision making is less well explored. We argue that users need to\nknow which is the most probable outcome, how likely that is to be true and, in\naddition, whether the model is capable enough to provide an answer. It is the\nlast case, where the potential outcomes of the model explicitly include \"don't\nknow\" that is addressed in this paper. Including this outcome would better\nseparate those predictions that can lead directly to a decision from those\nwhere more data is needed. Where models produce an \"Uncertain\" answer similar\nto a human reply of \"don't know\" or \"50:50\" in the examples we refer to\nearlier, this would translate to actions such as \"operate on tumour\" or \"remove\ncompound from use\" where the models give a \"more true than not\" answer. Where\nthe models judge the result \"Uncertain\" the practical decision might be \"carry\nout more detailed laboratory testing of compound\" or \"commission new tissue\nanalyses\". The paper presents several examples where we first analyse the\neffect of its introduction, then present a methodology for separating\n\"Uncertain\" from binary predictions and finally, we provide arguments for its\nuse in practice.",
        "arxiv_firstAuthor": "Damjan Krstajic",
        "arxiv_title": "Binary classification models with \"Uncertain\" predictions",
        "arxiv_num": "1711.09677",
        "arxiv_published": "2017-11-27T13:29:42Z",
        "arxiv_updated": "2017-12-04T15:10:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1004.5370.pdf",
        "tag": [
            "http://www.semanlink.net/tag/learning_to_hash",
            "http://www.semanlink.net/tag/nearest_neighbor_search",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/semantic_hashing"
        ],
        "comment": "Emphasise following issue in Semantic Hashing: obtaining the codes for previously unseen documents. Propose following approach:\r\nfirst find the optimal l-bit binary codes for all documents in\r\nthe given corpus via unsupervised learning, then train\r\nl classifiers via supervised learning to predict the l-bit code\r\nfor any query document unseen before.\r\n\r\n(m\u00e9thode r\u00e9sum\u00e9e [ici](https://www.semanticscholar.org/paper/Semantic-hashing-using-tags-and-topic-modeling-Wang-Zhang/1a0f660f70fd179003edc271694736baaa39dec4))\r\n\r\n\r\n",
        "title": "[1004.5370] Self-Taught Hashing for Fast Similarity Search",
        "relatedDoc": [
            "https://www.semanticscholar.org/paper/Semantic-hashing-using-tags-and-topic-modeling-Wang-Zhang/1a0f660f70fd179003edc271694736baaa39dec4"
        ],
        "creationTime": "2017-11-07T11:48:17Z",
        "creationDate": "2017-11-07",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jun Wang",
            "Dell Zhang",
            "Jinsong Lu",
            "Deng Cai"
        ],
        "arxiv_summary": "The ability of fast similarity search at large scale is of great importance\nto many Information Retrieval (IR) applications. A promising way to accelerate\nsimilarity search is semantic hashing which designs compact binary codes for a\nlarge number of documents so that semantically similar documents are mapped to\nsimilar codes (within a short Hamming distance). Although some recently\nproposed techniques are able to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents remains to be a\nvery challenging problem. In this paper, we emphasise this issue and propose a\nnovel Self-Taught Hashing (STH) approach to semantic hashing: we first find the\noptimal $l$-bit binary codes for all documents in the given corpus via\nunsupervised learning, and then train $l$ classifiers via supervised learning\nto predict the $l$-bit code for any query document unseen before. Our\nexperiments on three real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine\n(SVM) outperforms state-of-the-art techniques significantly.",
        "arxiv_firstAuthor": "Dell Zhang",
        "arxiv_title": "Self-Taught Hashing for Fast Similarity Search",
        "arxiv_num": "1004.5370",
        "arxiv_published": "2010-04-29T19:25:17Z",
        "arxiv_updated": "2010-04-29T19:25:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/2003_08001_realistic_re_evalu",
        "tag": [
            "http://www.semanlink.net/tag/critical_evaluation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embedding_evaluation",
            "http://www.semanlink.net/tag/knowledge_graph_completion",
            "http://www.semanlink.net/tag/link_prediction",
            "http://www.semanlink.net/tag/graph_embeddings"
        ],
        "comment": "data redundancy (reverse relations), Cartesian product relations\r\n\r\n> A more fundamental defect\r\nof these models is that the link prediction scenario, given\r\nsuch data, is non-existent in the real-world",
        "title": "[2003.08001] Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study",
        "relatedDoc": [],
        "creationTime": "2020-05-15T17:26:28Z",
        "creationDate": "2020-05-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.08001"
        ],
        "arxiv_author": [
            "Qingheng Zhang State Key Laboratory for Novel Software Technology, Nanjing University",
            "Chengkai Li Department of Computer Science and Engineering, University of Texas at Arlington",
            "Mohammed Samiul Saeef Department of Computer Science and Engineering, University of Texas at Arlington",
            "Farahnaz Akrami Department of Computer Science and Engineering, University of Texas at Arlington",
            "Wei Hu State Key Laboratory for Novel Software Technology, Nanjing University"
        ],
        "arxiv_summary": "In the active research area of employing embedding models for knowledge graph\ncompletion, particularly for the task of link prediction, most prior studies\nused two benchmark datasets FB15k and WN18 in evaluating such models. Most\ntriples in these and other datasets in such studies belong to reverse and\nduplicate relations which exhibit high data redundancy due to semantic\nduplication, correlation or data incompleteness. This is a case of excessive\ndata leakage---a model is trained using features that otherwise would not be\navailable when the model needs to be applied for real prediction. There are\nalso Cartesian product relations for which every triple formed by the Cartesian\nproduct of applicable subjects and objects is a true fact. Link prediction on\nthe aforementioned relations is easy and can be achieved with even better\naccuracy using straightforward rules instead of sophisticated embedding models.\nA more fundamental defect of these models is that the link prediction scenario,\ngiven such data, is non-existent in the real-world. This paper is the first\nsystematic study with the main objective of assessing the true effectiveness of\nembedding models when the unrealistic triples are removed. Our experiment\nresults show these models are much less accurate than what we used to perceive.\nTheir poor accuracy renders link prediction a task without truly effective\nautomated solution. Hence, we call for re-investigation of possible effective\napproaches.",
        "arxiv_firstAuthor": "Farahnaz Akrami Department of Computer Science and Engineering, University of Texas at Arlington",
        "arxiv_title": "Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study",
        "arxiv_num": "2003.08001",
        "arxiv_published": "2020-03-18T01:18:09Z",
        "arxiv_updated": "2020-03-18T01:18:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1902.10618",
        "tag": [
            "http://www.semanlink.net/tag/contextualised_word_representations",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/word_embedding_compositionality"
        ],
        "comment": "How well do contextualized word embeddings address lexical composition? They are good in recognizing meaning shift (\"give in\" is different from \"give\") but much worse with revealing implicit meaning (\"hot tea\" is about temperature, \"hot debate\" isn't).",
        "title": "[1902.10618] Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition",
        "relatedDoc": [],
        "creationTime": "2019-02-28T13:10:48Z",
        "creationDate": "2019-02-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Ido Dagan",
            "Vered Shwartz"
        ],
        "arxiv_summary": "Building meaningful phrase representations is challenging because phrase\nmeanings are not simply the sum of their constituent meanings. Lexical\ncomposition can shift the meanings of the constituent words and introduce\nimplicit information. We tested a broad range of textual representations for\ntheir capacity to address these issues. We found that as expected,\ncontextualized word representations perform better than static word embeddings,\nmore so on detecting meaning shift than in recovering implicit information, in\nwhich their performance is still far from that of humans. Our evaluation suite,\nincluding 5 tasks related to lexical composition effects, can serve future\nresearch aiming to improve such representations.",
        "arxiv_firstAuthor": "Vered Shwartz",
        "arxiv_title": "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition",
        "arxiv_num": "1902.10618",
        "arxiv_published": "2019-02-27T16:16:37Z",
        "arxiv_updated": "2019-05-19T13:47:16Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/07/_1901_00596_a_comprehensive_su",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/graph_neural_networks",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "an overview of graph neural networks (GNNs) in data mining and machine learning fields",
        "title": "[1901.00596] A Comprehensive Survey on Graph Neural Networks",
        "relatedDoc": [],
        "creationTime": "2019-07-15T23:15:09Z",
        "creationDate": "2019-07-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/1901.00596"
        ],
        "arxiv_author": [
            "Chengqi Zhang",
            "Philip S. Yu",
            "Fengwen Chen",
            "Zonghan Wu",
            "Shirui Pan",
            "Guodong Long"
        ],
        "arxiv_summary": "Deep learning has revolutionized many machine learning tasks in recent years,\nranging from image classification and video processing to speech recognition\nand natural language understanding. The data in these tasks are typically\nrepresented in the Euclidean space. However, there is an increasing number of\napplications where data are generated from non-Euclidean domains and are\nrepresented as graphs with complex relationships and interdependency between\nobjects. The complexity of graph data has imposed significant challenges on\nexisting machine learning algorithms. Recently, many studies on extending deep\nlearning approaches for graph data have emerged. In this survey, we provide a\ncomprehensive overview of graph neural networks (GNNs) in data mining and\nmachine learning fields. We propose a new taxonomy to divide the\nstate-of-the-art graph neural networks into four categories, namely recurrent\ngraph neural networks, convolutional graph neural networks, graph autoencoders,\nand spatial-temporal graph neural networks. We further discuss the applications\nof graph neural networks across various domains and summarize the open source\ncodes, benchmark data sets, and model evaluation of graph neural networks.\nFinally, we propose potential research directions in this rapidly growing\nfield.",
        "arxiv_firstAuthor": "Zonghan Wu",
        "arxiv_title": "A Comprehensive Survey on Graph Neural Networks",
        "arxiv_num": "1901.00596",
        "arxiv_published": "2019-01-03T03:20:55Z",
        "arxiv_updated": "2019-12-04T01:43:00Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1708.00214.pdf",
        "tag": [
            "http://www.semanlink.net/tag/nn_4_nlp",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "google guys: \r\n\r\n> We show that small and shallow feed- forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.\r\n",
        "title": "[1708.00214] Natural Language Processing with Small Feed-Forward Networks",
        "relatedDoc": [],
        "creationTime": "2017-08-04T00:43:05Z",
        "creationDate": "2017-08-04",
        "bookmarkOf": [],
        "arxiv_author": [
            "Anton Bakalov",
            "Jan A. Botha",
            "Alex Salcianu",
            "Slav Petrov",
            "Emily Pitler",
            "Ryan McDonald",
            "Ji Ma",
            "David Weiss"
        ],
        "arxiv_summary": "We show that small and shallow feed-forward neural networks can achieve near\nstate-of-the-art results on a range of unstructured and structured language\nprocessing tasks while being considerably cheaper in memory and computational\nrequirements than deep recurrent models. Motivated by resource-constrained\nenvironments like mobile phones, we showcase simple techniques for obtaining\nsuch small neural network models, and investigate different tradeoffs when\ndeciding how to allocate a small memory budget.",
        "arxiv_firstAuthor": "Jan A. Botha",
        "arxiv_title": "Natural Language Processing with Small Feed-Forward Networks",
        "arxiv_num": "1708.00214",
        "arxiv_published": "2017-08-01T09:13:44Z",
        "arxiv_updated": "2017-08-01T09:13:44Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1912_03263_your_classifier_is",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/statistical_classification"
        ],
        "comment": "",
        "title": "[1912.03263] Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One",
        "relatedDoc": [],
        "creationTime": "2019-12-09T23:28:51Z",
        "creationDate": "2019-12-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/1912.03263"
        ],
        "arxiv_author": [
            "David Duvenaud",
            "Mohammad Norouzi",
            "Kuan-Chieh Wang",
            "Will Grathwohl",
            "J\u00f6rn-Henrik Jacobsen",
            "Kevin Swersky"
        ],
        "arxiv_summary": "We propose to reinterpret a standard discriminative classifier of p(y|x) as\nan energy based model for the joint distribution p(x,y). In this setting, the\nstandard class probabilities can be easily computed as well as unnormalized\nvalues of p(x) and p(x|y). Within this framework, standard discriminative\narchitectures may beused and the model can also be trained on unlabeled data.\nWe demonstrate that energy based training of the joint distribution improves\ncalibration, robustness, andout-of-distribution detection while also enabling\nour models to generate samplesrivaling the quality of recent GAN approaches. We\nimprove upon recently proposed techniques for scaling up the training of energy\nbased models and presentan approach which adds little overhead compared to\nstandard classification training. Our approach is the first to achieve\nperformance rivaling the state-of-the-artin both generative and discriminative\nlearning within one hybrid model.",
        "arxiv_firstAuthor": "Will Grathwohl",
        "arxiv_title": "Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One",
        "arxiv_num": "1912.03263",
        "arxiv_published": "2019-12-06T18:00:36Z",
        "arxiv_updated": "2019-12-11T19:57:55Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_2002_05867v1_transformers_as_",
        "tag": [
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/knowledge_representation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/rules",
            "http://www.semanlink.net/tag/reasoning"
        ],
        "comment": "> AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but **using rules expressed in language, thus bypassing a formal representation**.",
        "title": "[2002.05867] Transformers as Soft Reasoners over Language",
        "relatedDoc": [],
        "creationTime": "2020-02-17T09:06:44Z",
        "creationDate": "2020-02-17",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.05867"
        ],
        "arxiv_author": [
            "Oyvind Tafjord",
            "Peter Clark",
            "Kyle Richardson"
        ],
        "arxiv_summary": "AI has long pursued the goal of having systems reason over *explicitly\nprovided* knowledge, but building suitable representations has proved\nchallenging. Here we explore whether transformers can similarly learn to reason\n(or emulate reasoning), but using rules expressed in language, thus bypassing a\nformal representation. We provide the first demonstration that this is\npossible, and characterize the extent of this capability. To do this, we use a\ncollection of synthetic datasets that test increasing levels of reasoning\ncomplexity (number of rules, presence of negation, and depth of chaining). We\nfind transformers appear to learn rule-based reasoning with high (99%) accuracy\non these datasets, and in a way that generalizes to test data requiring\nsubstantially deeper chaining than in the training data (95%+ scores). We also\ndemonstrate that the models transfer well to two hand-authored rulebases, and\nto rulebases paraphrased into more natural language. These findings are\nsignificant as it suggests a new role for transformers, namely as a limited\n\"soft theorem prover\" operating over explicit theories in language. This in\nturn suggests new possibilities for explainability, correctability, and\ncounterfactual reasoning in question-answering. All datasets and a live demo\nare available at http://rule-reasoning.apps.allenai.org/",
        "arxiv_firstAuthor": "Peter Clark",
        "arxiv_title": "Transformers as Soft Reasoners over Language",
        "arxiv_num": "2002.05867",
        "arxiv_published": "2020-02-14T04:23:28Z",
        "arxiv_updated": "2020-02-14T04:23:28Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2106_04612_neural_extractive_",
        "tag": [
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/cognitive_search",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_models_for_information_retrieval",
            "http://www.semanlink.net/tag/search",
            "http://www.semanlink.net/tag/yoav_goldberg"
        ],
        "comment": "how to extend a\r\nsearch paradigm we call \u201c**extractive search**\u201d with\r\nneural similarity techniques.\r\n\r\n> some information needs require extracting\r\nand aggregating sub-sentence information\r\n(words, phrases, or entities) from multiple documents\r\n(e.g. a list of all the risk factors for a specific\r\ndisease and their number of mentions, or a comprehensive\r\ntable of startups and CEOs).\r\n\r\n> extractive search combines\r\ndocument selection with information extraction. **The query is extended with capture slots**:\r\nthese are **search terms that act as variables, whose\r\nvalues should be extracted**.\r\n> The user\r\nis then presented with the matched documents, each\r\nannotated with the corresponding captured spans,\r\nas well as aggregate information over the captured\r\nspans\r\n\r\nConclusion : \r\n\r\n> We presented a system for neural extractive search.\r\nWhile we found our system to be useful for scientific\r\nsearch, it also has clear limitations and areas\r\nfor improvement, both in terms of accuracy (only\r\n72.2% of the returned results are relevant, both the\r\nalignment and similarity models generalize well to\r\nsome relations but not to others), and in terms of\r\nscale\r\n\r\n[Video of demo](https://www.youtube.com/watch?v=TtqWi2GgB5A&t=1832s)",
        "title": "[2106.04612] Neural Extractive Search",
        "relatedDoc": [],
        "creationTime": "2021-06-23T01:47:35Z",
        "creationDate": "2021-06-23",
        "bookmarkOf": [
            "https://arxiv.org/abs/2106.04612"
        ],
        "arxiv_author": [
            "Yoav Goldberg",
            "Hillel Taub-Tabib",
            "Shauli Ravfogel"
        ],
        "arxiv_summary": "Domain experts often need to extract structured information from large\ncorpora. We advocate for a search paradigm called ``extractive search'', in\nwhich a search query is enriched with capture-slots, to allow for such rapid\nextraction. Such an extractive search system can be built around syntactic\nstructures, resulting in high-precision, low-recall results. We show how the\nrecall can be improved using neural retrieval and alignment. The goals of this\npaper are to concisely introduce the extractive-search paradigm; and to\ndemonstrate a prototype neural retrieval system for extractive search and its\nbenefits and potential. Our prototype is available at\n\\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is\navailable at \\url{https://vimeo.com/559586687}.",
        "arxiv_firstAuthor": "Shauli Ravfogel",
        "arxiv_title": "Neural Extractive Search",
        "arxiv_num": "2106.04612",
        "arxiv_published": "2021-06-08T18:03:31Z",
        "arxiv_updated": "2021-06-08T18:03:31Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/12/2011_06993_flert_document_le",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/flair",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "> Current state-of-the-art approaches for named entity recognition (NER) using BERT-style transformers typically use one of two different approaches: \r\n>\r\n>1. The first fine-tunes the transformer itself on the NER task and adds only a simple linear layer for word-level predictions.\r\n>2. The second uses the transformer only to provide features to a standard LSTM-CRF sequence labeling architecture and thus performs no fine-tuning.\r\n>\r\n> In this paper, we perform a comparative analysis of both approaches\r\n\r\nConclusion: \r\n\r\n> We recommend the combination of\r\ndocument-level features and fine-tuning for NER.",
        "title": "[2011.06993] FLERT: Document-Level Features for Named Entity Recognition",
        "relatedDoc": [],
        "creationTime": "2020-12-01T09:25:14Z",
        "creationDate": "2020-12-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/2011.06993"
        ],
        "arxiv_author": [
            "Alan Akbik",
            "Stefan Schweter"
        ],
        "arxiv_summary": "Current state-of-the-art approaches for named entity recognition (NER) using\nBERT-style transformers typically use one of two different approaches: (1) The\nfirst fine-tunes the transformer itself on the NER task and adds only a simple\nlinear layer for word-level predictions. (2) The second uses the transformer\nonly to provide features to a standard LSTM-CRF sequence labeling architecture\nand thus performs no fine-tuning. In this paper, we perform a comparative\nanalysis of both approaches in a variety of settings currently considered in\nthe literature. In particular, we evaluate how well they work when\ndocument-level features are leveraged. Our evaluation on the classic CoNLL\nbenchmark datasets for 4 languages shows that document-level features\nsignificantly improve NER quality and that fine-tuning generally outperforms\nthe feature-based approaches. We present recommendations for parameters as well\nas several new state-of-the-art numbers. Our approach is integrated into the\nFlair framework to facilitate reproduction of our experiments.",
        "arxiv_firstAuthor": "Stefan Schweter",
        "arxiv_title": "FLERT: Document-Level Features for Named Entity Recognition",
        "arxiv_num": "2011.06993",
        "arxiv_published": "2020-11-13T16:13:59Z",
        "arxiv_updated": "2020-11-13T16:13:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1703.03129",
        "tag": [
            "http://www.semanlink.net/tag/k_nearest_neighbors_algorithm",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/rare_events",
            "http://www.semanlink.net/tag/google_brain",
            "http://www.semanlink.net/tag/memory_in_deep_learning",
            "http://www.semanlink.net/tag/few_shot_learning"
        ],
        "comment": "> a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. \r\n> Our memory module can be easily added to any part of a supervised neural network",
        "title": "[1703.03129] Learning to Remember Rare Events",
        "relatedDoc": [],
        "creationTime": "2018-10-23T12:36:58Z",
        "creationDate": "2018-10-23",
        "bookmarkOf": [],
        "arxiv_author": [
            "Aurko Roy",
            "Ofir Nachum",
            "\u0141ukasz Kaiser",
            "Samy Bengio"
        ],
        "arxiv_summary": "Despite recent advances, memory-augmented deep neural networks are still\nlimited when it comes to life-long and one-shot learning, especially in\nremembering rare events. We present a large-scale life-long memory module for\nuse in deep learning. The module exploits fast nearest-neighbor algorithms for\nefficiency and thus scales to large memory sizes. Except for the\nnearest-neighbor query, the module is fully differentiable and trained\nend-to-end with no extra supervision. It operates in a life-long manner, i.e.,\nwithout the need to reset it during training.\nOur memory module can be easily added to any part of a supervised neural\nnetwork. To show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models. In all cases, the enhanced network gains\nthe ability to remember and do life-long one-shot learning. Our module\nremembers training examples shown many thousands of steps in the past and it\ncan successfully generalize from them. We set new state-of-the-art for one-shot\nlearning on the Omniglot dataset and demonstrate, for the first time, life-long\none-shot learning in recurrent neural networks on a large-scale machine\ntranslation task.",
        "arxiv_firstAuthor": "\u0141ukasz Kaiser",
        "arxiv_title": "Learning to Remember Rare Events",
        "arxiv_num": "1703.03129",
        "arxiv_published": "2017-03-09T04:36:15Z",
        "arxiv_updated": "2017-03-09T04:36:15Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/2009_00318_more_is_not_always",
        "tag": [
            "http://www.semanlink.net/tag/knowledge_graph_completion",
            "http://www.semanlink.net/tag/rdf2vec",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> we argue that despite the huge body of work devoted on completing missing information in knowledge graphs, such missing implicit information is actually a signal, not a defect",
        "title": "[2009.00318] More is not Always Better: The Negative Impact of A-box Materialization on RDF2vec Knowledge Graph Embeddings",
        "relatedDoc": [],
        "creationTime": "2020-09-02T16:52:32Z",
        "creationDate": "2020-09-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/2009.00318"
        ],
        "arxiv_author": [
            "Andreea Iana",
            "Heiko Paulheim"
        ],
        "arxiv_summary": "RDF2vec is an embedding technique for representing knowledge graph entities\nin a continuous vector space. In this paper, we investigate the effect of\nmaterializing implicit A-box axioms induced by subproperties, as well as\nsymmetric and transitive properties. While it might be a reasonable assumption\nthat such a materialization before computing embeddings might lead to better\nembeddings, we conduct a set of experiments on DBpedia which demonstrate that\nthe materialization actually has a negative effect on the performance of\nRDF2vec. In our analysis, we argue that despite the huge body of work devoted\non completing missing information in knowledge graphs, such missing implicit\ninformation is actually a signal, not a defect, and we show examples\nillustrating that assumption.",
        "arxiv_firstAuthor": "Andreea Iana",
        "arxiv_title": "More is not Always Better: The Negative Impact of A-box Materialization on RDF2vec Knowledge Graph Embeddings",
        "arxiv_num": "2009.00318",
        "arxiv_published": "2020-09-01T09:52:33Z",
        "arxiv_updated": "2020-09-01T09:52:33Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1810.00438",
        "tag": [
            "http://www.semanlink.net/tag/good",
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft"
        ],
        "comment": "**training-free approach for building sentence representations**, \"Geometric Embedding\" (GEM), based on the **geometric structure** of word embedding space.\r\n\r\n> we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. **We model the semantic meaning of a word in a sentence** based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word\u2019s novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace\r\n\r\n[on www.groundai.com](https://www.groundai.com/project/zero-training-sentence-embedding-via-orthogonal-basis/)\r\n\r\n[Open Revieww](/doc/?uri=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DrJedbn0ctQ) ; [Related to this paper](/doc/?uri=https%3A%2F%2Farxiv.org%2Fabs%2F1704.05358)\r\n\r\n\r\n",
        "title": "[1810.00438] Parameter-free Sentence Embedding via Orthogonal Basis",
        "relatedDoc": [
            "https://arxiv.org/abs/1704.05358",
            "https://openreview.net/forum?id=rJedbn0ctQ"
        ],
        "creationTime": "2018-10-06T18:01:18Z",
        "creationDate": "2018-10-06",
        "bookmarkOf": [],
        "arxiv_author": [
            "Chenguang Zhu",
            "Weizhu Chen",
            "Ziyi Yang"
        ],
        "arxiv_summary": "We propose a simple and robust non-parameterized approach for building\nsentence representations. Inspired by the Gram-Schmidt Process in geometric\ntheory, we build an orthogonal basis of the subspace spanned by a word and its\nsurrounding context in a sentence. We model the semantic meaning of a word in a\nsentence based on two aspects. One is its relatedness to the word vector\nsubspace already spanned by its contextual words. The other is the word's novel\nsemantic meaning which shall be introduced as a new basis vector perpendicular\nto this existing subspace. Following this motivation, we develop an innovative\nmethod based on orthogonal basis to combine pre-trained word embeddings into\nsentence representations. This approach requires zero parameters, along with\nefficient inference performance. We evaluate our approach on 11 downstream NLP\ntasks. Our model shows superior performance compared with non-parameterized\nalternatives and it is competitive to other approaches relying on either large\namounts of labelled data or prolonged training time.",
        "arxiv_firstAuthor": "Ziyi Yang",
        "arxiv_title": "Parameter-free Sentence Embedding via Orthogonal Basis",
        "arxiv_num": "1810.00438",
        "arxiv_published": "2018-09-30T18:26:30Z",
        "arxiv_updated": "2019-12-06T05:01:36Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1705.08039.pdf",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/these_irit_renault_biblio_initiale",
            "http://www.semanlink.net/tag/poincare_embeddings",
            "http://www.semanlink.net/tag/nlp_facebook"
        ],
        "comment": "> While complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space",
        "title": "[1705.08039] Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
        "relatedDoc": [],
        "creationTime": "2017-12-16T14:41:31Z",
        "creationDate": "2017-12-16",
        "bookmarkOf": [],
        "arxiv_author": [
            "Maximilian Nickel",
            "Douwe Kiela"
        ],
        "arxiv_summary": "Representation learning has become an invaluable approach for learning from\nsymbolic data such as text and graphs. However, while complex symbolic datasets\noften exhibit a latent hierarchical structure, state-of-the-art methods\ntypically learn embeddings in Euclidean vector spaces, which do not account for\nthis property. For this purpose, we introduce a new approach for learning\nhierarchical representations of symbolic data by embedding them into hyperbolic\nspace -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the\nunderlying hyperbolic geometry, this allows us to learn parsimonious\nrepresentations of symbolic data by simultaneously capturing hierarchy and\nsimilarity. We introduce an efficient algorithm to learn the embeddings based\non Riemannian optimization and show experimentally that Poincar\\'e embeddings\noutperform Euclidean embeddings significantly on data with latent hierarchies,\nboth in terms of representation capacity and in terms of generalization\nability.",
        "arxiv_firstAuthor": "Maximilian Nickel",
        "arxiv_title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
        "arxiv_num": "1705.08039",
        "arxiv_published": "2017-05-22T23:14:36Z",
        "arxiv_updated": "2017-05-26T17:40:55Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1807.06036",
        "tag": [
            "http://www.semanlink.net/tag/loosely_formatted_text",
            "http://www.semanlink.net/tag/entity_linking",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "a production system for entity disambiguation on messy tex, based\r\non probabilistic tokenization and context-dependent document embeddings\r\n\r\n\"Probabilistic tokenization\": uses the method described [here](/doc/2019/07/mining_quality_phrases_from_mas)",
        "title": "[1807.06036] Pangloss: Fast Entity Linking in Noisy Text Environments",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/07/mining_quality_phrases_from_mas"
        ],
        "creationTime": "2019-04-23T23:58:40Z",
        "creationDate": "2019-04-23",
        "bookmarkOf": [],
        "arxiv_author": [
            "Michael Conover",
            "Matthew Hayes",
            "Pete Skomoroch",
            "Scott Blackburn",
            "Sam Shah"
        ],
        "arxiv_summary": "Entity linking is the task of mapping potentially ambiguous terms in text to\ntheir constituent entities in a knowledge base like Wikipedia. This is useful\nfor organizing content, extracting structured data from textual documents, and\nin machine learning relevance applications like semantic search, knowledge\ngraph construction, and question answering. Traditionally, this work has\nfocused on text that has been well-formed, like news articles, but in common\nreal world datasets such as messaging, resumes, or short-form social media,\nnon-grammatical, loosely-structured text adds a new dimension to this problem.\nThis paper presents Pangloss, a production system for entity disambiguation\non noisy text. Pangloss combines a probabilistic linear-time key phrase\nidentification algorithm with a semantic similarity engine based on\ncontext-dependent document embeddings to achieve better than state-of-the-art\nresults (>5% in F1) compared to other research or commercially available\nsystems. In addition, Pangloss leverages a local embedded database with a\ntiered architecture to house its statistics and metadata, which allows rapid\ndisambiguation in streaming contexts and on-device disambiguation in low-memory\nenvironments such as mobile phones.",
        "arxiv_firstAuthor": "Michael Conover",
        "arxiv_title": "Pangloss: Fast Entity Linking in Noisy Text Environments",
        "arxiv_num": "1807.06036",
        "arxiv_published": "2018-07-16T18:04:08Z",
        "arxiv_updated": "2018-07-16T18:04:08Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/2006_09462_selective_question",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/uncertainty_in_deep_learning",
            "http://www.semanlink.net/tag/question_answering",
            "http://www.semanlink.net/tag/nlp_stanford"
        ],
        "comment": "**How you can get a QA model to abstain from answering when it doesn\u2019t know the answer.**\r\n\r\n> Abstention policies based solely on the model's softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely.",
        "title": "[2006.09462] Selective Question Answering under Domain Shift",
        "relatedDoc": [],
        "creationTime": "2020-06-30T10:59:53Z",
        "creationDate": "2020-06-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/2006.09462"
        ],
        "arxiv_author": [
            "Percy Liang",
            "Robin Jia",
            "Amita Kamath"
        ],
        "arxiv_summary": "To avoid giving wrong answers, question answering (QA) models need to know\nwhen to abstain from answering. Moreover, users often ask questions that\ndiverge from the model's training data, making errors more likely and thus\nabstention more critical. In this work, we propose the setting of selective\nquestion answering under domain shift, in which a QA model is tested on a\nmixture of in-domain and out-of-domain data, and must answer (i.e., not abstain\non) as many questions as possible while maintaining high accuracy. Abstention\npolicies based solely on the model's softmax probabilities fare poorly, since\nmodels are overconfident on out-of-domain inputs. Instead, we train a\ncalibrator to identify inputs on which the QA model errs, and abstain when it\npredicts an error is likely. Crucially, the calibrator benefits from observing\nthe model's behavior on out-of-domain data, even if from a different domain\nthan the test data. We combine this method with a SQuAD-trained QA model and\nevaluate on mixtures of SQuAD and five other QA datasets. Our method answers\n56% of questions while maintaining 80% accuracy; in contrast, directly using\nthe model's probabilities only answers 48% at 80% accuracy.",
        "arxiv_firstAuthor": "Amita Kamath",
        "arxiv_title": "Selective Question Answering under Domain Shift",
        "arxiv_num": "2006.09462",
        "arxiv_published": "2020-06-16T19:13:21Z",
        "arxiv_updated": "2020-06-16T19:13:21Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/1911_11506_word_class_embeddi",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_text_classification",
            "http://www.semanlink.net/tag/label_embedding"
        ],
        "comment": "> In supervised tasks such as multiclass\r\ntext classification (the focus of this article) it seems appealing to enhance word representations\r\nwith ad-hoc embeddings that encode task-specific information. We propose (supervised) word-class\r\nembeddings (WCEs), and show that, when concatenated to (unsupervised) pre-trained word embeddings,\r\nthey substantially facilitate the training of deep-learning models in multiclass classification by\r\ntopic.\r\n>\r\n> A differentiating aspect of our method is that it keeps the modelling of word-class interactions separate from the\r\noriginal word embedding. Word-class correlations are confined in a dedicated vector space, whose vectors enhance\r\n(by concatenation) the unsupervised representations. The net effect is an embedding matrix that is better suited to\r\nclassification, and imposes no restriction to the network architecture using it.\r\n\r\n[github](https://github.com/AlexMoreo/word-class-embeddings). Refers to [LEAM](doc:2020/02/joint_embedding_of_words_and_la) :\r\n\r\n> [in LEAM] Once words and labels are embedded in a common vector space, word-label\r\ncompatibility is measured via cosine similarity. Our method instead models these compatibilities directly, without\r\ngenerating intermediate embeddings for words or labels.",
        "title": "[1911.11506] Word-Class Embeddings for Multiclass Text Classification",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/02/joint_embedding_of_words_and_la"
        ],
        "creationTime": "2020-10-11T19:29:28Z",
        "creationDate": "2020-10-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.11506"
        ],
        "arxiv_author": [
            "Fabrizio Sebastiani",
            "Alejandro Moreo",
            "Andrea Esuli"
        ],
        "arxiv_summary": "Pre-trained word embeddings encode general word semantics and lexical\nregularities of natural language, and have proven useful across many NLP tasks,\nincluding word sense disambiguation, machine translation, and sentiment\nanalysis, to name a few. In supervised tasks such as multiclass text\nclassification (the focus of this article) it seems appealing to enhance word\nrepresentations with ad-hoc embeddings that encode task-specific information.\nWe propose (supervised) word-class embeddings (WCEs), and show that, when\nconcatenated to (unsupervised) pre-trained word embeddings, they substantially\nfacilitate the training of deep-learning models in multiclass classification by\ntopic. We show empirical evidence that WCEs yield a consistent improvement in\nmulticlass classification accuracy, using four popular neural architectures and\nsix widely used and publicly available datasets for multiclass text\nclassification. Our code that implements WCEs is publicly available at\nhttps://github.com/AlexMoreo/word-class-embeddings",
        "arxiv_firstAuthor": "Alejandro Moreo",
        "arxiv_title": "Word-Class Embeddings for Multiclass Text Classification",
        "arxiv_num": "1911.11506",
        "arxiv_published": "2019-11-26T13:11:00Z",
        "arxiv_updated": "2019-11-26T13:11:00Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/1906_05685_a_focus_on_neural_",
        "tag": [
            "http://www.semanlink.net/tag/nlp_4_africa",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/neural_machine_translation"
        ],
        "comment": "",
        "title": "[1906.05685] A Focus on Neural Machine Translation for African Languages",
        "relatedDoc": [],
        "creationTime": "2021-06-30T01:03:36Z",
        "creationDate": "2021-06-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.05685"
        ],
        "arxiv_author": [
            "Laura Martinus",
            "Jade Z. Abbott"
        ],
        "arxiv_summary": "African languages are numerous, complex and low-resourced. The datasets\nrequired for machine translation are difficult to discover, and existing\nresearch is hard to reproduce. Minimal attention has been given to machine\ntranslation for African languages so there is scant research regarding the\nproblems that arise when using machine translation techniques. To begin\naddressing these problems, we trained models to translate English to five of\nthe official South African languages (Afrikaans, isiZulu, Northern Sotho,\nSetswana, Xitsonga), making use of modern neural machine translation\ntechniques. The results obtained show the promise of using neural machine\ntranslation techniques for African languages. By providing reproducible\npublicly-available data, code and results, this research aims to provide a\nstarting point for other researchers in African machine translation to compare\nto and build upon.",
        "arxiv_firstAuthor": "Laura Martinus",
        "arxiv_title": "A Focus on Neural Machine Translation for African Languages",
        "arxiv_num": "1906.05685",
        "arxiv_published": "2019-06-11T15:38:34Z",
        "arxiv_updated": "2019-06-14T12:48:25Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1809.00782",
        "tag": [
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/open_domain_question_answering",
            "http://www.semanlink.net/tag/ruslan_salakhutdinov",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/kg_and_nlp",
            "http://www.semanlink.net/tag/emnlp_2018",
            "http://www.semanlink.net/tag/these_irit_renault_biblio"
        ],
        "comment": "QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus.\r\n\r\n> In practice, some questions are best answered\r\nusing text, while others are best answered using\r\nKBs. A natural question, then, is how to effectively\r\ncombine both types of information. Surprisingly\r\nlittle prior work has looked at this problem.",
        "title": "[1809.00782] Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
        "relatedDoc": [],
        "creationTime": "2018-09-06T01:38:28Z",
        "creationDate": "2018-09-06",
        "bookmarkOf": [],
        "arxiv_author": [
            "Haitian Sun",
            "Bhuwan Dhingra",
            "Kathryn Mazaitis",
            "Manzil Zaheer",
            "William W. Cohen",
            "Ruslan Salakhutdinov"
        ],
        "arxiv_summary": "Open Domain Question Answering (QA) is evolving from complex pipelined\nsystems to end-to-end deep neural networks. Specialized neural models have been\ndeveloped for extracting answers from either text alone or Knowledge Bases\n(KBs) alone. In this paper we look at a more practical setting, namely QA over\nthe combination of a KB and entity-linked text, which is appropriate when an\nincomplete KB is available with a large text corpus. Building on recent\nadvances in graph representation learning we propose a novel model, GRAFT-Net,\nfor extracting answers from a question-specific subgraph containing text and KB\nentities and relations. We construct a suite of benchmark tasks for this\nproblem, varying the difficulty of questions, the amount of training data, and\nKB completeness. We show that GRAFT-Net is competitive with the\nstate-of-the-art when tested using either KBs or text alone, and vastly\noutperforms existing methods in the combined setting. Source code is available\nat https://github.com/OceanskySun/GraftNet .",
        "arxiv_firstAuthor": "Haitian Sun",
        "arxiv_title": "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
        "arxiv_num": "1809.00782",
        "arxiv_published": "2018-09-04T03:15:56Z",
        "arxiv_updated": "2018-09-04T03:15:56Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/1405_5893_computerization_of_",
        "tag": [
            "http://www.semanlink.net/tag/african_languages",
            "http://www.semanlink.net/tag/nlp_4_africa",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/haoussa",
            "http://www.semanlink.net/tag/jerma",
            "http://www.semanlink.net/tag/songhai"
        ],
        "comment": "This paper relates work done during the DiLAF project. It consists in converting 5 bilingual African language-French dictionaries originally in Word format into XML following the LMF model. The languages processed are Bambara, Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced languages concerning Natural Language Processing tools.",
        "title": "[1405.5893] Computerization of African languages-French dictionaries",
        "relatedDoc": [],
        "creationTime": "2021-06-30T00:33:09Z",
        "creationDate": "2021-06-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/1405.5893"
        ],
        "arxiv_author": [
            "Mathieu Mangeot LIG",
            "Chantal Enguehard LINA"
        ],
        "arxiv_summary": "This paper relates work done during the DiLAF project. It consists in\nconverting 5 bilingual African language-French dictionaries originally in Word\nformat into XML following the LMF model. The languages processed are Bambara,\nHausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced\nlanguages concerning Natural Language Processing tools. Once converted, the\ndictionaries are available online on the Jibiki platform for lookup and\nmodification. The DiLAF project is first presented. A description of each\ndictionary follows. Then, the conversion methodology from .doc format to XML\nfiles is presented. A specific point on the usage of Unicode follows. Then,\neach step of the conversion into XML and LMF is detailed. The last part\npresents the Jibiki lexical resources management platform used for the project.",
        "arxiv_firstAuthor": "Chantal Enguehard LINA",
        "arxiv_title": "Computerization of African languages-French dictionaries",
        "arxiv_num": "1405.5893",
        "arxiv_published": "2014-05-22T20:15:57Z",
        "arxiv_updated": "2014-05-22T20:15:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/10/_1802_07044_the_description_le",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/facebook_fair",
            "http://www.semanlink.net/tag/overfitting",
            "http://www.semanlink.net/tag/nlp_ens",
            "http://www.semanlink.net/tag/occam_s_razor",
            "http://www.semanlink.net/tag/minimum_description_length_principle",
            "http://www.semanlink.net/tag/dl_why_does_it_work",
            "http://www.semanlink.net/tag/information_theory_and_deep_learning"
        ],
        "comment": "> Solomonoff\u2019s general theory of inference (Solomonoff, 1964) and the [Minimum Description Length Principle](tag:minimum_description_length_principle) (Gr\u00fcnwald, 2007; Rissanen, 2007) formalize [Occam's razor](tag:occam_s_razor), and hold that **a good model of data is a model that is good at losslessly\r\ncompressing the data, including the cost of describing the model itself**. Deep neural\r\nnetworks might seem to go against this principle given the large number of\r\nparameters to be encoded.\r\nWe demonstrate experimentally the ability of deep neural networks to compress\r\nthe training data even when accounting for parameter encoding.",
        "title": "[1802.07044] The Description Length of Deep Learning Models",
        "relatedDoc": [],
        "creationTime": "2019-10-11T01:59:35Z",
        "creationDate": "2019-10-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1802.07044"
        ],
        "arxiv_author": [
            "Yann Ollivier",
            "L\u00e9onard Blier"
        ],
        "arxiv_summary": "Solomonoff's general theory of inference and the Minimum Description Length\nprinciple formalize Occam's razor, and hold that a good model of data is a\nmodel that is good at losslessly compressing the data, including the cost of\ndescribing the model itself. Deep neural networks might seem to go against this\nprinciple given the large number of parameters to be encoded.\nWe demonstrate experimentally the ability of deep neural networks to compress\nthe training data even when accounting for parameter encoding. The compression\nviewpoint originally motivated the use of variational methods in neural\nnetworks. Unexpectedly, we found that these variational methods provide\nsurprisingly poor compression bounds, despite being explicitly built to\nminimize such bounds. This might explain the relatively poor practical\nperformance of variational methods in deep learning. On the other hand, simple\nincremental encoding methods yield excellent compression values on deep\nnetworks, vindicating Solomonoff's approach.",
        "arxiv_firstAuthor": "L\u00e9onard Blier",
        "arxiv_title": "The Description Length of Deep Learning Models",
        "arxiv_num": "1802.07044",
        "arxiv_published": "2018-02-20T10:15:26Z",
        "arxiv_updated": "2018-11-01T11:23:09Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/1910_00163_specializing_word_",
        "tag": [
            "http://www.semanlink.net/tag/information_bottleneck_method",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/emnlp_2019"
        ],
        "comment": "EMNLP best paper award. [Related blog post](doc:2020/06/information_bottleneck_for_nlp_)",
        "title": "[1910.00163] Specializing Word Embeddings (for Parsing) by Information Bottleneck",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/06/information_bottleneck_for_nlp_"
        ],
        "creationTime": "2020-06-29T10:08:09Z",
        "creationDate": "2020-06-29",
        "bookmarkOf": [
            "https://www.aclweb.org/anthology/D19-1276/",
            "https://arxiv.org/abs/1910.00163"
        ],
        "arxiv_author": [
            "Xiang Lisa Li",
            "Jason Eisner"
        ],
        "arxiv_summary": "Pre-trained word embeddings like ELMo and BERT contain rich syntactic and\nsemantic information, resulting in state-of-the-art performance on various\ntasks. We propose a very fast variational information bottleneck (VIB) method\nto nonlinearly compress these embeddings, keeping only the information that\nhelps a discriminative parser. We compress each word embedding to either a\ndiscrete tag or a continuous vector. In the discrete version, our automatically\ncompressed tags form an alternative tag set: we show experimentally that our\ntags capture most of the information in traditional POS tag annotations, but\nour tag sequences can be parsed more accurately at the same level of tag\ngranularity. In the continuous version, we show experimentally that moderately\ncompressing the word embeddings by our method yields a more accurate parser in\n8 of 9 languages, unlike simple dimensionality reduction.",
        "arxiv_firstAuthor": "Xiang Lisa Li",
        "arxiv_title": "Specializing Word Embeddings (for Parsing) by Information Bottleneck",
        "arxiv_num": "1910.00163",
        "arxiv_published": "2019-10-01T00:47:31Z",
        "arxiv_updated": "2019-10-01T00:47:31Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1906_07241_barack_s_wife_hill",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/kg_and_nlp",
            "http://www.semanlink.net/tag/kd_mkb_biblio",
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/language_model"
        ],
        "comment": "> a **neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context**. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens.",
        "title": "[1906.07241] Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling",
        "relatedDoc": [],
        "creationTime": "2020-05-11T18:55:35Z",
        "creationDate": "2020-05-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.07241"
        ],
        "arxiv_author": [
            "Sameer Singh",
            "Matt Gardner",
            "Matthew E. Peters",
            "Robert L. Logan IV",
            "Nelson F. Liu"
        ],
        "arxiv_summary": "Modeling human language requires the ability to not only generate fluent text\nbut also encode factual knowledge. However, traditional language models are\nonly capable of remembering facts seen at training time, and often have\ndifficulty recalling them. To address this, we introduce the knowledge graph\nlanguage model (KGLM), a neural language model with mechanisms for selecting\nand copying facts from a knowledge graph that are relevant to the context.\nThese mechanisms enable the model to render information it has never seen\nbefore, as well as generate out-of-vocabulary tokens. We also introduce the\nLinked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata\nknowledge graph whose contents (roughly) match the popular WikiText-2\nbenchmark. In experiments, we demonstrate that the KGLM achieves significantly\nbetter performance than a strong baseline language model. We additionally\ncompare different language model's ability to complete sentences requiring\nfactual knowledge, showing that the KGLM outperforms even very large language\nmodels in generating facts.",
        "arxiv_firstAuthor": "Robert L. Logan IV",
        "arxiv_title": "Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling",
        "arxiv_num": "1906.07241",
        "arxiv_published": "2019-06-17T19:48:41Z",
        "arxiv_updated": "2019-06-20T18:37:00Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/05/_1709_07604_a_comprehensive_su",
        "tag": [
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1709.07604] A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications",
        "relatedDoc": [],
        "creationTime": "2019-05-29T17:26:26Z",
        "creationDate": "2019-05-29",
        "bookmarkOf": [
            "https://arxiv.org/abs/1709.07604"
        ],
        "arxiv_author": [
            "Vincent W. Zheng",
            "Kevin Chen-Chuan Chang",
            "Hongyun Cai"
        ],
        "arxiv_summary": "Graph is an important data representation which appears in a wide diversity\nof real-world scenarios. Effective graph analytics provides users a deeper\nunderstanding of what is behind the data, and thus can benefit a lot of useful\napplications such as node classification, node recommendation, link prediction,\netc. However, most graph analytics methods suffer the high computation and\nspace cost. Graph embedding is an effective yet efficient way to solve the\ngraph analytics problem. It converts the graph data into a low dimensional\nspace in which the graph structural information and graph properties are\nmaximally preserved. In this survey, we conduct a comprehensive review of the\nliterature in graph embedding. We first introduce the formal definition of\ngraph embedding as well as the related concepts. After that, we propose two\ntaxonomies of graph embedding which correspond to what challenges exist in\ndifferent graph embedding problem settings and how the existing work address\nthese challenges in their solutions. Finally, we summarize the applications\nthat graph embedding enables and suggest four promising future research\ndirections in terms of computation efficiency, problem settings, techniques and\napplication scenarios.",
        "arxiv_firstAuthor": "Hongyun Cai",
        "arxiv_title": "A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications",
        "arxiv_num": "1709.07604",
        "arxiv_published": "2017-09-22T05:54:16Z",
        "arxiv_updated": "2018-02-02T07:01:22Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_1905_06088_neural_symbolic_co",
        "tag": [
            "http://www.semanlink.net/tag/neural_symbolic_computing",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",
        "relatedDoc": [],
        "creationTime": "2020-03-15T11:06:28Z",
        "creationDate": "2020-03-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/1905.06088"
        ],
        "arxiv_author": [
            "Artur d'Avila Garcez",
            "Michael Spranger",
            "Luciano Serafini",
            "Luis C. Lamb",
            "Marco Gori",
            "Son N. Tran"
        ],
        "arxiv_summary": "Current advances in Artificial Intelligence and machine learning in general,\nand deep learning in particular have reached unprecedented impact not only\nacross research communities, but also over popular media channels. However,\nconcerns about interpretability and accountability of AI have been raised by\ninfluential thinkers. In spite of the recent impact of AI, several works have\nidentified the need for principled knowledge representation and reasoning\nmechanisms integrated with deep learning-based systems to provide sound and\nexplainable models for such systems. Neural-symbolic computing aims at\nintegrating, as foreseen by Valiant, two most fundamental cognitive abilities:\nthe ability to learn from the environment, and the ability to reason from what\nhas been learned. Neural-symbolic computing has been an active topic of\nresearch for many years, reconciling the advantages of robust learning in\nneural networks and reasoning and interpretability of symbolic representation.\nIn this paper, we survey recent accomplishments of neural-symbolic computing as\na principled methodology for integrated machine learning and reasoning. We\nillustrate the effectiveness of the approach by outlining the main\ncharacteristics of the methodology: principled integration of neural learning\nwith symbolic knowledge representation and reasoning allowing for the\nconstruction of explainable AI systems. The insights provided by\nneural-symbolic computing shed new light on the increasingly prominent need for\ninterpretable and accountable AI systems.",
        "arxiv_firstAuthor": "Artur d'Avila Garcez",
        "arxiv_title": "Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",
        "arxiv_num": "1905.06088",
        "arxiv_published": "2019-05-15T11:00:48Z",
        "arxiv_updated": "2019-05-15T11:00:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/07/_1907_07355_probing_neural_net",
        "tag": [
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bertology"
        ],
        "comment": "what has BERT learned about argument comprehension?\r\n\r\n[Comments](/doc/2019/07/bert_s_success_in_some_benchmar)",
        "title": "[1907.07355] Probing Neural Network Comprehension of Natural Language Arguments",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2019/07/bert_s_success_in_some_benchmar"
        ],
        "creationTime": "2019-07-24T01:34:54Z",
        "creationDate": "2019-07-24",
        "bookmarkOf": [
            "https://arxiv.org/abs/1907.07355"
        ],
        "arxiv_author": [
            "Hung-Yu Kao",
            "Timothy Niven"
        ],
        "arxiv_summary": "We are surprised to find that BERT's peak performance of 77% on the Argument\nReasoning Comprehension Task reaches just three points below the average\nuntrained human baseline. However, we show that this result is entirely\naccounted for by exploitation of spurious statistical cues in the dataset. We\nanalyze the nature of these cues and demonstrate that a range of models all\nexploit them. This analysis informs the construction of an adversarial dataset\non which all models achieve random accuracy. Our adversarial dataset provides a\nmore robust assessment of argument comprehension and should be adopted as the\nstandard in future work.",
        "arxiv_firstAuthor": "Timothy Niven",
        "arxiv_title": "Probing Neural Network Comprehension of Natural Language Arguments",
        "arxiv_num": "1907.07355",
        "arxiv_published": "2019-07-17T06:26:20Z",
        "arxiv_updated": "2019-09-16T04:07:54Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1601.01343",
        "tag": [
            "http://www.semanlink.net/tag/entity_linking",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/combining_word_and_entity_embeddings",
            "http://www.semanlink.net/tag/these_irit_renault_biblio_initiale",
            "http://www.semanlink.net/tag/ikuya_yamada",
            "http://www.semanlink.net/tag/wikipedia2vec",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": ">  An embedding method specifically **designed for NED** that jointly **maps words and entities into the same continuous vector space**. \r\n> We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words\r\n\r\nTechnique later used in [Wikipedia2Vec](doc:?uri=https%3A%2F%2Fwikipedia2vec.github.io%2Fwikipedia2vec%2F), by the same team. [Neural Attentive Bag-of-Entities Model for Text Classification](https://arxiv.org/abs/1909.01259) uses Wikipedia2Vec model.",
        "title": "[1601.01343] Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/09/1909_01259_neural_attentive_b",
            "https://wikipedia2vec.github.io/wikipedia2vec/"
        ],
        "creationTime": "2019-01-27T15:29:16Z",
        "creationDate": "2019-01-27",
        "bookmarkOf": [],
        "arxiv_author": [
            "Hideaki Takeda",
            "Ikuya Yamada",
            "Hiroyuki Shindo",
            "Yoshiyasu Takefuji"
        ],
        "arxiv_summary": "Named Entity Disambiguation (NED) refers to the task of resolving multiple\nnamed entity mentions in a document to their correct references in a knowledge\nbase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method\nspecifically designed for NED. The proposed method jointly maps words and\nentities into the same continuous vector space. We extend the skip-gram model\nby using two models. The KB graph model learns the relatedness of entities\nusing the link structure of the KB, whereas the anchor context model aims to\nalign vectors such that similar words and entities occur close to one another\nin the vector space by leveraging KB anchors and their context words. By\ncombining contexts based on the proposed embedding with standard NED features,\nwe achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset\nand 85.2% on the TAC 2010 dataset.",
        "arxiv_firstAuthor": "Ikuya Yamada",
        "arxiv_title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation",
        "arxiv_num": "1601.01343",
        "arxiv_published": "2016-01-06T22:19:20Z",
        "arxiv_updated": "2016-06-10T01:51:26Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/1906_03158_matching_the_blank",
        "tag": [
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/relation_learning",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> a new method\r\nof learning relation representations directly from\r\ntext\r\n>\r\n> First, we study the **ability of the Transformer\r\nneural network architecture (Vaswani et al., 2017)\r\nto encode relations between entity pairs**, and we\r\nidentify a method of representation that outperforms\r\nprevious work in supervised relation extraction.\r\nThen, we present a method of training this relation\r\nrepresentation **without any supervision from\r\na knowledge graph or human annotators** from widely available distant supervision\r\nin the form of entity linked text\r\n>\r\n> **we assume** access\r\nto a corpus of text in which entities have been\r\nlinked to unique identifiers and we define a relation statement to be a block of text containing two\r\nmarked entities.",
        "title": "[1906.03158] Matching the Blanks: Distributional Similarity for Relation Learning",
        "relatedDoc": [],
        "creationTime": "2021-05-13T00:39:03Z",
        "creationDate": "2021-05-13",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.03158"
        ],
        "arxiv_author": [
            "Nicholas FitzGerald",
            "Livio Baldini Soares",
            "Tom Kwiatkowski",
            "Jeffrey Ling"
        ],
        "arxiv_summary": "General purpose relation extractors, which can model arbitrary relations, are\na core aspiration in information extraction. Efforts have been made to build\ngeneral purpose extractors that represent relations with their surface forms,\nor which jointly embed surface forms with relations from an existing knowledge\ngraph. However, both of these approaches are limited in their ability to\ngeneralize. In this paper, we build on extensions of Harris' distributional\nhypothesis to relations, as well as recent advances in learning text\nrepresentations (specifically, BERT), to build task agnostic relation\nrepresentations solely from entity-linked text. We show that these\nrepresentations significantly outperform previous work on exemplar based\nrelation extraction (FewRel) even without using any of that task's training\ndata. We also show that models initialized with our task agnostic\nrepresentations, and then tuned on supervised relation extraction datasets,\nsignificantly outperform the previous methods on SemEval 2010 Task 8, KBP37,\nand TACRED.",
        "arxiv_firstAuthor": "Livio Baldini Soares",
        "arxiv_title": "Matching the Blanks: Distributional Similarity for Relation Learning",
        "arxiv_num": "1906.03158",
        "arxiv_published": "2019-06-07T15:26:50Z",
        "arxiv_updated": "2019-06-07T15:26:50Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/05/2104_10809_provable_limitatio",
        "tag": [
            "http://www.semanlink.net/tag/yoav_goldberg",
            "http://www.semanlink.net/tag/grounded_language_learning",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2104.10809] Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?",
        "relatedDoc": [],
        "creationTime": "2021-05-23T01:20:07Z",
        "creationDate": "2021-05-23",
        "bookmarkOf": [
            "https://arxiv.org/abs/2104.10809"
        ],
        "arxiv_author": [
            "William Merrill",
            "Roy Schwartz",
            "Yoav Goldberg",
            "Noah A. Smith"
        ],
        "arxiv_summary": "Language models trained on billions of tokens have recently led to\nunprecedented results on many NLP tasks. This success raises the question of\nwhether, in principle, a system can ever \"understand\" raw text without access\nto some form of grounding. We formally investigate the abilities of ungrounded\nsystems to acquire meaning. Our analysis focuses on the role of \"assertions\":\ncontexts within raw text that provide indirect clues about underlying\nsemantics. We study whether assertions enable a system to emulate\nrepresentations preserving semantic relations like equivalence. We find that\nassertions enable semantic emulation if all expressions in the language are\nreferentially transparent. However, if the language uses non-transparent\npatterns like variable binding, we show that emulation can become an\nuncomputable problem. Finally, we discuss differences between our formal model\nand natural language, exploring how our results generalize to a modal setting\nand other semantic relations. Together, our results suggest that assertions in\ncode or language do not provide sufficient signal to fully emulate semantic\nrepresentations. We formalize ways in which ungrounded language models appear\nto be fundamentally limited in their ability to \"understand\".",
        "arxiv_firstAuthor": "William Merrill",
        "arxiv_title": "Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?",
        "arxiv_num": "2104.10809",
        "arxiv_published": "2021-04-22T01:00:17Z",
        "arxiv_updated": "2021-04-22T01:00:17Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1412.1897v4.pdf",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/artificial_neural_network",
            "http://www.semanlink.net/tag/image_recognition"
        ],
        "comment": "",
        "title": "[1412.1897] Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
        "relatedDoc": [],
        "creationTime": "2017-08-24T00:47:56Z",
        "creationDate": "2017-08-24",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jason Yosinski",
            "Anh Nguyen",
            "Jeff Clune"
        ],
        "arxiv_summary": "Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision.",
        "arxiv_firstAuthor": "Anh Nguyen",
        "arxiv_title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
        "arxiv_num": "1412.1897",
        "arxiv_published": "2014-12-05T05:29:43Z",
        "arxiv_updated": "2015-04-02T23:12:56Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1405.4053",
        "tag": [
            "http://www.semanlink.net/tag/tomas_mikolov",
            "http://www.semanlink.net/tag/doc2vec",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Paragraph Vector: an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.Represents each document by a dense vector which is trained to predict words in the document. Overcomes the weaknesses of the [Bag Of Words](/tag/bag_of_words) model (order of words, semantic of words)\r\n\r\n\r\n",
        "title": "[1405.4053] Distributed Representations of Sentences and Documents",
        "relatedDoc": [],
        "creationTime": "2017-07-10T16:20:03Z",
        "creationDate": "2017-07-10",
        "bookmarkOf": [],
        "arxiv_author": [
            "Tomas Mikolov",
            "Quoc V. Le"
        ],
        "arxiv_summary": "Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.",
        "arxiv_firstAuthor": "Quoc V. Le",
        "arxiv_title": "Distributed Representations of Sentences and Documents",
        "arxiv_num": "1405.4053",
        "arxiv_published": "2014-05-16T07:12:16Z",
        "arxiv_updated": "2014-05-22T23:23:19Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1801.00631",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning",
            "http://www.semanlink.net/tag/nn_symbolic_ai_hybridation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ia_limites"
        ],
        "comment": "",
        "title": "[1801.00631] Deep Learning: A Critical Appraisal",
        "relatedDoc": [],
        "creationTime": "2018-01-03T11:33:53Z",
        "creationDate": "2018-01-03",
        "bookmarkOf": [],
        "arxiv_author": [
            "Gary Marcus"
        ],
        "arxiv_summary": "Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.",
        "arxiv_firstAuthor": "Gary Marcus",
        "arxiv_title": "Deep Learning: A Critical Appraisal",
        "arxiv_num": "1801.00631",
        "arxiv_published": "2018-01-02T12:49:35Z",
        "arxiv_updated": "2018-01-02T12:49:35Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1707_00306_variable_selection",
        "tag": [
            "http://www.semanlink.net/tag/cluster_analysis",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1707.00306] Variable Selection Methods for Model-based Clustering",
        "relatedDoc": [],
        "creationTime": "2019-12-11T03:15:56Z",
        "creationDate": "2019-12-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1707.00306"
        ],
        "arxiv_author": [
            "Thomas Brendan Murphy",
            "Michael Fop"
        ],
        "arxiv_summary": "Model-based clustering is a popular approach for clustering multivariate data\nwhich has seen applications in numerous fields. Nowadays, high-dimensional data\nare more and more common and the model-based clustering approach has adapted to\ndeal with the increasing dimensionality. In particular, the development of\nvariable selection techniques has received a lot of attention and research\neffort in recent years. Even for small size problems, variable selection has\nbeen advocated to facilitate the interpretation of the clustering results. This\nreview provides a summary of the methods developed for variable selection in\nmodel-based clustering. Existing R packages implementing the different methods\nare indicated and illustrated in application to two data analysis examples.",
        "arxiv_firstAuthor": "Michael Fop",
        "arxiv_title": "Variable Selection Methods for Model-based Clustering",
        "arxiv_num": "1707.00306",
        "arxiv_published": "2017-07-02T15:29:13Z",
        "arxiv_updated": "2018-06-04T07:52:56Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1909_01259_neural_attentive_b",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ikuya_yamada",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/nlp_text_classification",
            "http://www.semanlink.net/tag/wikipedia2vec",
            "http://www.semanlink.net/tag/entities",
            "http://www.semanlink.net/tag/entity_salience",
            "http://www.semanlink.net/tag/good"
        ],
        "comment": "A model that performs **text classification using entities in a knowledge base**.\r\n\r\n> Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts. We combine **simple high-recall entity detection based on a dictionary** (word->list of entities), to detect entities in a document, with a novel neural **attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities**. \r\n\r\n2 steps:\r\n\r\n1. Entity detection\r\n2. Classification using the detected entities (+text) as inputs\r\n\r\nRegarding entity linking, a local model which uses cosine\r\nsimilarity between the embedding of the target\r\nentity and the word-based representation of\r\nthe document to capture the relevance of an entity\r\ngiven a document.\r\n\r\nEmbeddings from the KB: computed using [#Wikipedia2Vec](tag:wikipedia2vec) (similar words and entities\r\nclose to one another in a unified vector space)\r\n\r\nModel using attention, with 2 features :\r\n\r\n- cosine similarity between the\r\nembedding of the entity and the word based\r\nrepresentation of the document\r\n- the probability that the entity\r\nname refers to the entity in KB.\r\n\r\nSomewhat [related](doc:2020/01/investigating_entity_knowledge_)",
        "title": "[1909.01259] Neural Attentive Bag-of-Entities Model for Text Classification",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/01/investigating_entity_knowledge_"
        ],
        "creationTime": "2020-09-02T16:46:43Z",
        "creationDate": "2020-09-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.01259"
        ],
        "arxiv_author": [
            "Ikuya Yamada",
            "Hiroyuki Shindo"
        ],
        "arxiv_summary": "This study proposes a Neural Attentive Bag-of-Entities model, which is a\nneural network model that performs text classification using entities in a\nknowledge base. Entities provide unambiguous and relevant semantic signals that\nare beneficial for capturing semantics in texts. We combine simple high-recall\nentity detection based on a dictionary, to detect entities in a document, with\na novel neural attention mechanism that enables the model to focus on a small\nnumber of unambiguous and relevant entities. We tested the effectiveness of our\nmodel using two standard text classification datasets (i.e., the 20 Newsgroups\nand R8 datasets) and a popular factoid question answering dataset based on a\ntrivia quiz game. As a result, our model achieved state-of-the-art results on\nall datasets. The source code of the proposed model is available online at\nhttps://github.com/wikipedia2vec/wikipedia2vec.",
        "arxiv_firstAuthor": "Ikuya Yamada",
        "arxiv_title": "Neural Attentive Bag-of-Entities Model for Text Classification",
        "arxiv_num": "1909.01259",
        "arxiv_published": "2019-09-03T15:50:34Z",
        "arxiv_updated": "2019-09-10T10:23:49Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/07/2010_06467_pretrained_transfo",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/neural_models_for_information_retrieval",
            "http://www.semanlink.net/tag/nlp_long_documents",
            "http://www.semanlink.net/tag/survey"
        ],
        "comment": "a 155 pages paper!",
        "title": "[2010.06467] Pretrained Transformers for Text Ranking: BERT and Beyond",
        "relatedDoc": [],
        "creationTime": "2021-07-09T14:50:44Z",
        "creationDate": "2021-07-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.06467"
        ],
        "arxiv_author": [
            "Rodrigo Nogueira",
            "Andrew Yates",
            "Jimmy Lin"
        ],
        "arxiv_summary": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has, without exaggeration, revolutionized the fields of natural\nlanguage processing (NLP), information retrieval (IR), and beyond. In this\nsurvey, we provide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\nranking architectures and learned dense representations that attempt to perform\nranking directly. There are two themes that pervade our survey: techniques for\nhandling long documents, beyond the typical sentence-by-sentence processing\napproaches used in NLP, and techniques for addressing the tradeoff between\neffectiveness (result quality) and efficiency (query latency). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
        "arxiv_firstAuthor": "Jimmy Lin",
        "arxiv_title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
        "arxiv_num": "2010.06467",
        "arxiv_published": "2020-10-13T15:20:32Z",
        "arxiv_updated": "2020-10-13T15:20:32Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1404.5367",
        "tag": [
            "http://www.semanlink.net/tag/named_entity_recognition",
            "http://www.semanlink.net/tag/phrase_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/andrew_mccallum",
            "http://www.semanlink.net/tag/word_embeddings_with_lexical_resources"
        ],
        "comment": "Employs lexicons as part of the word embedding training: \r\n\r\n> The skip-gram model can be trained to\r\npredict not only neighboring words but also lexicon\r\nmembership of the central word (or phrase).\r\n\r\nQuickly demonstrates how we can plug phrase embeddings\r\ninto an existing log-linear CRF System.\r\n\r\n",
        "title": "[1404.5367] Lexicon Infused Phrase Embeddings for Named Entity Resolution",
        "relatedDoc": [],
        "creationTime": "2018-05-22T16:22:37Z",
        "creationDate": "2018-05-22",
        "bookmarkOf": [],
        "arxiv_author": [
            "Andrew McCallum",
            "Alexandre Passos",
            "Vineet Kumar"
        ],
        "arxiv_summary": "Most state-of-the-art approaches for named-entity recognition (NER) use semi\nsupervised information in the form of word clusters and lexicons. Recently\nneural network-based language models have been explored, as they as a byproduct\ngenerate highly informative vector representations for words, known as word\nembeddings. In this paper we present two contributions: a new form of learning\nword embeddings that can leverage information from relevant lexicons to improve\nthe representations, and the first system to use neural word embeddings to\nachieve state-of-the-art results on named-entity recognition in both CoNLL and\nOntonotes NER. Our system achieves an F1 score of 90.90 on the test set for\nCoNLL 2003---significantly better than any previous system trained on public\ndata, and matching a system employing massive private industrial query-log\ndata.",
        "arxiv_firstAuthor": "Alexandre Passos",
        "arxiv_title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution",
        "arxiv_num": "1404.5367",
        "arxiv_published": "2014-04-22T02:12:06Z",
        "arxiv_updated": "2014-04-22T02:12:06Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1603.05106v1",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/one_shot_generalization",
            "http://www.semanlink.net/tag/machine_learning"
        ],
        "comment": "",
        "title": "[1603.05106] One-Shot Generalization in Deep Generative Models",
        "relatedDoc": [],
        "creationTime": "2016-03-18T00:02:19Z",
        "creationDate": "2016-03-18",
        "bookmarkOf": [],
        "arxiv_author": [
            "Daan Wierstra",
            "Danilo Jimenez Rezende",
            "Karol Gregor",
            "Shakir Mohamed",
            "Ivo Danihelka"
        ],
        "arxiv_summary": "Humans have an impressive ability to reason about new concepts and\nexperiences from just a single example. In particular, humans have an ability\nfor one-shot generalization: an ability to encounter a new concept, understand\nits structure, and then be able to generate compelling alternative variations\nof the concept. We develop machine learning systems with this important\ncapacity by developing new deep generative models, models that combine the\nrepresentational power of deep learning with the inferential power of Bayesian\nreasoning. We develop a class of sequential generative models that are built on\nthe principles of feedback and attention. These two characteristics lead to\ngenerative models that are among the state-of-the art in density estimation and\nimage generation. We demonstrate the one-shot generalization ability of our\nmodels using three tasks: unconditional sampling, generating new exemplars of a\ngiven concept, and generating new exemplars of a family of concepts. In all\ncases our models are able to generate compelling and diverse samples---having\nseen new examples just once---providing an important class of general-purpose\nmodels for one-shot machine learning.",
        "arxiv_firstAuthor": "Danilo Jimenez Rezende",
        "arxiv_title": "One-Shot Generalization in Deep Generative Models",
        "arxiv_num": "1603.05106",
        "arxiv_published": "2016-03-16T14:10:00Z",
        "arxiv_updated": "2016-05-25T12:57:19Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_2002_12327_a_primer_in_bertol",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bertology",
            "http://www.semanlink.net/tag/bert"
        ],
        "comment": "(article praised on [twitter](https://twitter.com/dennybritz/status/1233343170596917248?s=20) by D Britz and Y. Goldberg)",
        "title": "[2002.12327] A Primer in BERTology: What we know about how BERT works",
        "relatedDoc": [],
        "creationTime": "2020-02-28T13:25:30Z",
        "creationDate": "2020-02-28",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.12327"
        ],
        "arxiv_author": [
            "Anna Rogers",
            "Anna Rumshisky",
            "Olga Kovaleva"
        ],
        "arxiv_summary": "Transformer-based models are now widely used in NLP, but we still do not\nunderstand a lot about their inner workings. This paper describes what is known\nto date about the famous BERT model (Devlin et al. 2019), synthesizing over 40\nanalysis studies. We also provide an overview of the proposed modifications to\nthe model and its training regime. We then outline the directions for further\nresearch.",
        "arxiv_firstAuthor": "Anna Rogers",
        "arxiv_title": "A Primer in BERTology: What we know about how BERT works",
        "arxiv_num": "2002.12327",
        "arxiv_published": "2020-02-27T18:46:42Z",
        "arxiv_updated": "2020-02-27T18:46:42Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1806.04411",
        "tag": [
            "http://www.semanlink.net/tag/named_entity_recognition",
            "http://www.semanlink.net/tag/conditional_random_field",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/information_retrieval"
        ],
        "comment": "**\"Named Entity Search (NES)\"**\r\n\r\n> We propose exploring **named entity recognition as a search task**, where the named entity class of interest is a query, and entities of that class are the relevant \"documents\". What should that query look like? Can we even perform NER-style labeling with tens of labels? This study presents an exploration of CRF-based NER models with handcrafted features and of how we might transform them into search queries.\r\n\r\n> We do not propose this as a replacement\r\nfor NER, but as something to be used for an ephemeral or contextual\r\nclass of entity, when it does not make sense to label hundreds or\r\nthousands of instances to learn a classifier",
        "title": "[1806.04411] Named Entity Recognition with Extremely Limited Data",
        "relatedDoc": [],
        "creationTime": "2019-04-11T23:40:34Z",
        "creationDate": "2019-04-11",
        "bookmarkOf": [],
        "arxiv_author": [
            "John Foley",
            "James Allan",
            "Sheikh Muhammad Sarwar"
        ],
        "arxiv_summary": "Traditional information retrieval treats named entity recognition as a\npre-indexing corpus annotation task, allowing entity tags to be indexed and\nused during search. Named entity taggers themselves are typically trained on\nthousands or tens of thousands of examples labeled by humans.\nHowever, there is a long tail of named entities classes, and for these cases,\nlabeled data may be impossible to find or justify financially. We propose\nexploring named entity recognition as a search task, where the named entity\nclass of interest is a query, and entities of that class are the relevant\n\"documents\". What should that query look like? Can we even perform NER-style\nlabeling with tens of labels? This study presents an exploration of CRF-based\nNER models with handcrafted features and of how we might transform them into\nsearch queries.",
        "arxiv_firstAuthor": "John Foley",
        "arxiv_title": "Named Entity Recognition with Extremely Limited Data",
        "arxiv_num": "1806.04411",
        "arxiv_published": "2018-06-12T09:33:23Z",
        "arxiv_updated": "2018-06-13T17:12:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2101_00345_modeling_fine_grai",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/andrew_mccallum",
            "http://www.semanlink.net/tag/entity_type_representation",
            "http://www.semanlink.net/tag/discute_avec_raphael"
        ],
        "comment": "",
        "title": "[2101.00345] Modeling Fine-Grained Entity Types with Box Embeddings",
        "relatedDoc": [],
        "creationTime": "2021-06-22T13:40:30Z",
        "creationDate": "2021-06-22",
        "bookmarkOf": [
            "https://arxiv.org/abs/2101.00345"
        ],
        "arxiv_author": [
            "Yasumasa Onoe",
            "Michael Boratko",
            "Andrew McCallum",
            "Greg Durrett"
        ],
        "arxiv_summary": "Neural entity typing models typically represent fine-grained entity types as\nvectors in a high-dimensional space, but such spaces are not well-suited to\nmodeling these types' complex interdependencies. We study the ability of box\nembeddings, which embed concepts as d-dimensional hyperrectangles, to capture\nhierarchies of types even when these relationships are not defined explicitly\nin the ontology. Our model represents both types and entity mentions as boxes.\nEach mention and its context are fed into a BERT-based model to embed that\nmention in our box space; essentially, this model leverages typological clues\npresent in the surface text to hypothesize a type representation for the\nmention. Box containment can then be used to derive both the posterior\nprobability of a mention exhibiting a given type and the conditional\nprobability relations between types themselves. We compare our approach with a\nvector-based typing model and observe state-of-the-art performance on several\nentity typing benchmarks. In addition to competitive typing performance, our\nbox-based model shows better performance in prediction consistency (predicting\na supertype and a subtype together) and confidence (i.e., calibration),\ndemonstrating that the box-based model captures the latent type hierarchies\nbetter than the vector-based model does.",
        "arxiv_firstAuthor": "Yasumasa Onoe",
        "arxiv_title": "Modeling Fine-Grained Entity Types with Box Embeddings",
        "arxiv_num": "2101.00345",
        "arxiv_published": "2021-01-02T00:59:10Z",
        "arxiv_updated": "2021-06-03T05:51:55Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1803_07828_expeditious_genera",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings"
        ],
        "comment": "> a simple and fast approach to Knowledge Graph Embedding based on the skip-gram model. Instead of using a predefined scoring function, we learn it relying on Long Short-Term Memories. We show that our embeddings achieve results comparable with the most scalable approaches on knowledge graph completion as well as on a new metric. Yet, KG2Vec can embed large graphs in lesser time by processing more than **250 million triples in less than 7 hours on common hardware**.",
        "title": "[1803.07828] Expeditious Generation of Knowledge Graph Embeddings",
        "relatedDoc": [],
        "creationTime": "2020-09-02T16:57:44Z",
        "creationDate": "2020-09-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1803.07828"
        ],
        "arxiv_author": [
            "Andr\u00e9 Valdestilhas",
            "Alexander Bigerl",
            "Stefano Ruberto",
            "Edgard Marx",
            "Diego Moussallem",
            "Diego Esteves",
            "Tommaso Soru"
        ],
        "arxiv_summary": "Knowledge Graph Embedding methods aim at representing entities and relations\nin a knowledge base as points or vectors in a continuous vector space. Several\napproaches using embeddings have shown promising results on tasks such as link\nprediction, entity recommendation, question answering, and triplet\nclassification. However, only a few methods can compute low-dimensional\nembeddings of very large knowledge bases without needing state-of-the-art\ncomputational resources. In this paper, we propose KG2Vec, a simple and fast\napproach to Knowledge Graph Embedding based on the skip-gram model. Instead of\nusing a predefined scoring function, we learn it relying on Long Short-Term\nMemories. We show that our embeddings achieve results comparable with the most\nscalable approaches on knowledge graph completion as well as on a new metric.\nYet, KG2Vec can embed large graphs in lesser time by processing more than 250\nmillion triples in less than 7 hours on common hardware.",
        "arxiv_firstAuthor": "Tommaso Soru",
        "arxiv_title": "Expeditious Generation of Knowledge Graph Embeddings",
        "arxiv_num": "1803.07828",
        "arxiv_published": "2018-03-21T10:06:28Z",
        "arxiv_updated": "2018-11-09T14:26:16Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1803.02893",
        "tag": [
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_google"
        ],
        "comment": "\"**Quick Thoughts**\". Framework for learning sentence representations from unlabelled data.\r\n\r\n> we reformulate the problem of predicting the context in which a sentence appears as a classification problem.\r\n",
        "title": "[1803.02893] An efficient framework for learning sentence representations",
        "relatedDoc": [],
        "creationTime": "2019-03-20T17:47:59Z",
        "creationDate": "2019-03-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Honglak Lee",
            "Lajanugen Logeswaran"
        ],
        "arxiv_summary": "In this work we propose a simple and efficient framework for learning\nsentence representations from unlabelled data. Drawing inspiration from the\ndistributional hypothesis and recent work on learning sentence representations,\nwe reformulate the problem of predicting the context in which a sentence\nappears as a classification problem. Given a sentence and its context, a\nclassifier distinguishes context sentences from other contrastive sentences\nbased on their vector representations. This allows us to efficiently learn\ndifferent types of encoding functions, and we show that the model learns\nhigh-quality sentence representations. We demonstrate that our sentence\nrepresentations outperform state-of-the-art unsupervised and supervised\nrepresentation learning methods on several downstream NLP tasks that involve\nunderstanding sentence semantics while achieving an order of magnitude speedup\nin training time.",
        "arxiv_firstAuthor": "Lajanugen Logeswaran",
        "arxiv_title": "An efficient framework for learning sentence representations",
        "arxiv_num": "1803.02893",
        "arxiv_published": "2018-03-07T22:02:10Z",
        "arxiv_updated": "2018-03-07T22:02:10Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1903.05872v1",
        "tag": [
            "http://www.semanlink.net/tag/semanlink2_related",
            "http://www.semanlink.net/tag/personal_information_management",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/personal_knowledge_graph",
            "http://www.semanlink.net/tag/cold_start_problem"
        ],
        "comment": "Cold start problem in personal semantic services. An interactive concept mining\r\napproach proposing concept candidates.",
        "title": "[1903.05872] Interactive Concept Mining on Personal Data -- Bootstrapping Semantic Services",
        "relatedDoc": [],
        "creationTime": "2019-03-17T23:33:13Z",
        "creationDate": "2019-03-17",
        "bookmarkOf": [],
        "arxiv_author": [
            "Christian Jilek",
            "Markus Schr\u00f6der",
            "Andreas Dengel"
        ],
        "arxiv_summary": "Semantic services (e.g. Semantic Desktops) are still afflicted by a cold\nstart problem: in the beginning, the user's personal information sphere, i.e.\nfiles, mails, bookmarks, etc., is not represented by the system. Information\nextraction tools used to kick-start the system typically create 1:1\nrepresentations of the different information items. Higher level concepts, for\nexample found in file names, mail subjects or in the content body of these\nitems, are not extracted. Leaving these concepts out may lead to\nunderperformance, having to many of them (e.g. by making every found term a\nconcept) will clutter the arising knowledge graph with non-helpful relations.\nIn this paper, we present an interactive concept mining approach proposing\nconcept candidates gathered by exploiting given schemata of usual personal\ninformation management applications and analysing the personal information\nsphere using various metrics. To heed the subjective view of the user, a\ngraphical user interface allows to easily rank and give feedback on proposed\nconcept candidates, thus keeping only those actually considered relevant. A\nprototypical implementation demonstrates major steps of our approach.",
        "arxiv_firstAuthor": "Markus Schr\u00f6der",
        "arxiv_title": "Interactive Concept Mining on Personal Data -- Bootstrapping Semantic Services",
        "arxiv_num": "1903.05872",
        "arxiv_published": "2019-03-14T09:37:53Z",
        "arxiv_updated": "2019-03-14T09:37:53Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/07/1911_03903_a_re_evaluation_of",
        "tag": [
            "http://www.semanlink.net/tag/critical_evaluation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge_graph_completion"
        ],
        "comment": "",
        "title": "[1911.03903] A Re-evaluation of Knowledge Graph Completion Methods",
        "relatedDoc": [],
        "creationTime": "2020-07-28T11:27:26Z",
        "creationDate": "2020-07-28",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.03903"
        ],
        "arxiv_author": [
            "Yiming Yang",
            "Zhiqing Sun",
            "Soumya Sanyal",
            "Shikhar Vashishth",
            "Partha Talukdar"
        ],
        "arxiv_summary": "Knowledge Graph Completion (KGC) aims at automatically predicting missing\nlinks for large-scale knowledge graphs. A vast number of state-of-the-art KGC\ntechniques have got published at top conferences in several research fields,\nincluding data mining, machine learning, and natural language processing.\nHowever, we notice that several recent papers report very high performance,\nwhich largely outperforms previous state-of-the-art methods. In this paper, we\nfind that this can be attributed to the inappropriate evaluation protocol used\nby them and propose a simple evaluation protocol to address this problem. The\nproposed protocol is robust to handle bias in the model, which can\nsubstantially affect the final results. We conduct extensive experiments and\nreport the performance of several existing methods using our protocol. The\nreproducible code has been made publicly available",
        "arxiv_firstAuthor": "Zhiqing Sun",
        "arxiv_title": "A Re-evaluation of Knowledge Graph Completion Methods",
        "arxiv_num": "1911.03903",
        "arxiv_published": "2019-11-10T11:19:08Z",
        "arxiv_updated": "2020-07-08T19:32:34Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1602.02410",
        "tag": [
            "http://www.semanlink.net/tag/language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/recurrent_neural_network"
        ],
        "comment": "recent advances in Recurrent Neural Networks for large scale Language Modeling",
        "title": "[1602.02410] Exploring the Limits of Language Modeling",
        "relatedDoc": [],
        "creationTime": "2016-02-09T19:00:54Z",
        "creationDate": "2016-02-09",
        "bookmarkOf": [],
        "arxiv_author": [
            "Mike Schuster",
            "Noam Shazeer",
            "Rafal Jozefowicz",
            "Oriol Vinyals",
            "Yonghui Wu"
        ],
        "arxiv_summary": "In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.",
        "arxiv_firstAuthor": "Rafal Jozefowicz",
        "arxiv_title": "Exploring the Limits of Language Modeling",
        "arxiv_num": "1602.02410",
        "arxiv_published": "2016-02-07T19:11:17Z",
        "arxiv_updated": "2016-02-11T23:01:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/01/2012_15723",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/pre_trained_language_models",
            "http://www.semanlink.net/tag/few_shot_learning"
        ],
        "comment": "[Tweet](https://twitter.com/adamjfisch/status/1345185238276861953)",
        "title": "[2012.15723] Making Pre-trained Language Models Better Few-shot Learners",
        "relatedDoc": [],
        "creationTime": "2021-01-02T22:42:12Z",
        "creationDate": "2021-01-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/2012.15723"
        ],
        "arxiv_author": [
            "Danqi Chen",
            "Tianyu Gao",
            "Adam Fisch"
        ],
        "arxiv_summary": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF--better\nfew-shot fine-tuning of language models--a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.",
        "arxiv_firstAuthor": "Tianyu Gao",
        "arxiv_title": "Making Pre-trained Language Models Better Few-shot Learners",
        "arxiv_num": "2012.15723",
        "arxiv_published": "2020-12-31T17:21:26Z",
        "arxiv_updated": "2020-12-31T17:21:26Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1307.5101",
        "tag": [
            "http://www.semanlink.net/tag/multi_label_classification",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1307.5101] Large-scale Multi-label Learning with Missing Labels",
        "relatedDoc": [],
        "creationTime": "2018-03-04T17:05:39Z",
        "creationDate": "2018-03-04",
        "bookmarkOf": [],
        "arxiv_author": [
            "Inderjit S. Dhillon",
            "Purushottam Kar",
            "Hsiang-Fu Yu",
            "Prateek Jain"
        ],
        "arxiv_summary": "The multi-label classification problem has generated significant interest in\nrecent years. However, existing approaches do not adequately address two key\nchallenges: (a) the ability to tackle problems with a large number (say\nmillions) of labels, and (b) the ability to handle data with missing labels. In\nthis paper, we directly address both these problems by studying the multi-label\nproblem in a generic empirical risk minimization (ERM) framework. Our\nframework, despite being simple, is surprisingly able to encompass several\nrecent label-compression based methods which can be derived as special cases of\nour method. To optimize the ERM problem, we develop techniques that exploit the\nstructure of specific loss functions - such as the squared loss function - to\noffer efficient algorithms. We further show that our learning framework admits\nformal excess risk bounds even in the presence of missing labels. Our risk\nbounds are tight and demonstrate better generalization performance for low-rank\npromoting trace-norm regularization when compared to (rank insensitive)\nFrobenius norm regularization. Finally, we present extensive empirical results\non a variety of benchmark datasets and show that our methods perform\nsignificantly better than existing label compression based methods and can\nscale up to very large datasets such as the Wikipedia dataset.",
        "arxiv_firstAuthor": "Hsiang-Fu Yu",
        "arxiv_title": "Large-scale Multi-label Learning with Missing Labels",
        "arxiv_num": "1307.5101",
        "arxiv_published": "2013-07-18T23:55:55Z",
        "arxiv_updated": "2013-11-25T16:57:43Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1807.03748",
        "tag": [
            "http://www.semanlink.net/tag/contrastive_self_supervised_learning",
            "http://www.semanlink.net/tag/unsupervised_machine_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/representation_learning",
            "http://www.semanlink.net/tag/google_deepmind"
        ],
        "comment": "> a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful [autoregressive models](/tag/autoregressive_model). We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using [negative sampling](/tag/negative_sampling).\r\n\r\na contrastive method that can be applied to any form of data that can be expressed in an ordered sequence: text, speech, video...",
        "title": "[1807.03748] Representation Learning with Contrastive Predictive Coding",
        "relatedDoc": [],
        "creationTime": "2018-07-21T10:05:02Z",
        "creationDate": "2018-07-21",
        "bookmarkOf": [],
        "arxiv_author": [
            "Oriol Vinyals",
            "Aaron van den Oord",
            "Yazhe Li"
        ],
        "arxiv_summary": "While supervised learning has enabled great progress in many applications,\nunsupervised learning has not seen such widespread adoption, and remains an\nimportant and challenging endeavor for artificial intelligence. In this work,\nwe propose a universal unsupervised learning approach to extract useful\nrepresentations from high-dimensional data, which we call Contrastive\nPredictive Coding. The key insight of our model is to learn such\nrepresentations by predicting the future in latent space by using powerful\nautoregressive models. We use a probabilistic contrastive loss which induces\nthe latent space to capture information that is maximally useful to predict\nfuture samples. It also makes the model tractable by using negative sampling.\nWhile most prior work has focused on evaluating representations for a\nparticular modality, we demonstrate that our approach is able to learn useful\nrepresentations achieving strong performance on four distinct domains: speech,\nimages, text and reinforcement learning in 3D environments.",
        "arxiv_firstAuthor": "Aaron van den Oord",
        "arxiv_title": "Representation Learning with Contrastive Predictive Coding",
        "arxiv_num": "1807.03748",
        "arxiv_published": "2018-07-10T16:52:11Z",
        "arxiv_updated": "2019-01-22T18:47:12Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/03/_2003_03384_automl_zero_evolv",
        "tag": [
            "http://www.semanlink.net/tag/evolutionary_algorithm",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/automl",
            "http://www.semanlink.net/tag/backpropagation_vs_biology",
            "http://www.semanlink.net/tag/quoc_le"
        ],
        "comment": "> Fun AutoML-Zero experiments: Evolutionary search discovers fundamental ML algorithms from scratch, e.g., small neural nets with backprop.\r\n> Can evolution be the \u201cMaster Algorithm\u201d? ;)",
        "title": "[2003.03384] AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
        "relatedDoc": [],
        "creationTime": "2020-03-17T21:57:40Z",
        "creationDate": "2020-03-17",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.03384"
        ],
        "arxiv_author": [
            "Chen Liang",
            "Esteban Real",
            "Quoc V. Le",
            "David R. So"
        ],
        "arxiv_summary": "Machine learning research has advanced in multiple aspects, including model\nstructures and learning methods. The effort to automate such research, known as\nAutoML, has also made significant progress. However, this progress has largely\nfocused on the architecture of neural networks, where it has relied on\nsophisticated expert-designed layers as building blocks---or similarly\nrestrictive search spaces. Our goal is to show that AutoML can go further: it\nis possible today to automatically discover complete machine learning\nalgorithms just using basic mathematical operations as building blocks. We\ndemonstrate this by introducing a novel framework that significantly reduces\nhuman bias through a generic search space. Despite the vastness of this space,\nevolutionary search can still discover two-layer neural networks trained by\nbackpropagation. These simple neural networks can then be surpassed by evolving\ndirectly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques\nemerge in the top algorithms, such as bilinear interactions, normalized\ngradients, and weight averaging. Moreover, evolution adapts algorithms to\ndifferent task types: e.g., dropout-like techniques appear when little data is\navailable. We believe these preliminary successes in discovering machine\nlearning algorithms from scratch indicate a promising new direction for the\nfield.",
        "arxiv_firstAuthor": "Esteban Real",
        "arxiv_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
        "arxiv_num": "2003.03384",
        "arxiv_published": "2020-03-06T19:00:04Z",
        "arxiv_updated": "2020-03-06T19:00:04Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/2004_14545_explainable_deep_l",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/explainable_ai"
        ],
        "comment": "",
        "title": "[2004.14545] Explainable Deep Learning: A Field Guide for the Uninitiated",
        "relatedDoc": [],
        "creationTime": "2020-05-01T13:56:26Z",
        "creationDate": "2020-05-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/2004.14545"
        ],
        "arxiv_author": [
            "Gabrielle Ras",
            "Marcel van Gerven",
            "Derek Doran",
            "Ning Xie"
        ],
        "arxiv_summary": "Deep neural network (DNN) is an indispensable machine learning tool for\nachieving human-level performance on many learning tasks. Yet, due to its\nblack-box nature, it is inherently difficult to understand which aspects of the\ninput data drive the decisions of the network. There are various real-world\nscenarios in which humans need to make actionable decisions based on the output\nDNNs. Such decision support systems can be found in critical domains, such as\nlegislation, law enforcement, etc. It is important that the humans making\nhigh-level decisions can be sure that the DNN decisions are driven by\ncombinations of data features that are appropriate in the context of the\ndeployment of the decision support system and that the decisions made are\nlegally or ethically defensible. Due to the incredible pace at which DNN\ntechnology is being developed, the development of new methods and studies on\nexplaining the decision-making process of DNNs has blossomed into an active\nresearch field. A practitioner beginning to study explainable deep learning may\nbe intimidated by the plethora of orthogonal directions the field is taking.\nThis complexity is further exacerbated by the general confusion that exists in\ndefining what it means to be able to explain the actions of a deep learning\nsystem and to evaluate a system's \"ability to explain\". To alleviate this\nproblem, this article offers a \"field guide\" to deep learning explainability\nfor those uninitiated in the field. The field guide: i) Discusses the traits of\na deep learning system that researchers enhance in explainability research, ii)\nplaces explainability in the context of other related deep learning research\nareas, and iii) introduces three simple dimensions defining the space of\nfoundational methods that contribute to explainable deep learning. The guide is\ndesigned as an easy-to-digest starting point for those just embarking in the\nfield.",
        "arxiv_firstAuthor": "Ning Xie",
        "arxiv_title": "Explainable Deep Learning: A Field Guide for the Uninitiated",
        "arxiv_num": "2004.14545",
        "arxiv_published": "2020-04-30T02:09:02Z",
        "arxiv_updated": "2020-04-30T02:09:02Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1908_08983_a_little_annotatio",
        "tag": [
            "http://www.semanlink.net/tag/labeled_data",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/cross_lingual_nlp",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "",
        "title": "[1908.08983] A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers",
        "relatedDoc": [],
        "creationTime": "2019-08-28T22:57:43Z",
        "creationDate": "2019-08-28",
        "bookmarkOf": [
            "https://arxiv.org/abs/1908.08983"
        ],
        "arxiv_author": [
            "Aditi Chaudhary",
            "Zaid Sheikh",
            "Jaime G. Carbonell",
            "Graham Neubig",
            "Jiateng Xie"
        ],
        "arxiv_summary": "Most state-of-the-art models for named entity recognition (NER) rely on the\navailability of large amounts of labeled data, making them challenging to\nextend to new, lower-resourced languages. However, there are now several\nproposed approaches involving either cross-lingual transfer learning, which\nlearns from other highly resourced languages, or active learning, which\nefficiently selects effective training data based on model predictions. This\npaper poses the question: given this recent progress, and limited human\nannotation, what is the most effective method for efficiently creating\nhigh-quality entity recognizers in under-resourced languages? Based on\nextensive experimentation using both simulated and real human annotation, we\nfind a dual-strategy approach best, starting with a cross-lingual transferred\nmodel, then performing targeted annotation of only uncertain entity spans in\nthe target language, minimizing annotator effort. Results demonstrate that\ncross-lingual transfer is a powerful tool when very little data can be\nannotated, but an entity-targeted annotation strategy can achieve competitive\naccuracy quickly, with just one-tenth of training data.",
        "arxiv_firstAuthor": "Aditi Chaudhary",
        "arxiv_title": "A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers",
        "arxiv_num": "1908.08983",
        "arxiv_published": "2019-08-23T19:15:07Z",
        "arxiv_updated": "2019-08-23T19:15:07Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1510.00726",
        "tag": [
            "http://www.semanlink.net/tag/nn_4_nlp",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1510.00726] A Primer on Neural Network Models for Natural Language Processing",
        "relatedDoc": [],
        "creationTime": "2017-07-20T13:22:06Z",
        "creationDate": "2017-07-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Yoav Goldberg"
        ],
        "arxiv_summary": "Over the past few years, neural networks have re-emerged as powerful\nmachine-learning models, yielding state-of-the-art results in fields such as\nimage recognition and speech processing. More recently, neural network models\nstarted to be applied also to textual natural language signals, again with very\npromising results. This tutorial surveys neural network models from the\nperspective of natural language processing research, in an attempt to bring\nnatural-language researchers up to speed with the neural techniques. The\ntutorial covers input encoding for natural language tasks, feed-forward\nnetworks, convolutional networks, recurrent networks and recursive networks, as\nwell as the computation graph abstraction for automatic gradient computation.",
        "arxiv_firstAuthor": "Yoav Goldberg",
        "arxiv_title": "A Primer on Neural Network Models for Natural Language Processing",
        "arxiv_num": "1510.00726",
        "arxiv_published": "2015-10-02T20:17:33Z",
        "arxiv_updated": "2015-10-02T20:17:33Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1911_03814_zero_shot_entity_l",
        "tag": [
            "http://www.semanlink.net/tag/blink",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/zero_shot_entity_linking"
        ],
        "comment": "",
        "title": "[1911.03814] Zero-shot Entity Linking with Dense Entity Retrieval",
        "relatedDoc": [],
        "creationTime": "2020-05-02T11:43:47Z",
        "creationDate": "2020-05-02",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.03814"
        ],
        "arxiv_author": [
            "Martin Josifoski",
            "Luke Zettlemoyer",
            "Ledell Wu",
            "Sebastian Riedel",
            "Fabio Petroni"
        ],
        "arxiv_summary": "We consider the zero-shot entity-linking challenge where each entity is\ndefined by a short textual description, and the model must read these\ndescriptions together with the mention context to make the final linking\ndecisions. In this setting, retrieving entity candidates can be particularly\nchallenging, since many of the common linking cues such as entity alias tables\nand link popularity are not available. In this paper, we introduce a simple and\neffective two stage approach for zero-shot linking, based on fine-tuned BERT\narchitectures. In the first stage, we do retrieval in a dense space defined by\na bi-encoder that independently embeds the mention context and the entity\ndescriptions. Each candidate is then examined more carefully with a\ncross-encoder, that concatenates the mention and entity text. Our approach\nachieves a nearly 5 point absolute gain on a recently introduced zero-shot\nentity linking benchmark, driven largely by improvements over previous IR-based\ncandidate retrieval. We also show that it performs well in the non-zero-shot\nsetting, obtaining the state-of-the-art result on TACKBP-2010.",
        "arxiv_firstAuthor": "Ledell Wu",
        "arxiv_title": "Zero-shot Entity Linking with Dense Entity Retrieval",
        "arxiv_num": "1911.03814",
        "arxiv_published": "2019-11-10T01:01:45Z",
        "arxiv_updated": "2019-11-10T01:01:45Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/1511.08154",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/hypothese_de_riemann",
            "http://www.semanlink.net/tag/jean_paul"
        ],
        "comment": "",
        "title": "[1511.08154] Notes on Cardinal's Matrices",
        "relatedDoc": [],
        "creationTime": "2016-01-12T23:36:39Z",
        "creationDate": "2016-01-12",
        "bookmarkOf": [],
        "arxiv_author": [
            "David Montague",
            "Jeffrey C. Lagarias"
        ],
        "arxiv_summary": "These notes are motivated by the work of Jean-Paul Cardinal on symmetric\nmatrices related to the Mertens function. He showed that certain norm bounds on\nhis matrices implied the Riemann hypothesis. Using a different matrix norm we\nshow an equivalence of the Riemann hypothesis to suitable norm bounds on his\nmatrices in the new norm. Then we specify a deformed version of his Mertens\nfunction matrices that unconditionally satisfies a norm bound that is of the\nsame strength as his Riemann hypothesis bound.",
        "arxiv_firstAuthor": "Jeffrey C. Lagarias",
        "arxiv_title": "Notes on Cardinal's Matrices",
        "arxiv_num": "1511.08154",
        "arxiv_published": "2015-11-25T19:02:43Z",
        "arxiv_updated": "2015-11-25T19:02:43Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1706.03762",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/attention_is_all_you_need"
        ],
        "comment": "> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the **Transformer**, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. ",
        "title": "[1706.03762] Attention Is All You Need",
        "relatedDoc": [],
        "creationTime": "2018-10-12T18:50:14Z",
        "creationDate": "2018-10-12",
        "bookmarkOf": [],
        "arxiv_author": [
            "Ashish Vaswani",
            "Niki Parmar",
            "Jakob Uszkoreit",
            "Noam Shazeer",
            "Llion Jones",
            "Illia Polosukhin",
            "Lukasz Kaiser",
            "Aidan N. Gomez"
        ],
        "arxiv_summary": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
        "arxiv_firstAuthor": "Ashish Vaswani",
        "arxiv_title": "Attention Is All You Need",
        "arxiv_num": "1706.03762",
        "arxiv_published": "2017-06-12T17:57:34Z",
        "arxiv_updated": "2017-12-06T03:30:32Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/pdf/1301.3781.pdf",
        "tag": [
            "http://www.semanlink.net/tag/tomas_mikolov",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/word2vec"
        ],
        "comment": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\r\n",
        "title": "[1301.3781] Efficient Estimation of Word Representations in Vector Space",
        "relatedDoc": [],
        "creationTime": "2016-01-13T23:07:45Z",
        "creationDate": "2016-01-13",
        "bookmarkOf": [],
        "arxiv_author": [
            "Greg Corrado",
            "Jeffrey Dean",
            "Tomas Mikolov",
            "Kai Chen"
        ],
        "arxiv_summary": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
        "arxiv_firstAuthor": "Tomas Mikolov",
        "arxiv_title": "Efficient Estimation of Word Representations in Vector Space",
        "arxiv_num": "1301.3781",
        "arxiv_published": "2013-01-16T18:24:43Z",
        "arxiv_updated": "2013-09-07T00:30:40Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1911_00172_generalization_thr",
        "tag": [
            "http://www.semanlink.net/tag/k_nearest_neighbors_algorithm",
            "http://www.semanlink.net/tag/language_model",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/dan_jurafsky",
            "http://www.semanlink.net/tag/facebook_fair"
        ],
        "comment": "extend LMs with nearest neighbor search in embedding space",
        "title": "[1911.00172] Generalization through Memorization: Nearest Neighbor Language Models",
        "relatedDoc": [],
        "creationTime": "2019-12-20T23:44:45Z",
        "creationDate": "2019-12-20",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.00172"
        ],
        "arxiv_author": [
            "Omer Levy",
            "Luke Zettlemoyer",
            "Mike Lewis",
            "Urvashi Khandelwal",
            "Dan Jurafsky"
        ],
        "arxiv_summary": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM)\nby linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The\nnearest neighbors are computed according to distance in the pre-trained LM\nembedding space, and can be drawn from any text collection, including the\noriginal LM training data. Applying this augmentation to a strong Wikitext-103\nLM, with neighbors drawn from the original training set, our $k$NN-LM achieves\na new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no\nadditional training. We also show that this approach has implications for\nefficiently scaling up to larger training sets and allows for effective domain\nadaptation, by simply varying the nearest neighbor datastore, again without\nfurther training. Qualitatively, the model is particularly helpful in\npredicting rare patterns, such as factual knowledge. Together, these results\nstrongly suggest that learning similarity between sequences of text is easier\nthan predicting the next word, and that nearest neighbor search is an effective\napproach for language modeling in the long tail.",
        "arxiv_firstAuthor": "Urvashi Khandelwal",
        "arxiv_title": "Generalization through Memorization: Nearest Neighbor Language Models",
        "arxiv_num": "1911.00172",
        "arxiv_published": "2019-11-01T01:09:53Z",
        "arxiv_updated": "2020-02-15T01:04:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_1912_12510_detecting_out_of_d",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/out_of_distribution_detection"
        ],
        "comment": "> we propose to detect OOD examples by identifying inconsistencies between activity patterns and class predicted... \r\n> Unlike many approaches, this can be used with any pre-trained softmax classifier and does not require access to OOD data",
        "title": "[1912.12510] Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices",
        "relatedDoc": [],
        "creationTime": "2020-01-15T13:04:14Z",
        "creationDate": "2020-01-15",
        "bookmarkOf": [
            "https://arxiv.org/abs/1912.12510"
        ],
        "arxiv_author": [
            "Chandramouli Shama Sastry",
            "Sageev Oore"
        ],
        "arxiv_summary": "When presented with Out-of-Distribution (OOD) examples, deep neural networks\nyield confident, incorrect predictions. Detecting OOD examples is challenging,\nand the potential risks are high. In this paper, we propose to detect OOD\nexamples by identifying inconsistencies between activity patterns and class\npredicted. We find that characterizing activity patterns by Gram matrices and\nidentifying anomalies in gram matrix values can yield high OOD detection rates.\nWe identify anomalies in the gram matrices by simply comparing each value with\nits respective range observed over the training data. Unlike many approaches,\nthis can be used with any pre-trained softmax classifier and does not require\naccess to OOD data for fine-tuning hyperparameters, nor does it require OOD\naccess for inferring parameters. The method is applicable across a variety of\narchitectures and vision datasets and, for the important and surprisingly hard\ntask of detecting far-from-distribution out-of-distribution examples, it\ngenerally performs better than or equal to state-of-the-art OOD detection\nmethods (including those that do assume access to OOD examples).",
        "arxiv_firstAuthor": "Chandramouli Shama Sastry",
        "arxiv_title": "Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices",
        "arxiv_num": "1912.12510",
        "arxiv_published": "2019-12-28T19:44:03Z",
        "arxiv_updated": "2020-01-09T15:17:55Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1704.08803",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/these_irit_renault_biblio_initiale",
            "http://www.semanlink.net/tag/okapi_bm25",
            "http://www.semanlink.net/tag/machine_learned_ranking",
            "http://www.semanlink.net/tag/weak_supervision"
        ],
        "comment": "Main Idea: To **leverage large amounts of unsupervised data to infer \u201cweak\u201d labels** and use that signal for learning supervised models as if we had the ground truth labels. See [blog post](/doc/?uri=http%3A%2F%2Fmostafadehghani.com%2F2017%2F04%2F23%2Fbeating-the-teacher-neural-ranking-models-with-weak-supervision%2F):\r\n\r\n> This is **truly awesome since we have only used  BM25 as the supervisor to train a model which performs better than BM25** itself!\r\n",
        "title": "[1704.08803] Neural Ranking Models with Weak Supervision",
        "relatedDoc": [
            "http://mostafadehghani.com/2017/04/23/beating-the-teacher-neural-ranking-models-with-weak-supervision/"
        ],
        "creationTime": "2019-01-27T17:31:01Z",
        "creationDate": "2019-01-27",
        "bookmarkOf": [],
        "arxiv_author": [
            "Mostafa Dehghani",
            "Jaap Kamps",
            "W. Bruce Croft",
            "Hamed Zamani",
            "Aliaksei Severyn"
        ],
        "arxiv_summary": "Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models.",
        "arxiv_firstAuthor": "Mostafa Dehghani",
        "arxiv_title": "Neural Ranking Models with Weak Supervision",
        "arxiv_num": "1704.08803",
        "arxiv_published": "2017-04-28T04:08:47Z",
        "arxiv_updated": "2017-05-29T11:58:34Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1604.00289",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/human_like_ai"
        ],
        "comment": "> we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations\r\n",
        "title": "[1604.00289] Building Machines That Learn and Think Like People",
        "relatedDoc": [],
        "creationTime": "2018-10-28T17:08:00Z",
        "creationDate": "2018-10-28",
        "bookmarkOf": [],
        "arxiv_author": [
            "Tomer D. Ullman",
            "Brenden M. Lake",
            "Samuel J. Gershman",
            "Joshua B. Tenenbaum"
        ],
        "arxiv_summary": "Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.",
        "arxiv_firstAuthor": "Brenden M. Lake",
        "arxiv_title": "Building Machines That Learn and Think Like People",
        "arxiv_num": "1604.00289",
        "arxiv_published": "2016-04-01T15:37:57Z",
        "arxiv_updated": "2016-11-02T17:26:50Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/08/_1808_02590_a_tutorial_on_netw",
        "tag": [
            "http://www.semanlink.net/tag/graph_embeddings",
            "http://www.semanlink.net/tag/ml_google",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1808.02590] A Tutorial on Network Embeddings",
        "relatedDoc": [],
        "creationTime": "2019-08-25T02:02:16Z",
        "creationDate": "2019-08-25",
        "bookmarkOf": [
            "https://arxiv.org/abs/1808.02590"
        ],
        "arxiv_author": [
            "Bryan Perozzi",
            "Rami Al-Rfou",
            "Haochen Chen",
            "Steven Skiena"
        ],
        "arxiv_summary": "Network embedding methods aim at learning low-dimensional latent\nrepresentation of nodes in a network. These representations can be used as\nfeatures for a wide range of tasks on graphs such as classification,\nclustering, link prediction, and visualization. In this survey, we give an\noverview of network embeddings by summarizing and categorizing recent\nadvancements in this research field. We first discuss the desirable properties\nof network embeddings and briefly introduce the history of network embedding\nalgorithms. Then, we discuss network embedding methods under different\nscenarios, such as supervised versus unsupervised learning, learning embeddings\nfor homogeneous networks versus for heterogeneous networks, etc. We further\ndemonstrate the applications of network embeddings, and conclude the survey\nwith future work in this area.",
        "arxiv_firstAuthor": "Haochen Chen",
        "arxiv_title": "A Tutorial on Network Embeddings",
        "arxiv_num": "1808.02590",
        "arxiv_published": "2018-08-08T00:54:01Z",
        "arxiv_updated": "2018-08-08T00:54:01Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1706.04902",
        "tag": [
            "http://www.semanlink.net/tag/sebastian_ruder",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/cross_lingual_word_embeddings"
        ],
        "comment": "",
        "title": "[1706.04902] A Survey Of Cross-lingual Word Embedding Models",
        "relatedDoc": [],
        "creationTime": "2018-05-20T12:01:50Z",
        "creationDate": "2018-05-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Sebastian Ruder",
            "Ivan Vuli\u0107",
            "Anders S\u00f8gaard"
        ],
        "arxiv_summary": "Cross-lingual representations of words enable us to reason about word meaning\nin multilingual contexts and are a key facilitator of cross-lingual transfer\nwhen developing natural language processing models for low-resource languages.\nIn this survey, we provide a comprehensive typology of cross-lingual word\nembedding models. We compare their data requirements and objective functions.\nThe recurring theme of the survey is that many of the models presented in the\nliterature optimize for the same objectives, and that seemingly different\nmodels are often equivalent modulo optimization strategies, hyper-parameters,\nand such. We also discuss the different ways cross-lingual word embeddings are\nevaluated, as well as future challenges and research horizons.",
        "arxiv_firstAuthor": "Sebastian Ruder",
        "arxiv_title": "A Survey Of Cross-lingual Word Embedding Models",
        "arxiv_num": "1706.04902",
        "arxiv_published": "2017-06-15T14:46:56Z",
        "arxiv_updated": "2019-10-06T10:01:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1712.01208v1",
        "tag": [
            "http://www.semanlink.net/tag/ml_google",
            "http://www.semanlink.net/tag/nips_2017",
            "http://www.semanlink.net/tag/learned_index_structures",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/semantic_hashing"
        ],
        "comment": "> we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs \r\n>\r\n> Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes.",
        "title": "[1712.01208] The Case for Learned Index Structures",
        "relatedDoc": [],
        "creationTime": "2017-12-11T19:25:09Z",
        "creationDate": "2017-12-11",
        "bookmarkOf": [],
        "arxiv_author": [
            "Ed H. Chi",
            "Neoklis Polyzotis",
            "Jeffrey Dean",
            "Tim Kraska",
            "Alex Beutel"
        ],
        "arxiv_summary": "Indexes are models: a B-Tree-Index can be seen as a model to map a key to the\nposition of a record within a sorted array, a Hash-Index as a model to map a\nkey to a position of a record within an unsorted array, and a BitMap-Index as a\nmodel to indicate if a data record exists or not. In this exploratory research\npaper, we start from this premise and posit that all existing index structures\ncan be replaced with other types of models, including deep-learning models,\nwhich we term learned indexes. The key idea is that a model can learn the sort\norder or structure of lookup keys and use this signal to effectively predict\nthe position or existence of records. We theoretically analyze under which\nconditions learned indexes outperform traditional index structures and describe\nthe main challenges in designing learned index structures. Our initial results\nshow, that by using neural nets we are able to outperform cache-optimized\nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over\nseveral real-world data sets. More importantly though, we believe that the idea\nof replacing core components of a data management system through learned models\nhas far reaching implications for future systems designs and that this work\njust provides a glimpse of what might be possible.",
        "arxiv_firstAuthor": "Tim Kraska",
        "arxiv_title": "The Case for Learned Index Structures",
        "arxiv_num": "1712.01208",
        "arxiv_published": "2017-12-04T17:18:41Z",
        "arxiv_updated": "2018-04-30T07:54:41Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/03/2010_02194_self_training_impr",
        "tag": [
            "http://www.semanlink.net/tag/nlp_pretraining",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/self_training",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[2010.02194] Self-training Improves Pre-training for Natural Language Understanding",
        "relatedDoc": [],
        "creationTime": "2021-03-12T06:17:22Z",
        "creationDate": "2021-03-12",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.02194"
        ],
        "arxiv_author": [
            "Ves Stoyanov",
            "Edouard Grave",
            "Onur Celebi",
            "Alexis Conneau",
            "Beliz Gunel",
            "Vishrav Chaudhary",
            "Michael Auli",
            "Jingfei Du"
        ],
        "arxiv_summary": "Unsupervised pre-training has led to much recent progress in natural language\nunderstanding. In this paper, we study self-training as another way to leverage\nunlabeled data through semi-supervised learning. To obtain additional data for\na specific task, we introduce SentAugment, a data augmentation method which\ncomputes task-specific query embeddings from labeled data to retrieve sentences\nfrom a bank of billions of unlabeled sentences crawled from the web. Unlike\nprevious semi-supervised methods, our approach does not require in-domain\nunlabeled data and is therefore more generally applicable. Experiments show\nthat self-training is complementary to strong RoBERTa baselines on a variety of\ntasks. Our augmentation approach leads to scalable and effective self-training\nwith improvements of up to 2.6% on standard text classification benchmarks.\nFinally, we also show strong gains on knowledge-distillation and few-shot\nlearning.",
        "arxiv_firstAuthor": "Jingfei Du",
        "arxiv_title": "Self-training Improves Pre-training for Natural Language Understanding",
        "arxiv_num": "2010.02194",
        "arxiv_published": "2020-10-05T17:52:25Z",
        "arxiv_updated": "2020-10-05T17:52:25Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/12/_1912_03927_large_deviations_f",
        "tag": [
            "http://www.semanlink.net/tag/active_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/lenka_zdeborova"
        ],
        "comment": "the task of choosing the subset of samples to be labeled from a fixed finite pool of samples",
        "title": "[1912.03927] Large deviations for the perceptron model and consequences for active learning",
        "relatedDoc": [],
        "creationTime": "2019-12-11T02:26:25Z",
        "creationDate": "2019-12-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1912.03927"
        ],
        "arxiv_author": [
            "Luca Saglietti",
            "Hugo Cui",
            "Lenka Zdeborov\u00e1"
        ],
        "arxiv_summary": "Active learning is a branch of machine learning that deals with problems\nwhere unlabeled data is abundant yet obtaining labels is expensive. The\nlearning algorithm has the possibility of querying a limited number of samples\nto obtain the corresponding labels, subsequently used for supervised learning.\nIn this work, we consider the task of choosing the subset of samples to be\nlabeled from a fixed finite pool of samples. We assume the pool of samples to\nbe a random matrix and the ground truth labels to be generated by a\nsingle-layer teacher random neural network. We employ replica methods to\nanalyze the large deviations for the accuracy achieved after supervised\nlearning on a subset of the original pool. These large deviations then provide\noptimal achievable performance boundaries for any active learning algorithm. We\nshow that the optimal learning performance can be efficiently approached by\nsimple message-passing active learning algorithms. We also provide a comparison\nwith the performance of some other popular active learning strategies.",
        "arxiv_firstAuthor": "Hugo Cui",
        "arxiv_title": "Large deviations for the perceptron model and consequences for active learning",
        "arxiv_num": "1912.03927",
        "arxiv_published": "2019-12-09T09:50:52Z",
        "arxiv_updated": "2019-12-09T09:50:52Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/08/2107_12708_qa_dataset_explosi",
        "tag": [
            "http://www.semanlink.net/tag/nlp_reading_comprehension",
            "http://www.semanlink.net/tag/sebastian_ruder",
            "http://www.semanlink.net/tag/allen_institute_for_ai_a2i",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/question_answering"
        ],
        "comment": "recommand\u00e9 par [Sebastian Ruder](tag:sebastian_ruder)",
        "title": "[2107.12708] QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
        "relatedDoc": [],
        "creationTime": "2021-08-06T22:01:16Z",
        "creationDate": "2021-08-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/2107.12708"
        ],
        "arxiv_author": [
            "Isabelle Augenstein",
            "Anna Rogers",
            "Matt Gardner"
        ],
        "arxiv_summary": "Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of ``reasoning types\" in question answering and\npropose a new taxonomy. We also discuss the implications of over-focusing on\nEnglish, and survey the current monolingual resources for other languages and\nmultilingual resources. The study is aimed at both practitioners looking for\npointers to the wealth of existing data, and at researchers working on new\nresources.",
        "arxiv_firstAuthor": "Anna Rogers",
        "arxiv_title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
        "arxiv_num": "2107.12708",
        "arxiv_published": "2021-07-27T10:09:13Z",
        "arxiv_updated": "2021-07-27T10:09:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1710.04099",
        "tag": [
            "http://www.semanlink.net/tag/gensim",
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/word2vec"
        ],
        "comment": "web service for querying an embedding of entities in the Wikidata knowledge graph. The embedding is trained on the Wikidata dump using Gensim's Word2Vec implementation and a simple graph walk",
        "title": "[1710.04099] Wembedder: Wikidata entity embedding web service",
        "relatedDoc": [],
        "creationTime": "2018-02-13T19:14:37Z",
        "creationDate": "2018-02-13",
        "bookmarkOf": [],
        "arxiv_author": [
            "Finn \u00c5rup Nielsen"
        ],
        "arxiv_summary": "I present a web service for querying an embedding of entities in the Wikidata\nknowledge graph. The embedding is trained on the Wikidata dump using Gensim's\nWord2Vec implementation and a simple graph walk. A REST API is implemented.\nTogether with the Wikidata API the web service exposes a multilingual resource\nfor over 600'000 Wikidata items and properties.",
        "arxiv_firstAuthor": "Finn \u00c5rup Nielsen",
        "arxiv_title": "Wembedder: Wikidata entity embedding web service",
        "arxiv_num": "1710.04099",
        "arxiv_published": "2017-10-11T14:56:27Z",
        "arxiv_updated": "2017-10-11T14:56:27Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/_1906_08237_xlnet_generalized",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/xlnet"
        ],
        "comment": "a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE)",
        "title": "[1906.08237] XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "relatedDoc": [],
        "creationTime": "2019-06-21T16:29:51Z",
        "creationDate": "2019-06-21",
        "bookmarkOf": [
            "https://arxiv.org/abs/1906.08237"
        ],
        "arxiv_author": [
            "Yiming Yang",
            "Zihang Dai",
            "Ruslan Salakhutdinov",
            "Jaime Carbonell",
            "Quoc V. Le",
            "Zhilin Yang"
        ],
        "arxiv_summary": "With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.",
        "arxiv_firstAuthor": "Zhilin Yang",
        "arxiv_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "arxiv_num": "1906.08237",
        "arxiv_published": "2019-06-19T17:35:48Z",
        "arxiv_updated": "2020-01-02T12:48:08Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/1904_09078_embracenet_a_robu",
        "tag": [
            "http://www.semanlink.net/tag/multimodal_classification",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1904.09078] EmbraceNet: A robust deep learning architecture for multimodal classification",
        "relatedDoc": [],
        "creationTime": "2020-10-14T09:55:10Z",
        "creationDate": "2020-10-14",
        "bookmarkOf": [
            "https://arxiv.org/abs/1904.09078"
        ],
        "arxiv_author": [
            "Jong-Seok Lee",
            "Jun-Ho Choi"
        ],
        "arxiv_summary": "Classification using multimodal data arises in many machine learning\napplications. It is crucial not only to model cross-modal relationship\neffectively but also to ensure robustness against loss of part of data or\nmodalities. In this paper, we propose a novel deep learning-based multimodal\nfusion architecture for classification tasks, which guarantees compatibility\nwith any kind of learning models, deals with cross-modal information carefully,\nand prevents performance degradation due to partial absence of data. We employ\ntwo datasets for multimodal classification tasks, build models based on our\narchitecture and other state-of-the-art models, and analyze their performance\non various situations. The results show that our architecture outperforms the\nother multimodal fusion architectures when some parts of data are not\navailable.",
        "arxiv_firstAuthor": "Jun-Ho Choi",
        "arxiv_title": "EmbraceNet: A robust deep learning architecture for multimodal classification",
        "arxiv_num": "1904.09078",
        "arxiv_published": "2019-04-19T04:46:29Z",
        "arxiv_updated": "2019-04-19T04:46:29Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/pdf/1701.00185.pdf",
        "tag": [
            "http://www.semanlink.net/tag/surprises_me",
            "http://www.semanlink.net/tag/short_text_clustering",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/convolutional_neural_network_and_nn_4_nlp"
        ],
        "comment": "> We propose a flexible short text clustering framework which explores the feasibility and effectiveness of combining CNN and traditional unsupervised dimensionality reduction methods.\r\n>\r\n> Non-biased deep feature representations can be learned through our self- taught CNN framework which does not use any external tags/labels or complicated NLP pre-processing.\r\n\r\n> The original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations.\r\n\r\n[conf paper, same authors](http://www.aclweb.org/anthology/W15-1509) ; [gitgub repo (matlab)](https://github.com/jacoxu/STC2)\r\n",
        "title": "[1701.00185] Self-Taught Convolutional Neural Networks for Short Text Clustering",
        "relatedDoc": [],
        "creationTime": "2017-11-04T19:27:04Z",
        "creationDate": "2017-11-04",
        "bookmarkOf": [],
        "arxiv_author": [
            "Guanhua Tian",
            "Bo Xu",
            "Jiaming Xu",
            "Jun Zhao",
            "Peng Wang",
            "Suncong Zheng"
        ],
        "arxiv_summary": "Short text clustering is a challenging problem due to its sparseness of text\nrepresentation. Here we propose a flexible Self-Taught Convolutional neural\nnetwork framework for Short Text Clustering (dubbed STC^2), which can flexibly\nand successfully incorporate more useful semantic features and learn non-biased\ndeep text representation in an unsupervised manner. In our framework, the\noriginal raw text features are firstly embedded into compact binary codes by\nusing one existing unsupervised dimensionality reduction methods. Then, word\nembeddings are explored and fed into convolutional neural networks to learn\ndeep feature representations, meanwhile the output units are used to fit the\npre-trained binary codes in the training process. Finally, we get the optimal\nclusters by employing K-means to cluster the learned representations. Extensive\nexperimental results demonstrate that the proposed framework is effective,\nflexible and outperform several popular clustering methods when tested on three\npublic short text datasets.",
        "arxiv_firstAuthor": "Jiaming Xu",
        "arxiv_title": "Self-Taught Convolutional Neural Networks for Short Text Clustering",
        "arxiv_num": "1701.00185",
        "arxiv_published": "2017-01-01T01:57:59Z",
        "arxiv_updated": "2017-01-01T01:57:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_1802_07569_continual_lifelong",
        "tag": [
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/continual_learning",
            "http://www.semanlink.net/tag/catastrophic_forgetting"
        ],
        "comment": "",
        "title": "[1802.07569] Continual Lifelong Learning with Neural Networks: A Review",
        "relatedDoc": [],
        "creationTime": "2020-01-01T12:12:08Z",
        "creationDate": "2020-01-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/1802.07569"
        ],
        "arxiv_author": [
            "German I. Parisi",
            "Stefan Wermter",
            "Christopher Kanan",
            "Jose L. Part",
            "Ronald Kemker"
        ],
        "arxiv_summary": "Humans and animals have the ability to continually acquire, fine-tune, and\ntransfer knowledge and skills throughout their lifespan. This ability, referred\nto as lifelong learning, is mediated by a rich set of neurocognitive mechanisms\nthat together contribute to the development and specialization of our\nsensorimotor skills as well as to long-term memory consolidation and retrieval.\nConsequently, lifelong learning capabilities are crucial for autonomous agents\ninteracting in the real world and processing continuous streams of information.\nHowever, lifelong learning remains a long-standing challenge for machine\nlearning and neural network models since the continual acquisition of\nincrementally available information from non-stationary data distributions\ngenerally leads to catastrophic forgetting or interference. This limitation\nrepresents a major drawback for state-of-the-art deep neural network models\nthat typically learn representations from stationary batches of training data,\nthus without accounting for situations in which information becomes\nincrementally available over time. In this review, we critically summarize the\nmain challenges linked to lifelong learning for artificial learning systems and\ncompare existing neural network approaches that alleviate, to different\nextents, catastrophic forgetting. We discuss well-established and emerging\nresearch motivated by lifelong learning factors in biological systems such as\nstructural plasticity, memory replay, curriculum and transfer learning,\nintrinsic motivation, and multisensory integration.",
        "arxiv_firstAuthor": "German I. Parisi",
        "arxiv_title": "Continual Lifelong Learning with Neural Networks: A Review",
        "arxiv_num": "1802.07569",
        "arxiv_published": "2018-02-21T13:53:35Z",
        "arxiv_updated": "2019-02-11T01:28:39Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_2002_11402_detecting_potentia",
        "tag": [
            "http://www.semanlink.net/tag/conditional_random_field",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/bert",
            "http://www.semanlink.net/tag/wikipedia",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "",
        "title": "[2002.11402] Detecting Potential Topics In News Using BERT, CRF and Wikipedia",
        "relatedDoc": [],
        "creationTime": "2020-02-27T23:36:54Z",
        "creationDate": "2020-02-27",
        "bookmarkOf": [
            "https://arxiv.org/abs/2002.11402"
        ],
        "arxiv_author": [
            "Swapnil Ashok Jadhav"
        ],
        "arxiv_summary": "For a news content distribution platform like Dailyhunt, Named Entity\nRecognition is a pivotal task for building better user recommendation and\nnotification algorithms. Apart from identifying names, locations, organisations\nfrom the news for 13+ Indian languages and use them in algorithms, we also need\nto identify n-grams which do not necessarily fit in the definition of\nNamed-Entity, yet they are important. For example, \"me too movement\", \"beef\nban\", \"alwar mob lynching\". In this exercise, given an English language text,\nwe are trying to detect case-less n-grams which convey important information\nand can be used as topics and/or hashtags for a news. Model is built using\nWikipedia titles data, private English news corpus and BERT-Multilingual\npre-trained model, Bi-GRU and CRF architecture. It shows promising results when\ncompared with industry best Flair, Spacy and Stanford-caseless-NER in terms of\nF1 and especially Recall.",
        "arxiv_firstAuthor": "Swapnil Ashok Jadhav",
        "arxiv_title": "Detecting Potential Topics In News Using BERT, CRF and Wikipedia",
        "arxiv_num": "2002.11402",
        "arxiv_published": "2020-02-26T10:48:53Z",
        "arxiv_updated": "2020-02-28T18:44:07Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/06/a_tutorial_on_distance_metric_l",
        "tag": [
            "http://www.semanlink.net/tag/similarity_learning",
            "http://www.semanlink.net/tag/tutorial",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "distance metric learning, a branch of machine learning that aims to learn distances from the data",
        "title": "[1812.05944] A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms and Experiments",
        "relatedDoc": [],
        "creationTime": "2019-06-18T10:41:40Z",
        "creationDate": "2019-06-18",
        "bookmarkOf": [
            "https://arxiv.org/abs/1812.05944"
        ],
        "arxiv_author": [
            "Salvador Garc\u00eda",
            "Francisco Herrera",
            "Juan Luis Su\u00e1rez"
        ],
        "arxiv_summary": "Distance metric learning is a branch of machine learning that aims to learn\ndistances from the data. Distance metric learning can be useful to improve\nsimilarity learning algorithms, and also has applications in dimensionality\nreduction. This paper describes the distance metric learning problem and\nanalyzes its main mathematical foundations. In addition, it also discusses some\nof the most popular distance metric learning techniques used in classification,\nshowing their goals and the required information to understand and use them.\nFurthermore, some experiments to evaluate the performance of the different\nalgorithms are also provided. Finally, this paper discusses several\npossibilities of future work in this topic.",
        "arxiv_firstAuthor": "Juan Luis Su\u00e1rez",
        "arxiv_title": "A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms and Experiments",
        "arxiv_num": "1812.05944",
        "arxiv_published": "2018-12-14T14:07:36Z",
        "arxiv_updated": "2019-12-17T14:42:23Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/04/2007_15779_domain_specific_la",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/biomedical_nlp",
            "http://www.semanlink.net/tag/domain_specific_bert"
        ],
        "comment": "> A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models",
        "title": "[2007.15779] Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
        "relatedDoc": [],
        "creationTime": "2021-04-11T16:38:59Z",
        "creationDate": "2021-04-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/2007.15779"
        ],
        "arxiv_author": [
            "Tristan Naumann",
            "Michael Lucas",
            "Yu Gu",
            "Naoto Usuyama",
            "Jianfeng Gao",
            "Hoifung Poon",
            "Xiaodong Liu",
            "Robert Tinn",
            "Hao Cheng"
        ],
        "arxiv_summary": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
        "arxiv_firstAuthor": "Yu Gu",
        "arxiv_title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
        "arxiv_num": "2007.15779",
        "arxiv_published": "2020-07-31T00:04:15Z",
        "arxiv_updated": "2021-02-11T19:13:59Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1806.06259",
        "tag": [
            "http://www.semanlink.net/tag/elmo",
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embedding_evaluation"
        ],
        "comment": "a simple approach using bag-of-words with a recently introduced language model for deep context-dependent word embeddings proved to yield better results in many tasks when compared to sentence encoders trained on entailment datasets\r\n\r\n> We also show, however, that we are still far away from a universal encoder that can perform consistently across several downstream tasks.\r\n\r\n\r\n\r\n\r\n",
        "title": "[1806.06259] Evaluation of sentence embeddings in downstream and linguistic probing tasks",
        "relatedDoc": [],
        "creationTime": "2018-06-19T10:15:34Z",
        "creationDate": "2018-06-19",
        "bookmarkOf": [],
        "arxiv_author": [
            "Roberto Silveira",
            "Thomas S. Paula",
            "Christian S. Perone"
        ],
        "arxiv_summary": "Despite the fast developmental pace of new sentence embedding methods, it is\nstill challenging to find comprehensive evaluations of these different\ntechniques. In the past years, we saw significant improvements in the field of\nsentence embeddings and especially towards the development of universal\nsentence encoders that could provide inductive transfer to a wide variety of\ndownstream tasks. In this work, we perform a comprehensive evaluation of recent\nmethods using a wide variety of downstream and linguistic feature probing\ntasks. We show that a simple approach using bag-of-words with a recently\nintroduced language model for deep context-dependent word embeddings proved to\nyield better results in many tasks when compared to sentence encoders trained\non entailment datasets. We also show, however, that we are still far away from\na universal encoder that can perform consistently across several downstream\ntasks.",
        "arxiv_firstAuthor": "Christian S. Perone",
        "arxiv_title": "Evaluation of sentence embeddings in downstream and linguistic probing tasks",
        "arxiv_num": "1806.06259",
        "arxiv_published": "2018-06-16T16:07:49Z",
        "arxiv_updated": "2018-06-16T16:07:49Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1710.04087",
        "tag": [
            "http://www.semanlink.net/tag/guillaume_lample",
            "http://www.semanlink.net/tag/unsupervised_machine_translation",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/word_embedding",
            "http://www.semanlink.net/tag/machine_translation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ludovic_denoyer"
        ],
        "comment": "> we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way",
        "title": "[1710.04087] Word Translation Without Parallel Data",
        "relatedDoc": [],
        "creationTime": "2017-10-14T13:56:33Z",
        "creationDate": "2017-10-14",
        "bookmarkOf": [],
        "arxiv_author": [
            "Guillaume Lample",
            "Alexis Conneau",
            "Herv\u00e9 J\u00e9gou",
            "Marc'Aurelio Ranzato",
            "Ludovic Denoyer"
        ],
        "arxiv_summary": "State-of-the-art methods for learning cross-lingual word embeddings have\nrelied on bilingual dictionaries or parallel corpora. Recent studies showed\nthat the need for parallel data supervision can be alleviated with\ncharacter-level information. While these methods showed encouraging results,\nthey are not on par with their supervised counterparts and are limited to pairs\nof languages sharing a common alphabet. In this work, we show that we can build\na bilingual dictionary between two languages without using any parallel\ncorpora, by aligning monolingual word embedding spaces in an unsupervised way.\nWithout using any character information, our model even outperforms existing\nsupervised methods on cross-lingual tasks for some language pairs. Our\nexperiments demonstrate that our method works very well also for distant\nlanguage pairs, like English-Russian or English-Chinese. We finally describe\nexperiments on the English-Esperanto low-resource language pair, on which there\nonly exists a limited amount of parallel data, to show the potential impact of\nour method in fully unsupervised machine translation. Our code, embeddings and\ndictionaries are publicly available.",
        "arxiv_firstAuthor": "Alexis Conneau",
        "arxiv_title": "Word Translation Without Parallel Data",
        "arxiv_num": "1710.04087",
        "arxiv_published": "2017-10-11T14:24:28Z",
        "arxiv_updated": "2018-01-30T14:41:51Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/09/2109_08133_phrase_retrieval_l",
        "tag": [
            "http://www.semanlink.net/tag/emnlp_2021",
            "http://www.semanlink.net/tag/dense_passage_retrieval",
            "http://www.semanlink.net/tag/discute_avec_raphael",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "[Github](doc:2021/09/princeton_nlp_densephrases_acl)",
        "title": "[2109.08133] Phrase Retrieval Learns Passage Retrieval, Too",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/09/princeton_nlp_densephrases_acl"
        ],
        "creationTime": "2021-09-30T14:50:09Z",
        "creationDate": "2021-09-30",
        "bookmarkOf": [
            "https://arxiv.org/abs/2109.08133"
        ],
        "arxiv_author": [
            "Jinhyuk Lee",
            "Danqi Chen",
            "Alexander Wettig"
        ],
        "arxiv_summary": "Dense retrieval methods have shown great promise over sparse retrieval\nmethods in a range of NLP problems. Among them, dense phrase retrieval-the most\nfine-grained retrieval unit-is appealing because phrases can be directly used\nas the output for question answering and slot filling tasks. In this work, we\nfollow the intuition that retrieving phrases naturally entails retrieving\nlarger text blocks and study whether phrase retrieval can serve as the basis\nfor coarse-level retrieval including passages and documents. We first observe\nthat a dense phrase-retrieval system, without any retraining, already achieves\nbetter passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage\nretrievers, which also helps achieve superior end-to-end QA performance with\nfewer passages. Then, we provide an interpretation for why phrase-level\nsupervision helps learn better fine-grained entailment compared to\npassage-level supervision, and also show that phrase retrieval can be improved\nto achieve competitive performance in document-retrieval tasks such as entity\nlinking and knowledge-grounded dialogue. Finally, we demonstrate how phrase\nfiltering and vector quantization can reduce the size of our index by 4-10x,\nmaking dense phrase retrieval a practical and versatile solution in\nmulti-granularity retrieval.",
        "arxiv_firstAuthor": "Jinhyuk Lee",
        "arxiv_title": "Phrase Retrieval Learns Passage Retrieval, Too",
        "arxiv_num": "2109.08133",
        "arxiv_published": "2021-09-16T17:42:45Z",
        "arxiv_updated": "2021-09-16T17:42:45Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1902.09229",
        "tag": [
            "http://www.semanlink.net/tag/noise_contrastive_estimation",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/embeddings",
            "http://www.semanlink.net/tag/sanjeev_arora"
        ],
        "comment": "[blog post](/doc/?uri=http%3A%2F%2Fwww.offconvex.org%2F2019%2F03%2F19%2FCURL%2F)",
        "title": "[1902.09229] A Theoretical Analysis of Contrastive Unsupervised Representation Learning",
        "relatedDoc": [
            "http://www.offconvex.org/2019/03/19/CURL/"
        ],
        "creationTime": "2019-03-20T16:57:53Z",
        "creationDate": "2019-03-20",
        "bookmarkOf": [],
        "arxiv_author": [
            "Nikunj Saunshi",
            "Mikhail Khodak",
            "Hrishikesh Khandeparkar",
            "Orestis Plevrakis",
            "Sanjeev Arora"
        ],
        "arxiv_summary": "Recent empirical works have successfully used unlabeled data to learn feature\nrepresentations that are broadly useful in downstream classification tasks.\nSeveral of these methods are reminiscent of the well-known word2vec embedding\nalgorithm: leveraging availability of pairs of semantically \"similar\" data\npoints and \"negative samples,\" the learner forces the inner product of\nrepresentations of similar pairs with each other to be higher on average than\nwith negative samples. The current paper uses the term contrastive learning for\nsuch algorithms and presents a theoretical framework for analyzing them by\nintroducing latent classes and hypothesizing that semantically similar points\nare sampled from the same latent class. This framework allows us to show\nprovable guarantees on the performance of the learned representations on the\naverage classification task that is comprised of a subset of the same set of\nlatent classes. Our generalization bound also shows that learned\nrepresentations can reduce (labeled) sample complexity on downstream tasks. We\nconduct controlled experiments in both the text and image domains to support\nthe theory.",
        "arxiv_firstAuthor": "Sanjeev Arora",
        "arxiv_title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning",
        "arxiv_num": "1902.09229",
        "arxiv_published": "2019-02-25T12:32:15Z",
        "arxiv_updated": "2019-02-25T12:32:15Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/06/2106_00882_efficient_passage_",
        "tag": [
            "http://www.semanlink.net/tag/learning_to_hash",
            "http://www.semanlink.net/tag/open_domain_question_answering",
            "http://www.semanlink.net/tag/dense_passage_retrieval",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ikuya_yamada"
        ],
        "comment": "> Integrates a learning to hash technique into [DPR](tag:dense_passage_retrieval) to represent passages using compact binary codes rather than continuous vectors. We simultaneously train the encoders and hash functions in an end-to-end manner.",
        "title": "[2106.00882] Efficient Passage Retrieval with Hashing for Open-domain Question Answering",
        "relatedDoc": [],
        "creationTime": "2021-06-03T11:11:35Z",
        "creationDate": "2021-06-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2106.00882"
        ],
        "arxiv_author": [
            "Ikuya Yamada",
            "Hannaneh Hajishirzi",
            "Akari Asai"
        ],
        "arxiv_summary": "Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.",
        "arxiv_firstAuthor": "Ikuya Yamada",
        "arxiv_title": "Efficient Passage Retrieval with Hashing for Open-domain Question Answering",
        "arxiv_num": "2106.00882",
        "arxiv_published": "2021-06-02T01:34:42Z",
        "arxiv_updated": "2021-06-02T01:34:42Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/06/1804_03235_large_scale_distri",
        "tag": [
            "http://www.semanlink.net/tag/geoffrey_hinton",
            "http://www.semanlink.net/tag/ml_google",
            "http://www.semanlink.net/tag/critical_evaluation",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": ">  we use *codistillation* to refer to distillation performed:\r\n> 1. using the same architecture for all the models;\r\n> 2. using the same dataset to train all the models; and\r\n> 3. using the distillation loss during training before any model has fully converged.\r\n\r\n> In general, we believe the quality gains of codistillation over well-tuned offline distillation will be\r\nminor in practice and the more interesting research direction is exploring codistillation as a distributed\r\ntraining algorithm\r\n\r\n> Codistillation with\r\nthe same data seems to be slightly better than the baseline, but codistillation using different data\r\ngets much better results. These results show that the codistilling models are indeed successfully\r\ntransmitting useful information about different parts of the training data to each other.\r\n\r\nRelated to [\"Deep mutual learning\"](doc:2020/05/1706_00384_deep_mutual_learni) paper",
        "title": "[1804.03235] Large scale distributed neural network training through online distillation",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/05/1706_00384_deep_mutual_learni"
        ],
        "creationTime": "2020-06-06T16:51:26Z",
        "creationDate": "2020-06-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/1804.03235"
        ],
        "arxiv_author": [
            "Rohan Anil",
            "Gabriel Pereyra",
            "Alexandre Passos",
            "Robert Ormandi",
            "George E. Dahl",
            "Geoffrey E. Hinton"
        ],
        "arxiv_summary": "Techniques such as ensembling and distillation promise model quality\nimprovements when paired with almost any base model. However, due to increased\ntest-time cost (for ensembles) and increased complexity of the training\npipeline (for distillation), these techniques are challenging to use in\nindustrial settings. In this paper we explore a variant of distillation which\nis relatively straightforward to use as it does not require a complicated\nmulti-stage setup or many new hyperparameters. Our first claim is that online\ndistillation enables us to use extra parallelism to fit very large datasets\nabout twice as fast. Crucially, we can still speed up training even after we\nhave already reached the point at which additional parallelism provides no\nbenefit for synchronous or asynchronous stochastic gradient descent. Two neural\nnetworks trained on disjoint subsets of the data can share knowledge by\nencouraging each model to agree with the predictions the other model would have\nmade. These predictions can come from a stale version of the other model so\nthey can be safely computed using weights that only rarely get transmitted. Our\nsecond claim is that online distillation is a cost-effective way to make the\nexact predictions of a model dramatically more reproducible. We support our\nclaims using experiments on the Criteo Display Ad Challenge dataset, ImageNet,\nand the largest to-date dataset used for neural language modeling, containing\n$6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",
        "arxiv_firstAuthor": "Rohan Anil",
        "arxiv_title": "Large scale distributed neural network training through online distillation",
        "arxiv_num": "1804.03235",
        "arxiv_published": "2018-04-09T20:56:03Z",
        "arxiv_updated": "2018-04-09T20:56:03Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/1910_12507_a_survey_on_knowle",
        "tag": [
            "http://www.semanlink.net/tag/text_aware_kg_embedding",
            "http://www.semanlink.net/tag/knowledge_graph_embeddings",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1910.12507] A Survey on Knowledge Graph Embeddings with Literals: Which model links better Literal-ly?",
        "relatedDoc": [],
        "creationTime": "2020-05-04T14:56:43Z",
        "creationDate": "2020-05-04",
        "bookmarkOf": [
            "https://arxiv.org/abs/1910.12507"
        ],
        "arxiv_author": [
            "Russa Biswas",
            "Genet Asefa Gesese",
            "Harald Sack",
            "Mehwish Alam"
        ],
        "arxiv_summary": "Knowledge Graphs (KGs) are composed of structured information about a\nparticular domain in the form of entities and relations. In addition to the\nstructured information KGs help in facilitating interconnectivity and\ninteroperability between different resources represented in the Linked Data\nCloud. KGs have been used in a variety of applications such as entity linking,\nquestion answering, recommender systems, etc. However, KG applications suffer\nfrom high computational and storage costs. Hence, there arises the necessity\nfor a representation able to map the high dimensional KGs into low dimensional\nspaces, i.e., embedding space, preserving structural as well as relational\ninformation. This paper conducts a survey of KG embedding models which not only\nconsider the structured information contained in the form of entities and\nrelations in a KG but also the unstructured information represented as literals\nsuch as text, numerical values, images, etc. Along with a theoretical analysis\nand comparison of the methods proposed so far for generating KG embeddings with\nliterals, an empirical evaluation of the different methods under identical\nsettings has been performed for the general task of link prediction.",
        "arxiv_firstAuthor": "Genet Asefa Gesese",
        "arxiv_title": "A Survey on Knowledge Graph Embeddings with Literals: Which model links better Literal-ly?",
        "arxiv_num": "1910.12507",
        "arxiv_published": "2019-10-28T09:06:00Z",
        "arxiv_updated": "2019-10-28T09:06:00Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/09/2106_04647_compacter_efficie",
        "tag": [
            "http://www.semanlink.net/tag/nlp_google",
            "http://www.semanlink.net/tag/sebastian_ruder",
            "http://www.semanlink.net/tag/language_model_fine_tuning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/language_models_size"
        ],
        "comment": "> Compacter (Compact Adapter) layers, a method to adapt large-scale language models, which only trains around 0.05% of a model's parameters and performs on par with fine-tuning. [twitter](https://twitter.com/KarimiRabeeh/status/1404774464441794560)",
        "title": "[2106.04647] Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
        "relatedDoc": [],
        "creationTime": "2021-09-29T02:05:29Z",
        "creationDate": "2021-09-29",
        "bookmarkOf": [
            "https://arxiv.org/abs/2106.04647"
        ],
        "arxiv_author": [
            "James Henderson",
            "Rabeeh Karimi Mahabadi",
            "Sebastian Ruder"
        ],
        "arxiv_summary": "Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers.\nSpecifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared ``slow'' weights and ``fast'' rank-one\nmatrices defined per Compacter layer. By only training 0.047% of a pretrained\nmodel's parameters, Compacter performs on par with standard fine-tuning on GLUE\nand outperforms fine-tuning in low-resource settings. Our code is publicly\navailable in https://github.com/rabeehk/compacter/",
        "arxiv_firstAuthor": "Rabeeh Karimi Mahabadi",
        "arxiv_title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
        "arxiv_num": "2106.04647",
        "arxiv_published": "2021-06-08T19:17:04Z",
        "arxiv_updated": "2021-06-08T19:17:04Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/01/_2001_01447v1_improving_entity",
        "tag": [
            "http://www.semanlink.net/tag/entity_linking",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_microsoft",
            "http://www.semanlink.net/tag/entity_type_representation"
        ],
        "comment": "",
        "title": "[2001.01447] Improving Entity Linking by Modeling Latent Entity Type Information",
        "relatedDoc": [],
        "creationTime": "2020-01-09T02:37:01Z",
        "creationDate": "2020-01-09",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.01447"
        ],
        "arxiv_author": [
            "Chin-Yew Lin",
            "Feng Jiang",
            "Shuang Chen",
            "Jinpeng Wang"
        ],
        "arxiv_summary": "Existing state of the art neural entity linking models employ attention-based\nbag-of-words context model and pre-trained entity embeddings bootstrapped from\nword embeddings to assess topic level context compatibility. However, the\nlatent entity type information in the immediate context of the mention is\nneglected, which causes the models often link mentions to incorrect entities\nwith incorrect type. To tackle this problem, we propose to inject latent entity\ntype information into the entity embeddings based on pre-trained BERT. In\naddition, we integrate a BERT-based entity similarity score into the local\ncontext model of a state-of-the-art model to better capture latent entity type\ninformation. Our model significantly outperforms the state-of-the-art entity\nlinking models on standard benchmark (AIDA-CoNLL). Detailed experiment analysis\ndemonstrates that our model corrects most of the type errors produced by the\ndirect baseline.",
        "arxiv_firstAuthor": "Shuang Chen",
        "arxiv_title": "Improving Entity Linking by Modeling Latent Entity Type Information",
        "arxiv_num": "2001.01447",
        "arxiv_published": "2020-01-06T09:18:29Z",
        "arxiv_updated": "2020-01-06T09:18:29Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2001_08053_contextualized_emb",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/patrick_gallinari",
            "http://www.semanlink.net/tag/ner_unseen_mentions",
            "http://www.semanlink.net/tag/named_entity_recognition"
        ],
        "comment": "> In this paper, we quantify the impact of ELMo, Flair and BERT representations on generalization to unseen mentions and new domains in NER.",
        "title": "[2001.08053] Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization",
        "relatedDoc": [],
        "creationTime": "2020-10-01T11:43:28Z",
        "creationDate": "2020-10-01",
        "bookmarkOf": [
            "https://arxiv.org/abs/2001.08053"
        ],
        "arxiv_author": [
            "Patrick Gallinari",
            "Vincent Guigue",
            "Bruno Taill\u00e9"
        ],
        "arxiv_summary": "Contextualized embeddings use unsupervised language model pretraining to\ncompute word representations depending on their context. This is intuitively\nuseful for generalization, especially in Named-Entity Recognition where it is\ncrucial to detect mentions never seen during training. However, standard\nEnglish benchmarks overestimate the importance of lexical over contextual\nfeatures because of an unrealistic lexical overlap between train and test\nmentions. In this paper, we perform an empirical analysis of the generalization\ncapabilities of state-of-the-art contextualized embeddings by separating\nmentions by novelty and with out-of-domain evaluation. We show that they are\nparticularly beneficial for unseen mentions detection, especially\nout-of-domain. For models trained on CoNLL03, language model contextualization\nleads to a +1.2% maximal relative micro-F1 score increase in-domain against\n+13% out-of-domain on the WNUT dataset",
        "arxiv_firstAuthor": "Bruno Taill\u00e9",
        "arxiv_title": "Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization",
        "arxiv_num": "2001.08053",
        "arxiv_published": "2020-01-22T15:15:34Z",
        "arxiv_updated": "2020-01-22T15:15:34Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/1503_02531_distilling_the_kno",
        "tag": [
            "http://www.semanlink.net/tag/geoffrey_hinton",
            "http://www.semanlink.net/tag/knowledge_distillation",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> **a different kind of training**, which we call \u201c**distillation**\u201d to transfer the\r\nknowledge from the cumbersome model to a small model that is more\r\nsuitable for deployment\r\n\r\n\r\n> Caruana and his collaborators have shown that it is possible to compress the knowledge in an [#ensemble](/tag/ensemble_learning.html) into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST.",
        "title": "[1503.02531] Distilling the Knowledge in a Neural Network",
        "relatedDoc": [],
        "creationTime": "2020-04-16T14:40:33Z",
        "creationDate": "2020-04-16",
        "bookmarkOf": [
            "https://arxiv.org/abs/1503.02531"
        ],
        "arxiv_author": [
            "Geoffrey Hinton",
            "Jeff Dean",
            "Oriol Vinyals"
        ],
        "arxiv_summary": "A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.",
        "arxiv_firstAuthor": "Geoffrey Hinton",
        "arxiv_title": "Distilling the Knowledge in a Neural Network",
        "arxiv_num": "1503.02531",
        "arxiv_published": "2015-03-09T15:44:49Z",
        "arxiv_updated": "2015-03-09T15:44:49Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/03/2010_12321_barthez_a_skilled",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/nlp_french"
        ],
        "comment": "[On HuggingFace](doc:2021/03/barthez_transformers_4_5_0_de) ;\r\n[GitHub](https://github.com/moussaKam/BARThez)\r\n\r\n([same author](doc:?uri=https%3A%2F%2Fwww2018.thewebconf.org%2Fprogram%2Ftutorials-track%2Ftutorial-213%2F))",
        "title": "[2010.12321] BARThez: a Skilled Pretrained French Sequence-to-Sequence Model",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/03/barthez_transformers_4_5_0_de",
            "https://www2018.thewebconf.org/program/tutorials-track/tutorial-213/"
        ],
        "creationTime": "2021-03-31T19:08:05Z",
        "creationDate": "2021-03-31",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.12321"
        ],
        "arxiv_author": [
            "Antoine J. -P. Tixier",
            "Moussa Kamal Eddine",
            "Michalis Vazirgiannis"
        ],
        "arxiv_summary": "Inductive transfer learning has taken the entire NLP field by storm, with\nmodels such as BERT and BART setting new state of the art on countless NLU\ntasks. However, most of the available models and research have been conducted\nfor English. In this work, we introduce BARThez, the first large-scale\npretrained seq2seq model for French. Being based on BART, BARThez is\nparticularly well-suited for generative tasks. We evaluate BARThez on five\ndiscriminative tasks from the FLUE benchmark and two generative tasks from a\nnovel summarization dataset, OrangeSum, that we created for this research. We\nshow BARThez to be very competitive with state-of-the-art BERT-based French\nlanguage models such as CamemBERT and FlauBERT. We also continue the\npretraining of a multilingual BART on BARThez' corpus, and show our resulting\nmodel, mBARThez, to significantly boost BARThez' generative performance. Code,\ndata and models are publicly available.",
        "arxiv_firstAuthor": "Moussa Kamal Eddine",
        "arxiv_title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model",
        "arxiv_num": "2010.12321",
        "arxiv_published": "2020-10-23T11:57:33Z",
        "arxiv_updated": "2021-02-09T09:31:57Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/02/_1911_05507_compressive_transf",
        "tag": [
            "http://www.semanlink.net/tag/nlp_long_documents",
            "http://www.semanlink.net/tag/memory_in_deep_learning",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/google_deepmind",
            "http://www.semanlink.net/tag/attention_is_all_you_need",
            "http://www.semanlink.net/tag/sequence_to_sequence_learning"
        ],
        "comment": "> the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning.\r\n\r\n[Blog post](/doc/2020/02/a_new_model_and_dataset_for_lon)",
        "title": "[1911.05507] Compressive Transformers for Long-Range Sequence Modelling",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2020/02/a_new_model_and_dataset_for_lon"
        ],
        "creationTime": "2020-02-11T08:48:20Z",
        "creationDate": "2020-02-11",
        "bookmarkOf": [
            "https://arxiv.org/abs/1911.05507"
        ],
        "arxiv_author": [
            "Siddhant M. Jayakumar",
            "Anna Potapenko",
            "Jack W. Rae",
            "Timothy P. Lillicrap"
        ],
        "arxiv_summary": "We present the Compressive Transformer, an attentive sequence model which\ncompresses past memories for long-range sequence learning. We find the\nCompressive Transformer obtains state-of-the-art language modelling results in\nthe WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc\nrespectively. We also find it can model high-frequency speech effectively and\ncan be used as a memory mechanism for RL, demonstrated on an object matching\ntask. To promote the domain of long-range sequence learning, we propose a new\nopen-vocabulary language modelling benchmark derived from books, PG-19.",
        "arxiv_firstAuthor": "Jack W. Rae",
        "arxiv_title": "Compressive Transformers for Long-Range Sequence Modelling",
        "arxiv_num": "1911.05507",
        "arxiv_published": "2019-11-13T14:36:01Z",
        "arxiv_updated": "2019-11-13T14:36:01Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1608.05426",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/cross_lingual_word_embeddings",
            "http://www.semanlink.net/tag/yoav_goldberg"
        ],
        "comment": "",
        "title": "[1608.05426] A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments",
        "relatedDoc": [],
        "creationTime": "2018-07-23T12:54:24Z",
        "creationDate": "2018-07-23",
        "bookmarkOf": [],
        "arxiv_author": [
            "Anders S\u00f8gaard",
            "Yoav Goldberg",
            "Omer Levy"
        ],
        "arxiv_summary": "While cross-lingual word embeddings have been studied extensively in recent\nyears, the qualitative differences between the different algorithms remain\nvague. We observe that whether or not an algorithm uses a particular feature\nset (sentence IDs) accounts for a significant performance gap among these\nalgorithms. This feature set is also used by traditional alignment algorithms,\nsuch as IBM Model-1, which demonstrate similar performance to state-of-the-art\nembedding algorithms on a variety of benchmarks. Overall, we observe that\ndifferent algorithmic approaches for utilizing the sentence ID feature space\nresult in similar performance. This paper draws both empirical and theoretical\nparallels between the embedding and alignment literature, and suggests that\nadding additional sources of information, which go beyond the traditional\nsignal of bilingual sentence-aligned corpora, may substantially improve\ncross-lingual word embeddings, and that future baselines should at least take\nsuch features into account.",
        "arxiv_firstAuthor": "Omer Levy",
        "arxiv_title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments",
        "arxiv_num": "1608.05426",
        "arxiv_published": "2016-08-18T20:27:46Z",
        "arxiv_updated": "2017-01-09T20:49:18Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/04/cmp_lg_9511007_using_informat",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/taxonomies"
        ],
        "comment": "",
        "title": "[cmp-lg/9511007] Using Information Content to Evaluate Semantic Similarity in a Taxonomy (1995)",
        "relatedDoc": [],
        "creationTime": "2020-04-27T17:22:44Z",
        "creationDate": "2020-04-27",
        "bookmarkOf": [
            "https://arxiv.org/abs/cmp-lg/9511007"
        ],
        "arxiv_author": [
            "Philip Resnik"
        ],
        "arxiv_summary": "This paper presents a new measure of semantic similarity in an IS-A taxonomy,\nbased on the notion of information content. Experimental evaluation suggests\nthat the measure performs encouragingly well (a correlation of r = 0.79 with a\nbenchmark set of human similarity judgments, with an upper bound of r = 0.90\nfor human subjects performing the same task), and significantly better than the\ntraditional edge counting approach (r = 0.66).",
        "arxiv_firstAuthor": "Philip Resnik",
        "arxiv_title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy",
        "arxiv_num": "cmp-lg/9511007",
        "arxiv_published": "1995-11-29T19:32:04Z",
        "arxiv_updated": "1995-11-29T19:32:04Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1802.01021",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_linking"
        ],
        "comment": "",
        "title": "[1802.01021] DeepType: Multilingual Entity Linking by Neural Type System Evolution",
        "relatedDoc": [],
        "creationTime": "2019-04-25T16:06:44Z",
        "creationDate": "2019-04-25",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jonathan Raiman",
            "Olivier Raiman"
        ],
        "arxiv_summary": "The wealth of structured (e.g. Wikidata) and unstructured data about the\nworld available today presents an incredible opportunity for tomorrow's\nArtificial Intelligence. So far, integration of these two different modalities\nis a difficult process, involving many decisions concerning how best to\nrepresent the information so that it will be captured or useful, and\nhand-labeling large amounts of data. DeepType overcomes this challenge by\nexplicitly integrating symbolic information into the reasoning process of a\nneural network with a type system. First we construct a type system, and\nsecond, we use it to constrain the outputs of a neural network to respect the\nsymbolic structure. We achieve this by reformulating the design problem into a\nmixed integer problem: create a type system and subsequently train a neural\nnetwork with it. In this reformulation discrete variables select which\nparent-child relations from an ontology are types within the type system, while\ncontinuous variables control a classifier fit to the type system. The original\nproblem cannot be solved exactly, so we propose a 2-step algorithm: 1)\nheuristic search or stochastic optimization over discrete variables that define\na type system informed by an Oracle and a Learnability heuristic, 2) gradient\ndescent to fit classifier parameters. We apply DeepType to the problem of\nEntity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC\nKBP 2010) and find that it outperforms all existing solutions by a wide margin,\nincluding approaches that rely on a human-designed type system or recent deep\nlearning-based entity embeddings, while explicitly using symbolic information\nlets it integrate new entities without retraining.",
        "arxiv_firstAuthor": "Jonathan Raiman",
        "arxiv_title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution",
        "arxiv_num": "1802.01021",
        "arxiv_published": "2018-02-03T20:13:42Z",
        "arxiv_updated": "2018-02-03T20:13:42Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://arxiv.org/abs/0807.4145",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/hypothese_de_riemann",
            "http://www.semanlink.net/tag/jean_paul"
        ],
        "comment": "> we explore a class of equivalence relations over N* from which is constructed a sequence of symetric matrices related to the Mertens function. From numerical experimentations we suggest a conjecture, about the growth of the quadratic norm of these matrices, which implies the Riemann hypothesis. This suggests that matrix analysis methods may play a more important part in this classical and difficult problem.",
        "title": "[0807.4145] Une suite de matrices sym\u00e9triques en rapport avec la fonction de Mertens",
        "relatedDoc": [],
        "creationTime": "2008-08-17T12:29:55Z",
        "creationDate": "2008-08-17",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jean-Paul Cardinal"
        ],
        "arxiv_summary": "In this paper we explore a class of equivalence relations over $\\N^\\ast$ from\nwhich is constructed a sequence of symetric matrices related to the Mertens\nfunction. From numerical experimentations we suggest a conjecture, about the\ngrowth of the quadratic norm of these matrices, which implies the Riemann\nhypothesis. This suggests that matrix analysis methods may play a more\nimportant part in this classical and difficult problem.",
        "arxiv_firstAuthor": "Jean-Paul Cardinal",
        "arxiv_title": "Une suite de matrices sym\u00e9triques en rapport avec la fonction de Mertens",
        "arxiv_num": "0807.4145",
        "arxiv_published": "2008-07-25T17:34:34Z",
        "arxiv_updated": "2016-02-27T19:27:43Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2005_03675_machine_learning_o",
        "tag": [
            "http://www.semanlink.net/tag/graphs_machine_learning",
            "http://www.semanlink.net/tag/survey",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "> we aim to **bridge the gap between graph neural networks, network embedding and graph regularization models**. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach.",
        "title": "[2005.03675] Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
        "relatedDoc": [],
        "creationTime": "2020-10-03T15:14:22Z",
        "creationDate": "2020-10-03",
        "bookmarkOf": [
            "https://arxiv.org/abs/2005.03675"
        ],
        "arxiv_author": [
            "Christopher R\u00e9",
            "Kevin Murphy",
            "Ines Chami",
            "Bryan Perozzi",
            "Sami Abu-El-Haija"
        ],
        "arxiv_summary": "There has been a surge of recent interest in learning representations for\ngraph-structured data. Graph representation learning methods have generally\nfallen into three main categories, based on the availability of labeled data.\nThe first, network embedding (such as shallow graph embedding or graph\nauto-encoders), focuses on learning unsupervised representations of relational\nstructure. The second, graph regularized neural networks, leverages graphs to\naugment neural network losses with a regularization objective for\nsemi-supervised learning. The third, graph neural networks, aims to learn\ndifferentiable functions over discrete topologies with arbitrary structure.\nHowever, despite the popularity of these areas there has been surprisingly\nlittle work on unifying the three paradigms. Here, we aim to bridge the gap\nbetween graph neural networks, network embedding and graph regularization\nmodels. We propose a comprehensive taxonomy of representation learning methods\nfor graph-structured data, aiming to unify several disparate bodies of work.\nSpecifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which\ngeneralizes popular algorithms for semi-supervised learning on graphs (e.g.\nGraphSage, Graph Convolutional Networks, Graph Attention Networks), and\nunsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc)\ninto a single consistent approach. To illustrate the generality of this\napproach, we fit over thirty existing methods into this framework. We believe\nthat this unifying view both provides a solid foundation for understanding the\nintuition behind these methods, and enables future research in the area.",
        "arxiv_firstAuthor": "Ines Chami",
        "arxiv_title": "Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
        "arxiv_num": "2005.03675",
        "arxiv_published": "2020-05-07T18:00:02Z",
        "arxiv_updated": "2020-05-07T18:00:02Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1604.06737",
        "tag": [
            "http://www.semanlink.net/tag/statistical_classification",
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/entity_embeddings",
            "http://www.semanlink.net/tag/categorical_variables"
        ],
        "comment": "> We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables",
        "title": "[1604.06737] Entity Embeddings of Categorical Variables",
        "relatedDoc": [],
        "creationTime": "2018-03-03T17:13:44Z",
        "creationDate": "2018-03-03",
        "bookmarkOf": [],
        "arxiv_author": [
            "Cheng Guo",
            "Felix Berkhahn"
        ],
        "arxiv_summary": "We map categorical variables in a function approximation problem into\nEuclidean spaces, which are the entity embeddings of the categorical variables.\nThe mapping is learned by a neural network during the standard supervised\ntraining process. Entity embedding not only reduces memory usage and speeds up\nneural networks compared with one-hot encoding, but more importantly by mapping\nsimilar values close to each other in the embedding space it reveals the\nintrinsic properties of the categorical variables. We applied it successfully\nin a recent Kaggle competition and were able to reach the third position with\nrelative simple features. We further demonstrate in this paper that entity\nembedding helps the neural network to generalize better when the data is sparse\nand statistics is unknown. Thus it is especially useful for datasets with lots\nof high cardinality features, where other methods tend to overfit. We also\ndemonstrate that the embeddings obtained from the trained neural network boost\nthe performance of all tested machine learning methods considerably when used\nas the input features instead. As entity embedding defines a distance measure\nfor categorical variables it can be used for visualizing categorical data and\nfor data clustering.",
        "arxiv_firstAuthor": "Cheng Guo",
        "arxiv_title": "Entity Embeddings of Categorical Variables",
        "arxiv_num": "1604.06737",
        "arxiv_published": "2016-04-22T16:34:30Z",
        "arxiv_updated": "2016-04-22T16:34:30Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/10/2104_12016_learning_passage_i",
        "tag": [
            "http://www.semanlink.net/tag/retrieval_based_nlp",
            "http://www.semanlink.net/tag/nlp_stanford",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "Mentionned in [Building Scalable, Explainable, and Adaptive NLP Models with Retrieval | SAIL Blog](doc:2021/10/building_scalable_explainable_)",
        "title": "[2104.12016] Learning Passage Impacts for Inverted Indexes",
        "relatedDoc": [
            "http://www.semanlink.net/doc/2021/10/building_scalable_explainable_"
        ],
        "creationTime": "2021-10-08T14:05:42Z",
        "creationDate": "2021-10-08",
        "bookmarkOf": [
            "https://arxiv.org/abs/2104.12016"
        ],
        "arxiv_author": [
            "Omar Khattab",
            "Antonio Mallia",
            "Nicola Tonellotto",
            "Torsten Suel"
        ],
        "arxiv_summary": "Neural information retrieval systems typically use a cascading pipeline, in\nwhich a first-stage model retrieves a candidate set of documents and one or\nmore subsequent stages re-rank this set using contextualized language models\nsuch as BERT. In this paper, we propose DeepImpact, a new document\nterm-weighting scheme suitable for efficient retrieval using a standard\ninverted index. Compared to existing methods, DeepImpact improves impact-score\nmodeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact\nleverages DocT5Query to enrich the document collection and, using a\ncontextualized language model, directly estimates the semantic importance of\ntokens in a document, producing a single-value representation for each token in\neach document. Our experiments show that DeepImpact significantly outperforms\nprior first-stage retrieval approaches by up to 17% on effectiveness metrics\nw.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the\nsame effectiveness of state-of-the-art approaches with up to 5.1x speedup in\nefficiency.",
        "arxiv_firstAuthor": "Antonio Mallia",
        "arxiv_title": "Learning Passage Impacts for Inverted Indexes",
        "arxiv_num": "2104.12016",
        "arxiv_published": "2021-04-24T20:18:53Z",
        "arxiv_updated": "2021-04-24T20:18:53Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1704.05358",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/sentence_embeddings",
            "http://www.semanlink.net/tag/sentence_similarity"
        ],
        "comment": "> We observe a simple geometry of sentences -- the word representations of a given sentence roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors.\r\n\r\nA sentence of N words is a matrix (300, N) (if 300 is the dim of the word embeddings space). We take the eg. 4 (hyperparam) heaviest singular values -> a subspace with dim 4\r\n\r\nSimilarity between docs: principal angle between the subspaces (reminiscent of cosine similarity)\r\n\r\n",
        "title": "[1704.05358] Representing Sentences as Low-Rank Subspaces",
        "relatedDoc": [],
        "creationTime": "2018-10-06T11:22:58Z",
        "creationDate": "2018-10-06",
        "bookmarkOf": [],
        "arxiv_author": [
            "Jiaqi Mu",
            "Suma Bhat",
            "Pramod Viswanath"
        ],
        "arxiv_summary": "Sentences are important semantic units of natural language. A generic,\ndistributional representation of sentences that can capture the latent\nsemantics is beneficial to multiple downstream applications. We observe a\nsimple geometry of sentences -- the word representations of a given sentence\n(on average 10.23 words in all SemEval datasets with a standard deviation 4.84)\nroughly lie in a low-rank subspace (roughly, rank 4). Motivated by this\nobservation, we represent a sentence by the low-rank subspace spanned by its\nword vectors. Such an unsupervised representation is empirically validated via\nsemantic textual similarity tasks on 19 different datasets, where it\noutperforms the sophisticated neural network models, including skip-thought\nvectors, by 15% on average.",
        "arxiv_firstAuthor": "Jiaqi Mu",
        "arxiv_title": "Representing Sentences as Low-Rank Subspaces",
        "arxiv_num": "1704.05358",
        "arxiv_published": "2017-04-18T14:30:32Z",
        "arxiv_updated": "2017-04-18T14:30:32Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/10/2010_11882_learning_invarianc",
        "tag": [
            "http://www.semanlink.net/tag/deep_learning",
            "http://www.semanlink.net/tag/convolutional_neural_network",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "how to *learn* symmetries -- rotations, translations, scalings, shears -- from training data alone",
        "title": "[2010.11882] Learning Invariances in Neural Networks",
        "relatedDoc": [],
        "creationTime": "2020-10-25T12:38:17Z",
        "creationDate": "2020-10-25",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.11882"
        ],
        "arxiv_author": [
            "Marc Finzi",
            "Andrew Gordon Wilson",
            "Gregory Benton",
            "Pavel Izmailov"
        ],
        "arxiv_summary": "Invariances to translations have imbued convolutional neural networks with\npowerful generalization properties. However, we often do not know a priori what\ninvariances are present in the data, or to what extent a model should be\ninvariant to a given symmetry group. We show how to \\emph{learn} invariances\nand equivariances by parameterizing a distribution over augmentations and\noptimizing the training loss simultaneously with respect to the network\nparameters and augmentation parameters. With this simple procedure we can\nrecover the correct set and extent of invariances on image classification,\nregression, segmentation, and molecular property prediction from a large space\nof augmentations, on training data alone.",
        "arxiv_firstAuthor": "Gregory Benton",
        "arxiv_title": "Learning Invariances in Neural Networks",
        "arxiv_num": "2010.11882",
        "arxiv_published": "2020-10-22T17:18:48Z",
        "arxiv_updated": "2020-10-22T17:18:48Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/09/_1909_01066_language_models_as",
        "tag": [
            "http://www.semanlink.net/tag/language_models_as_knowledge_bases",
            "http://www.semanlink.net/tag/facebook_fair",
            "http://www.semanlink.net/tag/knowledge_graph_deep_learning",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/language_model",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "an analysis of the relational knowledge present in pretrained language models shows an ability of these models to recall factual knowledge ",
        "title": "[1909.01066] Language Models as Knowledge Bases?",
        "relatedDoc": [],
        "creationTime": "2019-09-05T22:32:00Z",
        "creationDate": "2019-09-05",
        "bookmarkOf": [
            "https://arxiv.org/abs/1909.01066"
        ],
        "arxiv_author": [
            "Patrick Lewis",
            "Yuxiang Wu",
            "Anton Bakhtin",
            "Alexander H. Miller",
            "Sebastian Riedel",
            "Fabio Petroni",
            "Tim Rockt\u00e4schel"
        ],
        "arxiv_summary": "Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.",
        "arxiv_firstAuthor": "Fabio Petroni",
        "arxiv_title": "Language Models as Knowledge Bases?",
        "arxiv_num": "1909.01066",
        "arxiv_published": "2019-09-03T11:11:08Z",
        "arxiv_updated": "2019-09-04T09:33:20Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/09/1609_02521_dismec_distribut",
        "tag": [
            "http://www.semanlink.net/tag/extreme_multi_label_classification",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "(WSDM 2017) [Code](https://sites.google.com/site/rohitbabbar/dismec) on author's site (several papers related to XClassification)",
        "title": "[1609.02521] DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification",
        "relatedDoc": [],
        "creationTime": "2020-09-06T10:57:36Z",
        "creationDate": "2020-09-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/1609.02521"
        ],
        "arxiv_author": [
            "Rohit Babbar",
            "Bernhard Shoelkopf"
        ],
        "arxiv_summary": "Extreme multi-label classification refers to supervised multi-label learning\ninvolving hundreds of thousands or even millions of labels. Datasets in extreme\nclassification exhibit fit to power-law distribution, i.e. a large fraction of\nlabels have very few positive instances in the data distribution. Most\nstate-of-the-art approaches for extreme multi-label classification attempt to\ncapture correlation among labels by embedding the label matrix to a\nlow-dimensional linear sub-space. However, in the presence of power-law\ndistributed extremely large and diverse label spaces, structural assumptions\nsuch as low rank can be easily violated.\nIn this work, we present DiSMEC, which is a large-scale distributed framework\nfor learning one-versus-rest linear classifiers coupled with explicit capacity\ncontrol to control model size. Unlike most state-of-the-art methods, DiSMEC\ndoes not make any low rank assumptions on the label matrix. Using double layer\nof parallelization, DiSMEC can learn classifiers for datasets consisting\nhundreds of thousands labels within few hours. The explicit capacity control\nmechanism filters out spurious parameters which keep the model compact in size,\nwithout losing prediction accuracy. We conduct extensive empirical evaluation\non publicly available real-world datasets consisting upto 670,000 labels. We\ncompare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\nis a leading approach for learning sparse local embeddings, and FastXML which\nis a tree-based approach optimizing ranking based loss function. On some of the\ndatasets, DiSMEC can significantly boost prediction accuracies - 10% better\ncompared to SLECC and 15% better compared to FastXML, in absolute terms.",
        "arxiv_firstAuthor": "Rohit Babbar",
        "arxiv_title": "DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification",
        "arxiv_num": "1609.02521",
        "arxiv_published": "2016-09-08T18:17:25Z",
        "arxiv_updated": "2016-09-08T18:17:25Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2021/01/2010_00904_autoregressive_ent",
        "tag": [
            "http://www.semanlink.net/tag/entity_linking",
            "http://www.semanlink.net/tag/nlp_facebook",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "One sentence sumary: \r\n\r\n> We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.\r\n\r\n> a transformer-based architecture, pre-trained\r\nwith a language modeling objective (i.e., we use BART weights from Lewis et al. (2019)) and finetuned\r\nto generate entity names.\r\n\r\n- [tweet](https://twitter.com/nicola_decao/status/1349354669643100161)\r\n- <https://openreview.net/forum?id=5k8F6UU39V>",
        "title": "[2010.00904] Autoregressive Entity Retrieval",
        "relatedDoc": [],
        "creationTime": "2021-01-14T10:04:01Z",
        "creationDate": "2021-01-14",
        "bookmarkOf": [
            "https://arxiv.org/abs/2010.00904"
        ],
        "arxiv_author": [
            "Nicola De Cao",
            "Gautier Izacard",
            "Sebastian Riedel",
            "Fabio Petroni"
        ],
        "arxiv_summary": "Entities are at the center of how we represent and aggregate knowledge. For\ninstance, Encyclopedias such as Wikipedia are structured by entities (e.g., one\nper article). The ability to retrieve such entities given a query is\nfundamental for knowledge-intensive tasks such as entity linking and\nopen-domain question answering. One way to understand current approaches is as\nclassifiers among atomic labels, one for each entity. Their weight vectors are\ndense entity representations produced by encoding entity information such as\ndescriptions. This approach leads to several shortcomings: i) context and\nentity affinity is mainly captured through a vector dot product, potentially\nmissing fine-grained interactions between the two; ii) a large memory footprint\nis needed to store dense representations when considering large entity sets;\niii) an appropriately hard set of negative data has to be subsampled at\ntraining time. We propose GENRE, the first system that retrieves entities by\ngenerating their unique names, left to right, token-by-token in an\nautoregressive fashion, and conditioned on the context. This enables to\nmitigate the aforementioned technical issues: i) the autoregressive formulation\nallows us to directly capture relations between context and entity name,\neffectively cross encoding both; ii) the memory footprint is greatly reduced\nbecause the parameters of our encoder-decoder architecture scale with\nvocabulary size, not entity count; iii) the exact softmax loss can be\nefficiently computed without the need to subsample negative data. We show the\nefficacy of the approach with more than 20 datasets on entity disambiguation,\nend-to-end entity linking and document retrieval tasks, achieving new SOTA, or\nvery competitive results while using a tiny fraction of the memory of competing\nsystems. Finally, we demonstrate that new entities can be added by simply\nspecifying their unambiguous name.",
        "arxiv_firstAuthor": "Nicola De Cao",
        "arxiv_title": "Autoregressive Entity Retrieval",
        "arxiv_num": "2010.00904",
        "arxiv_published": "2020-10-02T10:13:31Z",
        "arxiv_updated": "2020-10-02T10:13:31Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2020/05/2003_08505_a_metric_learning_",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/ml_evaluation",
            "http://www.semanlink.net/tag/ai_facebook",
            "http://www.semanlink.net/tag/metric_learning"
        ],
        "comment": "",
        "title": "[2003.08505] A Metric Learning Reality Check",
        "relatedDoc": [],
        "creationTime": "2020-05-10T11:06:07Z",
        "creationDate": "2020-05-10",
        "bookmarkOf": [
            "https://arxiv.org/abs/2003.08505"
        ],
        "arxiv_author": [
            "Kevin Musgrave",
            "Ser-Nam Lim",
            "Serge Belongie"
        ],
        "arxiv_summary": "Deep metric learning papers from the past four years have consistently\nclaimed great advances in accuracy, often more than doubling the performance of\ndecade-old methods. In this paper, we take a closer look at the field to see if\nthis is actually true. We find flaws in the experimental setup of these papers,\nand propose a new way to evaluate metric learning algorithms. Finally, we\npresent experimental results that show that the improvements over time have\nbeen marginal at best.",
        "arxiv_firstAuthor": "Kevin Musgrave",
        "arxiv_title": "A Metric Learning Reality Check",
        "arxiv_num": "2003.08505",
        "arxiv_published": "2020-03-18T23:28:04Z",
        "arxiv_updated": "2020-03-18T23:28:04Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "http://www.semanlink.net/doc/2019/11/_1910_09760_question_answering",
        "tag": [
            "http://www.semanlink.net/tag/arxiv_doc",
            "http://www.semanlink.net/tag/knowledge_graph",
            "http://www.semanlink.net/tag/question_answering"
        ],
        "comment": "",
        "title": "[1910.09760] Question Answering over Knowledge Graphs via Structural Query Patterns",
        "relatedDoc": [],
        "creationTime": "2019-11-06T13:19:45Z",
        "creationDate": "2019-11-06",
        "bookmarkOf": [
            "https://arxiv.org/abs/1910.09760"
        ],
        "arxiv_author": [
            "Mei Zhang",
            "Weiguo Zheng"
        ],
        "arxiv_summary": "Natural language question answering over knowledge graphs is an important and\ninteresting task as it enables common users to gain accurate answers in an easy\nand intuitive manner. However, it remains a challenge to bridge the gap between\nunstructured questions and structured knowledge graphs. To address the problem,\na natural discipline is building a structured query to represent the input\nquestion. Searching the structured query over the knowledge graph can produce\nanswers to the question. Distinct from the existing methods that are based on\nsemantic parsing or templates, we propose an effective approach powered by a\nnovel notion, structural query pattern, in this paper. Given an input question,\nwe first generate its query sketch that is compatible with the underlying\nstructure of the knowledge graph. Then, we complete the query graph by labeling\nthe nodes and edges under the guidance of the structural query pattern.\nFinally, answers can be retrieved by executing the constructed query graph over\nthe knowledge graph. Evaluations on three question answering benchmarks show\nthat our proposed approach outperforms state-of-the-art methods significantly.",
        "arxiv_firstAuthor": "Weiguo Zheng",
        "arxiv_title": "Question Answering over Knowledge Graphs via Structural Query Patterns",
        "arxiv_num": "1910.09760",
        "arxiv_published": "2019-10-22T04:21:06Z",
        "arxiv_updated": "2019-10-24T10:33:13Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    },
    {
        "document": "https://arxiv.org/abs/1511.08855",
        "tag": [
            "http://www.semanlink.net/tag/semantic_folding",
            "http://www.semanlink.net/tag/semantic_fingerprints",
            "http://www.semanlink.net/tag/arxiv_doc"
        ],
        "comment": "",
        "title": "[1511.08855] Semantic Folding Theory And its Application in Semantic Fingerprinting",
        "relatedDoc": [],
        "creationTime": "2017-11-19T15:59:15Z",
        "creationDate": "2017-11-19",
        "bookmarkOf": [],
        "arxiv_author": [
            "Francisco De Sousa Webber"
        ],
        "arxiv_summary": "Human language is recognized as a very complex domain since decades. No\ncomputer system has been able to reach human levels of performance so far. The\nonly known computational system capable of proper language processing is the\nhuman brain. While we gather more and more data about the brain, its\nfundamental computational processes still remain obscure. The lack of a sound\ncomputational brain theory also prevents the fundamental understanding of\nNatural Language Processing. As always when science lacks a theoretical\nfoundation, statistical modeling is applied to accommodate as many sampled\nreal-world data as possible. An unsolved fundamental issue is the actual\nrepresentation of language (data) within the brain, denoted as the\nRepresentational Problem. Starting with Jeff Hawkins' Hierarchical Temporal\nMemory (HTM) theory, a consistent computational theory of the human cortex, we\nhave developed a corresponding theory of language data representation: The\nSemantic Folding Theory. The process of encoding words, by using a topographic\nsemantic space as distributional reference frame into a sparse binary\nrepresentational vector is called Semantic Folding and is the central topic of\nthis document. Semantic Folding describes a method of converting language from\nits symbolic representation (text) into an explicit, semantically grounded\nrepresentation that can be generically processed by Hawkins' HTM networks. As\nit turned out, this change in representation, by itself, can solve many complex\nNLP problems by applying Boolean operators and a generic similarity function\nlike the Euclidian Distance. Many practical problems of statistical NLP\nsystems, like the high cost of computation, the fundamental incongruity of\nprecision and recall , the complex tuning procedures etc., can be elegantly\novercome by applying Semantic Folding.",
        "arxiv_firstAuthor": "Francisco De Sousa Webber",
        "arxiv_title": "Semantic Folding Theory And its Application in Semantic Fingerprinting",
        "arxiv_num": "1511.08855",
        "arxiv_published": "2015-11-28T00:13:09Z",
        "arxiv_updated": "2016-03-16T22:04:51Z",
        "source": "",
        "mainDoc": "",
        "date": "",
        "seeAlso": [],
        "bookmarOf": "",
        "references": "",
        "publish": "",
        "creator": "",
        "type": "",
        "homepage": "",
        "bookmakOf": "",
        "linkTo": ""
    }
]